{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WSO2 Enterprise Integrator Documentation \u00b6 WSO2 Enterprise Integrator (WSO2 EI) 7.x is an open-source hybrid integration platform that enables API-centric integration using integration architecture styles such as microservices or centralized ESB. The platform provides a graphical drag-and-drop flow designer and a configuration-driven approach to build low-code integration solutions for cloud and container-native projects. Enterprise Integrator 7.x series product suite consists of Micro Integrator and Streaming integrator runtimes. Micro Integrator A cloud-native variant of the battle-tested WSO2 EI/ESB runtime based on xml configurations. It comes with Integration Studio (a graphical drag-and-drop integration flow development environment). Additionally, it includes a CLI tool, a Kubernetes operator, a monitoring dashboard, and 100s of connectors that facilitate integration with on-premise systems and SaaS applications. Get Started Streaming Integrator An advanced stream processing engine that understands streaming SQL queries to capture, analyze, and process streaming data, and allows us to integrate and act on event streams in real-time. Streaming Integrator allows you to connect any data source to any destination with its 60+ prebuilt, production-grade connectors. It comes with a web-based IDE for designing, developing, testing, and deploying stream processing applications with a graphical drag-and-drop experience or by writing streaming SQL queries. Get Started","title":"Home"},{"location":"#wso2-enterprise-integrator-documentation","text":"WSO2 Enterprise Integrator (WSO2 EI) 7.x is an open-source hybrid integration platform that enables API-centric integration using integration architecture styles such as microservices or centralized ESB. The platform provides a graphical drag-and-drop flow designer and a configuration-driven approach to build low-code integration solutions for cloud and container-native projects. Enterprise Integrator 7.x series product suite consists of Micro Integrator and Streaming integrator runtimes.","title":"WSO2 Enterprise Integrator Documentation"},{"location":"concepts/","text":"Concepts in Streaming Integrator \u00b6","title":"Concepts in Streaming Integrator"},{"location":"concepts/#concepts-in-streaming-integrator","text":"","title":"Concepts in Streaming Integrator"},{"location":"page-not-found/","text":"Page not found. \u00b6 Sorry but the page you are looking for does not exist, has been removed, changed, or is temporarily unavailable. For inquiries, please reach us at dev@wso2.org .","title":""},{"location":"page-not-found/#page-not-found","text":"Sorry but the page you are looking for does not exist, has been removed, changed, or is temporarily unavailable. For inquiries, please reach us at dev@wso2.org .","title":"Page not found."},{"location":"admin/adding-third-party-non-osgi-libraries/","text":"Adding Third Party Non OSGi Libraries \u00b6 The Streaming Integrator is OSGi-based. Therefore, when you integrate third party products such as Oracle with the Streaming Integrator, you need to check whether the libraries you need to add to the Streaming Integrator are OSGi-based. If they are not, you need to convert them to OSGi bundles before adding them to the <SI_HOME>/lib directory. To convert jar files to OSGi bundles, follow the procedure given below: Download the non-OSGi jar for the required third party product, and save it in a preferred directory in your machine. In your CLI, navigate to the <SI_HOME>/bin directory. Then issue the following command. ./jartobundle.sh <PATH_TO_NON-OSGi_JAR> ../lib This generates the converted file in the <SI_HOME>/lib directory. Restart the WSO2 SI server.","title":"Adding Third Party Non OSGi Library"},{"location":"admin/adding-third-party-non-osgi-libraries/#adding-third-party-non-osgi-libraries","text":"The Streaming Integrator is OSGi-based. Therefore, when you integrate third party products such as Oracle with the Streaming Integrator, you need to check whether the libraries you need to add to the Streaming Integrator are OSGi-based. If they are not, you need to convert them to OSGi bundles before adding them to the <SI_HOME>/lib directory. To convert jar files to OSGi bundles, follow the procedure given below: Download the non-OSGi jar for the required third party product, and save it in a preferred directory in your machine. In your CLI, navigate to the <SI_HOME>/bin directory. Then issue the following command. ./jartobundle.sh <PATH_TO_NON-OSGi_JAR> ../lib This generates the converted file in the <SI_HOME>/lib directory. Restart the WSO2 SI server.","title":"Adding Third Party Non OSGi Libraries"},{"location":"admin/configuring-Cluster-Coordination/","text":"Configuring Cluster Coordination \u00b6 Multiple WSO2 SI nodes can be configured to work together by configuring a cluster coordination strategy that is used in various deployments such as the Minimum High Available(HA) Deployment and Scalable High Available(HA) Deployment . At present, cluster coordination is supported via an RDBMS instance using and RDBMS coordination strategy. Support for cluster coordination via a Zookeeper instance will be supported in the near future. At any given time, there is a leader in an SI cluster that is arbitrarily selected among the members of the cluster. The RDBMS coordination strategy that is used for cluster coordination works on the concept of heartbeats where the members of the cluster periodically send heartbeat signals via the datasource to the leader of the cluster. If the leader node does not detect a pre configured consecutive number of heartbeats from a specific node, the relevant node is removed from the cluster. Similarly, if the leader node fails to update its heartbeat, the cluster re-elects a new leader. Prerequisites \u00b6 In order to configure a cluster, the following prerequisites must be completed: A minimum of two binary packs of WSO2 SI must be available. A working RDBMS instance must be available to be shared among the nodes of the cluster. Configuring the Cluster with the RDBMS coordination strategy \u00b6 To configure a cluster for several nodes, the cluster.config section of the <SI_HOME>/conf/server/deployment.yaml should be configured for all the nodes as follows: Parameter Purpose Sample Values enabled Set this value to true to enable cluster coordination for the node. true/false groupId The group ID is used to identify the cluster to which the node belongs. Nodes that belong to the same cluster must be configured with the same group ID. group-1 coordinationStrategyClass The clustering class to be used. org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig > datasource The shared datasource to be used in the cluster. The datasource specified must be properly configured in the deployment.yaml file. For detailed instructions to configure a datasource, see Configuring Datasources . WSO2_CARBON_DB strategyConfig > heartbeatInterval This value defines the time interval in milliseconds between heartbeat pulses sent by nodes to indicate that they are still alive within the cluster. 1000 strategyConfig > heartbeatMaxRetry The number of times the heartbeat pulse can be unavailable until a node is identified as unresponsive. If a node fails to send its heartbeat pulse to the leader of the cluster after a number of retries equal to the number specified here, that node is removed from the cluster. 2 strategyConfig > eventPollingInterval The time interval in millseconds at which a node listens to identify the changes happening within the cluster. The changes may include a new node joining the cluster, a node being removed from the cluster and the coordinator changed event. 1000 Following is a sample segment of the configurations needed for RDBMS coordination in the deployment.yaml Sample deployment.yaml segment cluster.config: enabled: true groupId: <GROUP ID> coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: <DATASOURCE NAME> heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000","title":"Configuring Cluster Coordination"},{"location":"admin/configuring-Cluster-Coordination/#configuring-cluster-coordination","text":"Multiple WSO2 SI nodes can be configured to work together by configuring a cluster coordination strategy that is used in various deployments such as the Minimum High Available(HA) Deployment and Scalable High Available(HA) Deployment . At present, cluster coordination is supported via an RDBMS instance using and RDBMS coordination strategy. Support for cluster coordination via a Zookeeper instance will be supported in the near future. At any given time, there is a leader in an SI cluster that is arbitrarily selected among the members of the cluster. The RDBMS coordination strategy that is used for cluster coordination works on the concept of heartbeats where the members of the cluster periodically send heartbeat signals via the datasource to the leader of the cluster. If the leader node does not detect a pre configured consecutive number of heartbeats from a specific node, the relevant node is removed from the cluster. Similarly, if the leader node fails to update its heartbeat, the cluster re-elects a new leader.","title":"Configuring Cluster Coordination"},{"location":"admin/configuring-Cluster-Coordination/#prerequisites","text":"In order to configure a cluster, the following prerequisites must be completed: A minimum of two binary packs of WSO2 SI must be available. A working RDBMS instance must be available to be shared among the nodes of the cluster.","title":"Prerequisites"},{"location":"admin/configuring-Cluster-Coordination/#configuring-the-cluster-with-the-rdbms-coordination-strategy","text":"To configure a cluster for several nodes, the cluster.config section of the <SI_HOME>/conf/server/deployment.yaml should be configured for all the nodes as follows: Parameter Purpose Sample Values enabled Set this value to true to enable cluster coordination for the node. true/false groupId The group ID is used to identify the cluster to which the node belongs. Nodes that belong to the same cluster must be configured with the same group ID. group-1 coordinationStrategyClass The clustering class to be used. org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig > datasource The shared datasource to be used in the cluster. The datasource specified must be properly configured in the deployment.yaml file. For detailed instructions to configure a datasource, see Configuring Datasources . WSO2_CARBON_DB strategyConfig > heartbeatInterval This value defines the time interval in milliseconds between heartbeat pulses sent by nodes to indicate that they are still alive within the cluster. 1000 strategyConfig > heartbeatMaxRetry The number of times the heartbeat pulse can be unavailable until a node is identified as unresponsive. If a node fails to send its heartbeat pulse to the leader of the cluster after a number of retries equal to the number specified here, that node is removed from the cluster. 2 strategyConfig > eventPollingInterval The time interval in millseconds at which a node listens to identify the changes happening within the cluster. The changes may include a new node joining the cluster, a node being removed from the cluster and the coordinator changed event. 1000 Following is a sample segment of the configurations needed for RDBMS coordination in the deployment.yaml Sample deployment.yaml segment cluster.config: enabled: true groupId: <GROUP ID> coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: <DATASOURCE NAME> heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000","title":"Configuring the Cluster with the RDBMS coordination strategy"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/","text":"Configuring Database and File System State Persistence \u00b6 This section explains how to prevent the loss of data that can result from a system failure by persisting the state of WSO2 SI periodically either into a database system or into the file system. Prerequisites \u00b6 Before configuring RDBMS database persistence, the following prerequisites must be completed. One or more Siddhi Applications must be running in the WSO2 SI server. A working RDBMS instance that can be used for data persistence must exist. The requirements of the datasource must be already defined. Database persistence involves updating the databases connected to WSO2 Steaming Integrator with the latest information relating to the events that are being processed by WSO2 SI at a given time. Configuring database system persistence \u00b6 The supported databases are H2, MySQL, Postgres, MSSQL and Oracle. The relevant jdbc driver jar should be downloaded and added to the <SI_HOME>/lib directory to prior to using database system persistence. To configure periodic data persistence, update the <SI_HOME>/conf/server/deployment.yaml file under state.persistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store . org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config > datasource The datasource to be used in persisting the state. The provided datasource should be properly defined in the deployment.yaml. For detailed instructions of how to configure a datasource, see Configuring Datasources . WSO2_PERSISTENCE_DB (Datasource with this name should be defined in wso2.datasources) config > table The table that should be created and used for the persisting of the state. PERSISTENCE_TABLE The following is a sample segment of the required configurations in the <SI_HOME>/conf/server/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: <DATASOURCE NAME> # A datasource with this name should be defined in wso2.datasources namespace table: <TABLE NAME> Configuring file system persistence \u00b6 This section explains how to persist the states of Siddhi applications during a required time interval in the file system in order to maintain back-ups. To configure state persistence, update the <SI_HOME>/conf/server/deployment.yaml file under state .p ersistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store. org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config > location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample segment of the required configurations in the <SI_HOME>/conf/server/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Configuring Database and File System State Persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-database-and-file-system-state-persistence","text":"This section explains how to prevent the loss of data that can result from a system failure by persisting the state of WSO2 SI periodically either into a database system or into the file system.","title":"Configuring Database and File System State Persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#prerequisites","text":"Before configuring RDBMS database persistence, the following prerequisites must be completed. One or more Siddhi Applications must be running in the WSO2 SI server. A working RDBMS instance that can be used for data persistence must exist. The requirements of the datasource must be already defined. Database persistence involves updating the databases connected to WSO2 Steaming Integrator with the latest information relating to the events that are being processed by WSO2 SI at a given time.","title":"Prerequisites"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-database-system-persistence","text":"The supported databases are H2, MySQL, Postgres, MSSQL and Oracle. The relevant jdbc driver jar should be downloaded and added to the <SI_HOME>/lib directory to prior to using database system persistence. To configure periodic data persistence, update the <SI_HOME>/conf/server/deployment.yaml file under state.persistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store . org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config > datasource The datasource to be used in persisting the state. The provided datasource should be properly defined in the deployment.yaml. For detailed instructions of how to configure a datasource, see Configuring Datasources . WSO2_PERSISTENCE_DB (Datasource with this name should be defined in wso2.datasources) config > table The table that should be created and used for the persisting of the state. PERSISTENCE_TABLE The following is a sample segment of the required configurations in the <SI_HOME>/conf/server/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: <DATASOURCE NAME> # A datasource with this name should be defined in wso2.datasources namespace table: <TABLE NAME>","title":"Configuring database system persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-file-system-persistence","text":"This section explains how to persist the states of Siddhi applications during a required time interval in the file system in order to maintain back-ups. To configure state persistence, update the <SI_HOME>/conf/server/deployment.yaml file under state .p ersistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store. org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config > location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample segment of the required configurations in the <SI_HOME>/conf/server/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Configuring file system persistence"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/","text":"Configuring Single Sign-On for WSO2 SI \u00b6 Note The functionality described in this section is not yet released. SSO (Single Sign-On) allows you to be authenticated to access one application, and gain access to multiple other applications without having to repeatedly provide your credentials for authentication purposes. This section explains how you can configure single sign-on for the Status Dashboard and the Business Rules Manager. Tip Before you begin: Configure the external identity provider (IdP) that you are using for SSO. By default, WSO2 SI uses WSO2 IS (versions 5.4.0 and later) as the Identity Provider. For detailed instructions to configure WSO2 IS for this scenario, see OAuth2 Token Validation and Introspection . If you want to use any other identity provider, make sure that it supports OAuth 2 Dynamic Client Registration, and do the required configurations (which differ based on the IdP). Enabling SSO Testing the SSO configuration Enabling SSO \u00b6 To configure SSO for the WSO2 SP, open the <SI_TOOLING_HOME>/conf/server/deployment.yaml file and update it as follows: In the auth.configs section, start creating a new entry with a new client type. You need an external IdP client for SSO. Therefore, enter external as the type. auth.configs: type: external To enable SSO, set the ssoEnabled property as shown below. auth.configs: type: external ssoEnabled: true In order to allow SSO to function in your SP setup, you need to set the following properties under the ssoEnabled property. auth.configs: type: external ssoEnabled: true properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: authorization_code The purposes of these properties are explained in the table below. Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The application context of the Business Rules application in WSO2 SP. cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType authorization_code The grant type used in the OAuth application token request. externalLogoutURL https://localhost:9443/samlsso The URL via which you can llog out from the external IDP provider side in the SSO. Save your changes. Testing the SSO configuration \u00b6 Once the above changes are made, you can start the dashboard server of WSO2 SP and access all the UIs in it with a single sign-on. To try this out, follow the steps below: Start the tooling server by issuing one of the following commands: On Windows : tooling.bat --run On Linux/Mac OS : sh tooling.sh Access the Dashboard Portal via the following URL. https://localhost:9643/portal In the dialog box that appears to sign in, enter admin as both the user name and the password, and then click LOG IN . Now access the Business Rules Manager via the following URL. https://localhost:9643/b usiness-rules No dialog box appears for the Business Rules Manager. This because you provided your credentials to access the Dashboard Portal, and the activation of SSO makes that sign-in valid for all the UIs accessible via the dashboard profile.","title":"Configuring Single Sign-On for WSO2 SI"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#configuring-single-sign-on-for-wso2-si","text":"Note The functionality described in this section is not yet released. SSO (Single Sign-On) allows you to be authenticated to access one application, and gain access to multiple other applications without having to repeatedly provide your credentials for authentication purposes. This section explains how you can configure single sign-on for the Status Dashboard and the Business Rules Manager. Tip Before you begin: Configure the external identity provider (IdP) that you are using for SSO. By default, WSO2 SI uses WSO2 IS (versions 5.4.0 and later) as the Identity Provider. For detailed instructions to configure WSO2 IS for this scenario, see OAuth2 Token Validation and Introspection . If you want to use any other identity provider, make sure that it supports OAuth 2 Dynamic Client Registration, and do the required configurations (which differ based on the IdP). Enabling SSO Testing the SSO configuration","title":"Configuring Single Sign-On for WSO2 SI"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#enabling-sso","text":"To configure SSO for the WSO2 SP, open the <SI_TOOLING_HOME>/conf/server/deployment.yaml file and update it as follows: In the auth.configs section, start creating a new entry with a new client type. You need an external IdP client for SSO. Therefore, enter external as the type. auth.configs: type: external To enable SSO, set the ssoEnabled property as shown below. auth.configs: type: external ssoEnabled: true In order to allow SSO to function in your SP setup, you need to set the following properties under the ssoEnabled property. auth.configs: type: external ssoEnabled: true properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: authorization_code The purposes of these properties are explained in the table below. Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The application context of the Business Rules application in WSO2 SP. cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType authorization_code The grant type used in the OAuth application token request. externalLogoutURL https://localhost:9443/samlsso The URL via which you can llog out from the external IDP provider side in the SSO. Save your changes.","title":"Enabling SSO"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#testing-the-sso-configuration","text":"Once the above changes are made, you can start the dashboard server of WSO2 SP and access all the UIs in it with a single sign-on. To try this out, follow the steps below: Start the tooling server by issuing one of the following commands: On Windows : tooling.bat --run On Linux/Mac OS : sh tooling.sh Access the Dashboard Portal via the following URL. https://localhost:9643/portal In the dialog box that appears to sign in, enter admin as both the user name and the password, and then click LOG IN . Now access the Business Rules Manager via the following URL. https://localhost:9643/b usiness-rules No dialog box appears for the Business Rules Manager. This because you provided your credentials to access the Dashboard Portal, and the activation of SSO makes that sign-in valid for all the UIs accessible via the dashboard profile.","title":"Testing the SSO configuration"},{"location":"admin/configuring-System-Parameters-for-Siddhi-Extensions/","text":"Configuring System Parameters for Siddhi Extensions \u00b6 The pre-written Siddhi extensions supported by the Streaming Integrator are configured with default values for system parameters. If you need to override those values, you can refer to those extensions from the <SI_HOME>/conf/<RUNTIME>/deployment.yaml file and add the system parameters with the required values as key-value pairs. To do this, follow the procedure below: Open the <SI_HOME>/conf/server/deployment.yaml file. The extensions belong to the Siddhi component. Therefore, to edit the Siddhi component, add a main section to the file named siddhi . Then add a subsection named extensions to indicate that the configurations related to Siddhi extensions as shown below. siddhi: extensions: For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. siddhi: extensions: - extension: Under each extension subsection, add two key-value pairs as follows. Key Value name The name of the extension. e.g., tcp namespace The archetype of the extension. e.g., source Info The archetypes of extensions supported are source , sink , execution , io , map , script , and store . Add a subsection named properties to overide the system properties. Then add the system properties with the required values as key value pairs, as shown below. siddhi: extensions: - extension: name: [extension-name] namespace: [extension-namespace] properties: [key]: [value] Following are examples for overriding default values for system properties. Example 1: Defining host and port for TCP siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2: Overwriting the default RDBMS configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring System Parameters for Siddhi Extensions"},{"location":"admin/configuring-System-Parameters-for-Siddhi-Extensions/#configuring-system-parameters-for-siddhi-extensions","text":"The pre-written Siddhi extensions supported by the Streaming Integrator are configured with default values for system parameters. If you need to override those values, you can refer to those extensions from the <SI_HOME>/conf/<RUNTIME>/deployment.yaml file and add the system parameters with the required values as key-value pairs. To do this, follow the procedure below: Open the <SI_HOME>/conf/server/deployment.yaml file. The extensions belong to the Siddhi component. Therefore, to edit the Siddhi component, add a main section to the file named siddhi . Then add a subsection named extensions to indicate that the configurations related to Siddhi extensions as shown below. siddhi: extensions: For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. siddhi: extensions: - extension: Under each extension subsection, add two key-value pairs as follows. Key Value name The name of the extension. e.g., tcp namespace The archetype of the extension. e.g., source Info The archetypes of extensions supported are source , sink , execution , io , map , script , and store . Add a subsection named properties to overide the system properties. Then add the system properties with the required values as key value pairs, as shown below. siddhi: extensions: - extension: name: [extension-name] namespace: [extension-namespace] properties: [key]: [value] Following are examples for overriding default values for system properties. Example 1: Defining host and port for TCP siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2: Overwriting the default RDBMS configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring System Parameters for Siddhi Extensions"},{"location":"admin/creating-business-rules-templates/","text":"Working with Business Rules \u00b6 In streaming integration, there are common use cases for analyzing statistics that involve operations such as calculating the average, minimum, maximum etc., for different endpoints. The Business Rules Manager allows you to define templates and generate business rules from them for different scenarios with common requirements. Creating Business Rules \u00b6 This section explains how to create a business rule. A business rule can be created from a template or from scratch . Creating Business Rules from a Template \u00b6 Creating business rules from an existing template allows you to use sources, sinks and filters that have been already defined, and assign variable values to process events. Before you begin: The business rule template must be already configured in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. For detailed instructions, see Business Rules Templates . If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: On Windows: server.bat --run On Linux/Mac OS: ./server.sh To create a business rule from a template, follow the procedure below: Navigate to the <SI_TOOLING_HOME> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands: On Windows: tooling.bat --run On Linux/Mac OS: ./tooling.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http://<SI_TOOLING_HOST>:<HTTP_PORT>/business-rules http://0.0.0.0:9090/business-rules HTTPS https://<SI_TOOLING_HOST>:<HTTPS_PORT>/business-rules https://0.0.0.0:9443/business-rules This opens the following: Click Create to open the following page. Then click From Template to open the Select a Template Group page, where the available templates are displayed. Click on the template group that contains the required template to create a business rule from it. In this example, the business rule is created based on a template in the Sweet Factory template group that is packed with the Streaming Integrator by default. Therefore, click Sweet Factory to open this template group. In the template group, expand the Rule Template list as shown below, and click on the required template. For this example, click Identify Continuous Production Decrease . If you want to change the rule template from which you want to create the business rule, select the required value for the Rule Template field. Enter a name for the business rule in the Business Rule Name field. Enter values for the rest of the fields following the instructions in the UI. Info The fields displayed for the business rule differ based on the selected template. If you want to save the business rule and deploy it later, click Save . If you want to deploy the business rule immediately, click Save and Deploy . Creating a business rule from scratch \u00b6 Creating a business rule from scratch allows you to define the filter logic for the rule at the time of creating, instead of using the filter logic that has been already defined in a template. However, you can select the required source and sink configurations from an existing template. Before you begin: If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: - On Windows: server.bat --run - On Linux/Mac OS: ./server.sh To create a business rule from scratch, follow the procedure below: Navigate to the <SI_TOOLING_HOME> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands: On Windows: tooling.bat --run On Linux/Mac OS: ./tooling.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http://<SI_TOOLING_HOST>:<HTTP_PORT>/business-rules http://0.0.0.0:9090/business-rules HTTPS https://<SI_TOOLING_HOST>:<HTTPS_PORT>/business-rules https://0.0.0.0:9443/business-rules This opens the following: Click Create to open the following page, and then click From Scratch . This opens the Select a Template Group page where the available template groups are displayed as shown in the example below. Click on the template group from which you want to select the required sources and sinks for your business rule. For this example, click Stock Exchange to open that template group as shown below. Click Input to expand the Input section. Then select the rule template from which the source and input configurations for the business rule must be selected. This displays the list of available sources and the exposed attributes of the selected template as shown below. Click Filters to expand the Filters section, and click + to add a new filter. A table is displayed as shown below. To define a filter, follow the steps below: In the Attribute field, select the attribute based on which you want to define the filter condition. In the Operator field, select an operator. In the Value/Attribute field, enter the value or another attribute based on which, the Operator is applied to the Attribute . e.g., If you want to filter events where the price is less than 100 (which is a value), select values for the fields as follows: Field Value Attribute price Operator < Value/Attribute (Value) 100 If you want to filter events where the price is equal to the volume (which is another attribute), select values for the fields as follows: Field Value Attribute price Operator == Value/Attribute (Attribute) volume Once you have defined two or more filters, enter the rule logic in the Rule Logic field using OR , AND , and NOT conditions. The examples of how you can use these keywords are explained in the table below. Keyword Example OR 1 OR 2 returns events that match either filter 1 or 2. AND 1 AND 2 returns events that match both filters 1 and 2. NOT NOT 1 returns events that do not match filter 1. Click Output to expand the Output section. Then select the rule template from which the sink and output configurations for the business rule must be selected. This displays the section for mapping configurations as shown in the example below. Select the relevant attribute names for the Input column. When publishing the events to which the rule is applied via the selected predefined sink, each input event you select is published with the corresponding name in the Output column. Info The output mappings displayed differ based on the output rule template you select. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy . Managing Business Rules \u00b6 Once you have created one or more business rules, you can manage them by viewing, editing, deploying, undeploying, and deleting them as required. Viewing business rules \u00b6 Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. To view a business rule, click the icon for viewing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. Editing business rules \u00b6 Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. To edit a business rule, click the icon for editing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. Modify values for the parameters displayed as required and click Save . Deploying business rules \u00b6 Before you begin: Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing one of the following commands: - On Windows: server.bat --run - On Linux/Mac OS: ./server.sh To deploy a business rule that you have previously saved, click the icon for deploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully deployed. Undeploying business rules \u00b6 To undeploy a business rule, click the icon for undeploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully undeployed. Viewing deployment information \u00b6 If you want to view information relating to the deployment of a business rule, click the icon for viewing deployment information (marked in the image below) for the relevant row. As a result, the deployment information including the host and port of the nodes in which the rule is deployed and the deployment status are displayed as shown in the image below. Possible deployment statuses are as follows: Saved : The business rule is created, but not yet deployed in any Streaming Integrator node. Deployed : The business rule is created and deployed in all the required nodes in the Streaming Integrator cluster. Partially Deployed: The business rule is created and deployed only in some of the required nodes in the Streaming Integrator cluster. Partially Undeployed: The business rule has been previously deployed, and then un-deployed only in some of the nodes in the Streaming Integrator cluster. Info Required nodes are configured with respect to a rule template. For detailed instructions, see Deploying business rules in SI server . Deleting business rules \u00b6 To delete a business rule, click the icon for deleting (marked in the image below) for the relevant row. A message appears to confirm whether you want to proceed with the deletion. Click Delete in the message. As a result, another message appears to inform you that the rule is successfully deleted. Creating a Business Rules Template \u00b6 To create a business template using the Business Rules Template editor, follow the procedure below: If you have not already started the Streaming Integrator tooling, navigate to the <SI_TOOLING_HOME>/bin directory from the terminal and start the Streaming Integrator Tooling as mentioned below. On Windows: tooling.bat --run On Linux/Mac OS: ./ tooling.sh Access the Business Rules Template Editor via the URL that appears for it in the start up logs as shown in the example below. Info The default URL is http://localhost:9390/template-editor . The Template Editor opens as shown below. There are two views from which you can interact and create a template group. Design view allows you to visualize a template group and interact with it. Code view allows you to interact with a template group by typing content. (For more information about template group structure, see Business Rules Templates .) Warning Do not template sensitive information such as passwords in a Siddhi application or expose them directly in a Siddhi application. For detailed instructions to protect sensitive data by obfuscating them, see Protecting Sensitive Data via the Secure Vault . You can create a template group using the design view or the code view as explained in the following sections. Create from Design View \u00b6 To create a business rules template group from the design view, follow the procedure below: Enter a UUID (Universally Unique Identifier), name and a description for the template group as follows. Field Name UUID sweet-factory Name Sweet Factory Description Analyzes Sweet Factory scenarios Expand the first rule template that exists by default, and enter the following details. Field Name Value UUID identifying-continuous-production-decrease Name Identify Continuous Production Decrease Description Alert factory managers if the rate of production continuously decreases for a specified time period Type Template Instance Count One To include a Siddhi application template, expand the first template that is displayed by default, and enter the following Siddhi application template. @App:name('SweetFactory-TrendAnalysis') @source(type='http', @map(type='json')) define stream SweetProductionStream (name string, amount double, factoryId int); @sink(type='log', @map(type='text', @payload(\"\"\" Hi ${username}, Production at Factory {{factoryId}} has gone from {{initalamout}} to {{finalAmount}} in ${timeInterval} seconds!\"\"\"))) define stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double); from SweetProductionStream#window.timeBatch(${timeInterval} sec) select factoryId, sum(amount) as amount, currentTimeMillis() as ts insert into ProdRateStream; partition with ( factoryId of ProdRateStream ) begin from every e1=ProdRateStream, e2=ProdRateStream[ts - e1.ts <= ${timeRange} and e1.amount > amount ]*, e3=ProdRateStream[ts - e1.ts >= ${timeRange} and e1.amount > amount ] select e1.factoryId, e1.ts as initaltime, e3.ts as finalTime, e1.amount as initalamout, e3.amount as finalAmount insert into ContinousProdReductionStream; end; To add variable attributes to the script, click Add Variables . Info A script is a javascript that can be applied when the inputs provided by the business user who uses the template need to be processed before replacing the values for the template variables. e.g., If the average value is not provided, a function within the script can derive it by calculating it from the minimum value and the maximum value provided by the business user. To specify the attributes that need to be considered as variables, select the relevant check boxes under Select templated elements . In this example, you can select the username and timeRange check boxes to to select the attributes with those names as the variables. Then click Add To Script to update the script with the selected variables with auto-generated function bodies as shown below. Edit the script to add the required functions. In this example, let's rename myFunction1(input) to getUsername(email) , and myFunction2(input) to validateTimeRange(number) . var username = getUsername('${userInputForusername}'); var timeRange = validateTimeRange('${userInputFortimeRange}'); /** * Extracts the username from given email * @returns Extracted username * @param email Provided email */ function getUsername(email) { if (email.match(/\\S+@\\S+/g)) { if (email.match(/\\S+@\\S+/g)[0] === email) { return email.split('@')[0]; } throw 'Invalid email address provided'; } throw 'Invalid email address provided'; } /** * Validates the given value for time range * @returns Processed input * @param input User given value */ function validateTimeRange(number) { if (!isNaN(number) && (number > 0)) { return number; } else { throw 'A positive number expected for time range'; } } To generate properties, click Generate against Properties . This expands the Properties section as follows. Enter values for the available properties as follows. For this example, let's enter values as shown in the following table. Info A property is defined for each templated attribute (defined in the ${templatedElement} format), so that it is self descriptive for the business user who uses the template. The values configured for each property are as follows: Field Name : The name with which the templated attribute is displayed to the business user. Field Description : A description of the property for the business user to understand its purpose. Default Value : The value assigned to the property by default. The business user can change this value if required. Options : this is an optional configuration that allows you to define a set of values for a property so that the business user can select the required value from a list. This is useful when the possible value for the property is limited to a set of options. Property Field Name Field Description Default Value timeInterval Time interval (in seconds) Production amounts are considered per time interval 6 userInputForusername Manager Email ID Email address to show in greeting example@email.com userInputFortimeRange Time Range (in milliseconds) Time period in which, product amounts are analyzed for decrease 5 To save the template, click the save icon at the top of the page. Create from code view \u00b6 When you use the code view, the same parameters for which you enter values in the design view are represented as JSON keys. For each parameter, you can specify a value against the relevant JSON key as shown in the extract below. When you update the code view with a valid template group definition, the design view is updated simultaneously as shown below. However, if the content you enter in the code view is an invalid template group, the design view is not updated, and an error is displayed as follows. When an error is detected in the entered template group structure, the Recover button is displayed with the error message. When you click Recover , the code view is reset to the latest detected valid template group definition. At any given time, the design view displays information based on the latest detected valid template group definition. Info It is not recommended to add Siddhi application templates and scripts using the code view because they need to be provided as a single line, and the possible escape characters should be handled carefully. Editing a Business Rules Template \u00b6 WSO2 SI allows you to make edits to a business rules template that you have already created and saved. To edit a template via the Template Editor tool, follow the steps below. Start the WSO2 SI Tooling profile by issuing one of the following commands. For Windows: tooling.bat For Linux: ./tooling.sh Access the Template Editor via the URL that appears for it in the start up logs as shown in the example below. Info The default URL is http://localhost:9390/template-editor . The Template Editor opens as follows. To open an existing template, click the Open icon in the top panel (marked in the image above). In the Open Template File dialog box, click Choose File and browse for the required template. Once you have selected the template, click Load to open it in the Template Editor. Edit the template as required. You can update it in the Design View or the Source View as you prefer. For more information, see Creating a Business Rule Template . Save your edits by clicking the Save icon in the top panel. Business Rules Templates \u00b6 Rule Templates are used as specifications to gain inputs from users, through dynamically generated fields for the purpose of creating business rules. A template group is a business domain level grouping. The definition of a template looks as follows. { \"templateGroup\" : { \"name\" : \"<Name of the template group>\", \"uuid\":\"<UUID for the template group>\", \"description\" : \"<(Optional) description for the template group>\", \"ruleTemplates\" : [ { \"name\" : \"<Name of the rule template>\" , \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"template\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp_1 with ${templatedProperty_x}>\" }, { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp_n with ${templatedProperty_y}>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\"}, \"templatedProperty_y\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } }, { \"name\" : \"<Name of the rule template>\", \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"input\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp with ${templatedProperty_x}>\", \"exposedStreamDefinition\" :\"<Exposed stream definition>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } }, { \"name\" : \"<Name of the rule template>\", \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"output\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp with ${templatedProperty_x}>\", \"exposedStreamDefinition\" :\"<Exposed stream definition>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } } ] } } The following parameters are configured: Template Group basic data \u00b6 The following parameters are configured under templateGroup . Parameter Description Required/Optional name A name for the template group Required uuid A uniquely identifiable id for the template group Required description A description for the template. Optional Rule Template details \u00b6 Multiple rule templates can be defined under a templateGroup . For each ruleTemplate , the following set of parameters need to be configured: Parameter Description Required/Optional name A name for the rule template Required uuid A uniquely identifiable id for the rule template Required type The type of the rule template. Possible values are as follows: template: Used only to create an entire business rule from template input : Used only in creating a business rule from scratch output : Used only in creating a business rule from scratch Required instanceCount This specifies whether the business rules derived from the template can be deployed only on one node, or whether they can be deployed on many nodes. Possible values are as follows: one many Required script The Java script to be executed on the templated fields. Developers can use this script for: validating purposes. deriving values for a templated parameter by combining some other entered parameters You need to mention each templated element that needs to be derived from entered parameters as a variable in the global scope of the javascript. You also need to template the entered parameters in the script itself. These values are later replaced with their respective entered values. Consider the following script /* * Validates a number and returns after adding 10 to it * @throws Error when a non number is entered */ function deriveValue (value) { if ( ! isNan (value) ) { return value + 10 ; } throw \"A number is required\" ; } var derivedValue = deriveValue ($ { enteredValue } ) ; enteredValue should be defined as a property under properties in order to be filled by the user and replaced later. The derived value stored in derivedValue is then used to replace ${derivedValue} in the SiddhiApp template. Optional description A brief description of the rule template. Optional templates These are the artifacts (i.e SiddhiApps) with templated parameters that are instantiated with replaced values when a business rule is created. Required properties You can add a field name, description, default value and possible values (optional) for the templated parameters. Required Deploying business rules in SI server \u00b6 To deploy a business rule in the Streaming Integrator server, follow the procedure below. Before you begin: Both the Streaming Integrator server(s) and Streaming Integrator tooling must be up and running. Save the template group you created as a .json file in the <SI_TOOLING_HOME>/wso2/server/resources/businessRules/templates directory. In the BusinessRules section of the <SI_TOOLING_HOME>/conf/server/deployment.yaml file, add a configuration for the template you created as shown below. wso2.business.rules.manager: datasource: <datasourceName> - nodeURL1: - ruleTemplateUUID1 - ruleTemplateUUID2 nodeURL2: - ruleTemplateUUID1 - ruleTemplateUUID2 Info Specify the value for nodeURL1 in the IP:Port format. If you add this configuration, a business rule which is derived from this rule template (when you run the Streaming Integrator Tooling and the Streaming Integrator servers of your SI setup) is deployed only in the nodes under which - this rule template has been specified. If you do not specifically add business rule template IDs in the configuration, business rules are deployed in all the available Streaming Integrator servers. Configuring Business Rules Manager Permissions \u00b6 There are two permission levels for a business rules application: Manager : User roles with this permission level have administrative privileges over business rules. They are allowed to create, view, edit, deploy or delete business rules. Viewer : User roles with this permission level are only allowed to view business rules. This section covers how to configure Business Rules Manager permissions. Before you begin: Before configuring Business Rules Manager permissions, the user roles to be assigned permissions must be already defined in the user store with the required user IDs. For detailed instructions, see User Management . You need to define the roles related to the Business Rules Manager under the wso2.business.rules.manager component namespace in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. The following is a sample configuration of user roles for the Business Rules Manager. wso2.business.rules.manager: roles: manager: - name: role1 id: 1 viewer: - name: role2 id: 2","title":"Working with Business Rules"},{"location":"admin/creating-business-rules-templates/#working-with-business-rules","text":"In streaming integration, there are common use cases for analyzing statistics that involve operations such as calculating the average, minimum, maximum etc., for different endpoints. The Business Rules Manager allows you to define templates and generate business rules from them for different scenarios with common requirements.","title":"Working with Business Rules"},{"location":"admin/creating-business-rules-templates/#creating-business-rules","text":"This section explains how to create a business rule. A business rule can be created from a template or from scratch .","title":"Creating Business Rules"},{"location":"admin/creating-business-rules-templates/#creating-business-rules-from-a-template","text":"Creating business rules from an existing template allows you to use sources, sinks and filters that have been already defined, and assign variable values to process events. Before you begin: The business rule template must be already configured in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. For detailed instructions, see Business Rules Templates . If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: On Windows: server.bat --run On Linux/Mac OS: ./server.sh To create a business rule from a template, follow the procedure below: Navigate to the <SI_TOOLING_HOME> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands: On Windows: tooling.bat --run On Linux/Mac OS: ./tooling.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http://<SI_TOOLING_HOST>:<HTTP_PORT>/business-rules http://0.0.0.0:9090/business-rules HTTPS https://<SI_TOOLING_HOST>:<HTTPS_PORT>/business-rules https://0.0.0.0:9443/business-rules This opens the following: Click Create to open the following page. Then click From Template to open the Select a Template Group page, where the available templates are displayed. Click on the template group that contains the required template to create a business rule from it. In this example, the business rule is created based on a template in the Sweet Factory template group that is packed with the Streaming Integrator by default. Therefore, click Sweet Factory to open this template group. In the template group, expand the Rule Template list as shown below, and click on the required template. For this example, click Identify Continuous Production Decrease . If you want to change the rule template from which you want to create the business rule, select the required value for the Rule Template field. Enter a name for the business rule in the Business Rule Name field. Enter values for the rest of the fields following the instructions in the UI. Info The fields displayed for the business rule differ based on the selected template. If you want to save the business rule and deploy it later, click Save . If you want to deploy the business rule immediately, click Save and Deploy .","title":"Creating Business Rules from a Template"},{"location":"admin/creating-business-rules-templates/#creating-a-business-rule-from-scratch","text":"Creating a business rule from scratch allows you to define the filter logic for the rule at the time of creating, instead of using the filter logic that has been already defined in a template. However, you can select the required source and sink configurations from an existing template. Before you begin: If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: - On Windows: server.bat --run - On Linux/Mac OS: ./server.sh To create a business rule from scratch, follow the procedure below: Navigate to the <SI_TOOLING_HOME> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands: On Windows: tooling.bat --run On Linux/Mac OS: ./tooling.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http://<SI_TOOLING_HOST>:<HTTP_PORT>/business-rules http://0.0.0.0:9090/business-rules HTTPS https://<SI_TOOLING_HOST>:<HTTPS_PORT>/business-rules https://0.0.0.0:9443/business-rules This opens the following: Click Create to open the following page, and then click From Scratch . This opens the Select a Template Group page where the available template groups are displayed as shown in the example below. Click on the template group from which you want to select the required sources and sinks for your business rule. For this example, click Stock Exchange to open that template group as shown below. Click Input to expand the Input section. Then select the rule template from which the source and input configurations for the business rule must be selected. This displays the list of available sources and the exposed attributes of the selected template as shown below. Click Filters to expand the Filters section, and click + to add a new filter. A table is displayed as shown below. To define a filter, follow the steps below: In the Attribute field, select the attribute based on which you want to define the filter condition. In the Operator field, select an operator. In the Value/Attribute field, enter the value or another attribute based on which, the Operator is applied to the Attribute . e.g., If you want to filter events where the price is less than 100 (which is a value), select values for the fields as follows: Field Value Attribute price Operator < Value/Attribute (Value) 100 If you want to filter events where the price is equal to the volume (which is another attribute), select values for the fields as follows: Field Value Attribute price Operator == Value/Attribute (Attribute) volume Once you have defined two or more filters, enter the rule logic in the Rule Logic field using OR , AND , and NOT conditions. The examples of how you can use these keywords are explained in the table below. Keyword Example OR 1 OR 2 returns events that match either filter 1 or 2. AND 1 AND 2 returns events that match both filters 1 and 2. NOT NOT 1 returns events that do not match filter 1. Click Output to expand the Output section. Then select the rule template from which the sink and output configurations for the business rule must be selected. This displays the section for mapping configurations as shown in the example below. Select the relevant attribute names for the Input column. When publishing the events to which the rule is applied via the selected predefined sink, each input event you select is published with the corresponding name in the Output column. Info The output mappings displayed differ based on the output rule template you select. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy .","title":"Creating a business rule from scratch"},{"location":"admin/creating-business-rules-templates/#managing-business-rules","text":"Once you have created one or more business rules, you can manage them by viewing, editing, deploying, undeploying, and deleting them as required.","title":"Managing Business Rules"},{"location":"admin/creating-business-rules-templates/#viewing-business-rules","text":"Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. To view a business rule, click the icon for viewing (marked in the above image) for the relevant row. This opens the rule as shown in the example below.","title":"Viewing business rules"},{"location":"admin/creating-business-rules-templates/#editing-business-rules","text":"Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. To edit a business rule, click the icon for editing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. Modify values for the parameters displayed as required and click Save .","title":"Editing business rules"},{"location":"admin/creating-business-rules-templates/#deploying-business-rules","text":"Before you begin: Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing one of the following commands: - On Windows: server.bat --run - On Linux/Mac OS: ./server.sh To deploy a business rule that you have previously saved, click the icon for deploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully deployed.","title":"Deploying business rules"},{"location":"admin/creating-business-rules-templates/#undeploying-business-rules","text":"To undeploy a business rule, click the icon for undeploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully undeployed.","title":"Undeploying business rules"},{"location":"admin/creating-business-rules-templates/#viewing-deployment-information","text":"If you want to view information relating to the deployment of a business rule, click the icon for viewing deployment information (marked in the image below) for the relevant row. As a result, the deployment information including the host and port of the nodes in which the rule is deployed and the deployment status are displayed as shown in the image below. Possible deployment statuses are as follows: Saved : The business rule is created, but not yet deployed in any Streaming Integrator node. Deployed : The business rule is created and deployed in all the required nodes in the Streaming Integrator cluster. Partially Deployed: The business rule is created and deployed only in some of the required nodes in the Streaming Integrator cluster. Partially Undeployed: The business rule has been previously deployed, and then un-deployed only in some of the nodes in the Streaming Integrator cluster. Info Required nodes are configured with respect to a rule template. For detailed instructions, see Deploying business rules in SI server .","title":"Viewing deployment information"},{"location":"admin/creating-business-rules-templates/#deleting-business-rules","text":"To delete a business rule, click the icon for deleting (marked in the image below) for the relevant row. A message appears to confirm whether you want to proceed with the deletion. Click Delete in the message. As a result, another message appears to inform you that the rule is successfully deleted.","title":"Deleting business rules"},{"location":"admin/creating-business-rules-templates/#creating-a-business-rules-template","text":"To create a business template using the Business Rules Template editor, follow the procedure below: If you have not already started the Streaming Integrator tooling, navigate to the <SI_TOOLING_HOME>/bin directory from the terminal and start the Streaming Integrator Tooling as mentioned below. On Windows: tooling.bat --run On Linux/Mac OS: ./ tooling.sh Access the Business Rules Template Editor via the URL that appears for it in the start up logs as shown in the example below. Info The default URL is http://localhost:9390/template-editor . The Template Editor opens as shown below. There are two views from which you can interact and create a template group. Design view allows you to visualize a template group and interact with it. Code view allows you to interact with a template group by typing content. (For more information about template group structure, see Business Rules Templates .) Warning Do not template sensitive information such as passwords in a Siddhi application or expose them directly in a Siddhi application. For detailed instructions to protect sensitive data by obfuscating them, see Protecting Sensitive Data via the Secure Vault . You can create a template group using the design view or the code view as explained in the following sections.","title":"Creating a Business Rules Template"},{"location":"admin/creating-business-rules-templates/#create-from-design-view","text":"To create a business rules template group from the design view, follow the procedure below: Enter a UUID (Universally Unique Identifier), name and a description for the template group as follows. Field Name UUID sweet-factory Name Sweet Factory Description Analyzes Sweet Factory scenarios Expand the first rule template that exists by default, and enter the following details. Field Name Value UUID identifying-continuous-production-decrease Name Identify Continuous Production Decrease Description Alert factory managers if the rate of production continuously decreases for a specified time period Type Template Instance Count One To include a Siddhi application template, expand the first template that is displayed by default, and enter the following Siddhi application template. @App:name('SweetFactory-TrendAnalysis') @source(type='http', @map(type='json')) define stream SweetProductionStream (name string, amount double, factoryId int); @sink(type='log', @map(type='text', @payload(\"\"\" Hi ${username}, Production at Factory {{factoryId}} has gone from {{initalamout}} to {{finalAmount}} in ${timeInterval} seconds!\"\"\"))) define stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double); from SweetProductionStream#window.timeBatch(${timeInterval} sec) select factoryId, sum(amount) as amount, currentTimeMillis() as ts insert into ProdRateStream; partition with ( factoryId of ProdRateStream ) begin from every e1=ProdRateStream, e2=ProdRateStream[ts - e1.ts <= ${timeRange} and e1.amount > amount ]*, e3=ProdRateStream[ts - e1.ts >= ${timeRange} and e1.amount > amount ] select e1.factoryId, e1.ts as initaltime, e3.ts as finalTime, e1.amount as initalamout, e3.amount as finalAmount insert into ContinousProdReductionStream; end; To add variable attributes to the script, click Add Variables . Info A script is a javascript that can be applied when the inputs provided by the business user who uses the template need to be processed before replacing the values for the template variables. e.g., If the average value is not provided, a function within the script can derive it by calculating it from the minimum value and the maximum value provided by the business user. To specify the attributes that need to be considered as variables, select the relevant check boxes under Select templated elements . In this example, you can select the username and timeRange check boxes to to select the attributes with those names as the variables. Then click Add To Script to update the script with the selected variables with auto-generated function bodies as shown below. Edit the script to add the required functions. In this example, let's rename myFunction1(input) to getUsername(email) , and myFunction2(input) to validateTimeRange(number) . var username = getUsername('${userInputForusername}'); var timeRange = validateTimeRange('${userInputFortimeRange}'); /** * Extracts the username from given email * @returns Extracted username * @param email Provided email */ function getUsername(email) { if (email.match(/\\S+@\\S+/g)) { if (email.match(/\\S+@\\S+/g)[0] === email) { return email.split('@')[0]; } throw 'Invalid email address provided'; } throw 'Invalid email address provided'; } /** * Validates the given value for time range * @returns Processed input * @param input User given value */ function validateTimeRange(number) { if (!isNaN(number) && (number > 0)) { return number; } else { throw 'A positive number expected for time range'; } } To generate properties, click Generate against Properties . This expands the Properties section as follows. Enter values for the available properties as follows. For this example, let's enter values as shown in the following table. Info A property is defined for each templated attribute (defined in the ${templatedElement} format), so that it is self descriptive for the business user who uses the template. The values configured for each property are as follows: Field Name : The name with which the templated attribute is displayed to the business user. Field Description : A description of the property for the business user to understand its purpose. Default Value : The value assigned to the property by default. The business user can change this value if required. Options : this is an optional configuration that allows you to define a set of values for a property so that the business user can select the required value from a list. This is useful when the possible value for the property is limited to a set of options. Property Field Name Field Description Default Value timeInterval Time interval (in seconds) Production amounts are considered per time interval 6 userInputForusername Manager Email ID Email address to show in greeting example@email.com userInputFortimeRange Time Range (in milliseconds) Time period in which, product amounts are analyzed for decrease 5 To save the template, click the save icon at the top of the page.","title":"Create from Design View"},{"location":"admin/creating-business-rules-templates/#create-from-code-view","text":"When you use the code view, the same parameters for which you enter values in the design view are represented as JSON keys. For each parameter, you can specify a value against the relevant JSON key as shown in the extract below. When you update the code view with a valid template group definition, the design view is updated simultaneously as shown below. However, if the content you enter in the code view is an invalid template group, the design view is not updated, and an error is displayed as follows. When an error is detected in the entered template group structure, the Recover button is displayed with the error message. When you click Recover , the code view is reset to the latest detected valid template group definition. At any given time, the design view displays information based on the latest detected valid template group definition. Info It is not recommended to add Siddhi application templates and scripts using the code view because they need to be provided as a single line, and the possible escape characters should be handled carefully.","title":"Create from code view"},{"location":"admin/creating-business-rules-templates/#editing-a-business-rules-template","text":"WSO2 SI allows you to make edits to a business rules template that you have already created and saved. To edit a template via the Template Editor tool, follow the steps below. Start the WSO2 SI Tooling profile by issuing one of the following commands. For Windows: tooling.bat For Linux: ./tooling.sh Access the Template Editor via the URL that appears for it in the start up logs as shown in the example below. Info The default URL is http://localhost:9390/template-editor . The Template Editor opens as follows. To open an existing template, click the Open icon in the top panel (marked in the image above). In the Open Template File dialog box, click Choose File and browse for the required template. Once you have selected the template, click Load to open it in the Template Editor. Edit the template as required. You can update it in the Design View or the Source View as you prefer. For more information, see Creating a Business Rule Template . Save your edits by clicking the Save icon in the top panel.","title":"Editing a Business Rules Template"},{"location":"admin/creating-business-rules-templates/#business-rules-templates","text":"Rule Templates are used as specifications to gain inputs from users, through dynamically generated fields for the purpose of creating business rules. A template group is a business domain level grouping. The definition of a template looks as follows. { \"templateGroup\" : { \"name\" : \"<Name of the template group>\", \"uuid\":\"<UUID for the template group>\", \"description\" : \"<(Optional) description for the template group>\", \"ruleTemplates\" : [ { \"name\" : \"<Name of the rule template>\" , \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"template\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp_1 with ${templatedProperty_x}>\" }, { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp_n with ${templatedProperty_y}>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\"}, \"templatedProperty_y\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } }, { \"name\" : \"<Name of the rule template>\", \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"input\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp with ${templatedProperty_x}>\", \"exposedStreamDefinition\" :\"<Exposed stream definition>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } }, { \"name\" : \"<Name of the rule template>\", \"uuid\" : \"<UUID for the rule template>\", \"type\" : \"output\", \"instanceCount\" : \"one <or> many\", \"description\" : \"<(Optional) description for the rule template>\", \"script\" : \"<(Optional) Javascript with reference to the properties>\", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \"<SiddhiApp with ${templatedProperty_x}>\", \"exposedStreamDefinition\" :\"<Exposed stream definition>\" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \"<Field name for the property>\", \"description\" : \"<Description for the property>\", \"defaultValue\" : \"<Default value for the property>\", \"options\" : [\"<option_1>\", \"<option_n>\"]} } } ] } } The following parameters are configured:","title":"Business Rules Templates"},{"location":"admin/creating-business-rules-templates/#template-group-basic-data","text":"The following parameters are configured under templateGroup . Parameter Description Required/Optional name A name for the template group Required uuid A uniquely identifiable id for the template group Required description A description for the template. Optional","title":"Template Group basic data"},{"location":"admin/creating-business-rules-templates/#rule-template-details","text":"Multiple rule templates can be defined under a templateGroup . For each ruleTemplate , the following set of parameters need to be configured: Parameter Description Required/Optional name A name for the rule template Required uuid A uniquely identifiable id for the rule template Required type The type of the rule template. Possible values are as follows: template: Used only to create an entire business rule from template input : Used only in creating a business rule from scratch output : Used only in creating a business rule from scratch Required instanceCount This specifies whether the business rules derived from the template can be deployed only on one node, or whether they can be deployed on many nodes. Possible values are as follows: one many Required script The Java script to be executed on the templated fields. Developers can use this script for: validating purposes. deriving values for a templated parameter by combining some other entered parameters You need to mention each templated element that needs to be derived from entered parameters as a variable in the global scope of the javascript. You also need to template the entered parameters in the script itself. These values are later replaced with their respective entered values. Consider the following script /* * Validates a number and returns after adding 10 to it * @throws Error when a non number is entered */ function deriveValue (value) { if ( ! isNan (value) ) { return value + 10 ; } throw \"A number is required\" ; } var derivedValue = deriveValue ($ { enteredValue } ) ; enteredValue should be defined as a property under properties in order to be filled by the user and replaced later. The derived value stored in derivedValue is then used to replace ${derivedValue} in the SiddhiApp template. Optional description A brief description of the rule template. Optional templates These are the artifacts (i.e SiddhiApps) with templated parameters that are instantiated with replaced values when a business rule is created. Required properties You can add a field name, description, default value and possible values (optional) for the templated parameters. Required","title":"Rule Template details"},{"location":"admin/creating-business-rules-templates/#deploying-business-rules-in-si-server","text":"To deploy a business rule in the Streaming Integrator server, follow the procedure below. Before you begin: Both the Streaming Integrator server(s) and Streaming Integrator tooling must be up and running. Save the template group you created as a .json file in the <SI_TOOLING_HOME>/wso2/server/resources/businessRules/templates directory. In the BusinessRules section of the <SI_TOOLING_HOME>/conf/server/deployment.yaml file, add a configuration for the template you created as shown below. wso2.business.rules.manager: datasource: <datasourceName> - nodeURL1: - ruleTemplateUUID1 - ruleTemplateUUID2 nodeURL2: - ruleTemplateUUID1 - ruleTemplateUUID2 Info Specify the value for nodeURL1 in the IP:Port format. If you add this configuration, a business rule which is derived from this rule template (when you run the Streaming Integrator Tooling and the Streaming Integrator servers of your SI setup) is deployed only in the nodes under which - this rule template has been specified. If you do not specifically add business rule template IDs in the configuration, business rules are deployed in all the available Streaming Integrator servers.","title":"Deploying business rules in SI server"},{"location":"admin/creating-business-rules-templates/#configuring-business-rules-manager-permissions","text":"There are two permission levels for a business rules application: Manager : User roles with this permission level have administrative privileges over business rules. They are allowed to create, view, edit, deploy or delete business rules. Viewer : User roles with this permission level are only allowed to view business rules. This section covers how to configure Business Rules Manager permissions. Before you begin: Before configuring Business Rules Manager permissions, the user roles to be assigned permissions must be already defined in the user store with the required user IDs. For detailed instructions, see User Management . You need to define the roles related to the Business Rules Manager under the wso2.business.rules.manager component namespace in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. The following is a sample configuration of user roles for the Business Rules Manager. wso2.business.rules.manager: roles: manager: - name: role1 id: 1 viewer: - name: role2 id: 2","title":"Configuring Business Rules Manager Permissions"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/","text":"Downloading and Installing Siddhi Extensions \u00b6 The Siddhi extensions supported for the Streaming Integrator are shipped with the product by default. If you need to download and install a different version of an extension, it can be downloaded from the Siddhi Extensions Store . To download and install an Siddhi extension, follow the sections below. Downloading Siddhi extensions \u00b6 To download the Siddhii extensions, follow the steps below Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. Click on the required extension. In this example, let's click on the IBM MQ extension. In the dialog box that appears, enter your e-mail address and click Submit . The extension JAR is downloaded to the default location in your machine (based on your settings). If you are not using the latest version of the Streaming Integrator, and you want to select the version of the extension that matches your current product version, expand Version Support in the left navigator for the selected extension. Tip Each extension has a separate Version Support navigator item for the Streaming Integrator, SP, CEP and DAS. If you need to download an older version of an extension, follow the substeps below. Once you have clicked on the required extension, click on the Older Versions tab. Then click on the link displayed within the tab. You are directed to the maven central page where all the available versions of the extension are listed. Click on the relavent version. It directs you to the download page. To download the bundle, click on it. Installing Siddhi extensions \u00b6 To install the Siddhi extension in your Streaming Integrator pack, place the extension JAR you downloaded in the <SI_HOME>/lib directory.","title":"Downloading and Installing Siddhi Connectors"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#downloading-and-installing-siddhi-extensions","text":"The Siddhi extensions supported for the Streaming Integrator are shipped with the product by default. If you need to download and install a different version of an extension, it can be downloaded from the Siddhi Extensions Store . To download and install an Siddhi extension, follow the sections below.","title":"Downloading and Installing Siddhi Extensions"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#downloading-siddhi-extensions","text":"To download the Siddhii extensions, follow the steps below Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. Click on the required extension. In this example, let's click on the IBM MQ extension. In the dialog box that appears, enter your e-mail address and click Submit . The extension JAR is downloaded to the default location in your machine (based on your settings). If you are not using the latest version of the Streaming Integrator, and you want to select the version of the extension that matches your current product version, expand Version Support in the left navigator for the selected extension. Tip Each extension has a separate Version Support navigator item for the Streaming Integrator, SP, CEP and DAS. If you need to download an older version of an extension, follow the substeps below. Once you have clicked on the required extension, click on the Older Versions tab. Then click on the link displayed within the tab. You are directed to the maven central page where all the available versions of the extension are listed. Click on the relavent version. It directs you to the download page. To download the bundle, click on it.","title":"Downloading Siddhi extensions"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#installing-siddhi-extensions","text":"To install the Siddhi extension in your Streaming Integrator pack, place the extension JAR you downloaded in the <SI_HOME>/lib directory.","title":"Installing Siddhi extensions"},{"location":"admin/general-data-protection-regulations/","text":"General Data Protection Regulations (GDPR) for Streaming Integrator \u00b6 The General Data Protection Regulation (GDPR) is a new legal framework formalized by the European Union (EU) in 2016. This regulation is effective since 28, May 2018, and can affect any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our article series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here . Removing personally identifiable information via the Forget-me tool \u00b6 In the Streaming Integrator, streams specify the schema for events to be selected into the streaming integration event flow to be processed. This schema can include user IDs and other PII (Personally Identifiable Information) that you want to delete from log files and such. This can be done via the Forget-me Tool . Step 1: Configure the config.json file The <SI_HOME>/wso2/tools/identity-anonymization-tool-x.x.x/conf/config.json file specifies the locations from which persisted data needs to be removed. The log-file processor is specified in the configuration file of the Forget-Me tool as shown on the sample below in order to remove data with PII from the logs. If you have configured logs with PII to be saved in another location, you can add it to this list of processors. { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"logs\", \"log-file-name-regex\" : \"(.)*\" } ] } This extract shows the default configuration of the Streaming Integrator. The Streaming Integrator only saves PII in log files by default. Therefore, this configuration allows the Forget-me tool to delete these logs that are saved in the <SI_HOME>/wso2/server/logs directory. Step 2: Execute the Forget-me tool To execute the Forget-me tool, issue the following command pointing to the <SP_HOME> directory. forget-me -U <USERNAME> -d <CONF_DIR> -carbon <SP_HOME> Removing references to deleted user identities \u00b6 This section covers how to remove references to deleted user identities in the Streaming Integrator by running the Forget-me tool . Before you begin: Note that this tool is designed to run in offline mode (i.e., the server should be shut down or run on another machine) in order to prevent unnecessary load to the server. If this tool runs in online mode (i.e., when the server is running), DB lock situations on the H2 databases may occur. If you have configured any JDBC database other than the H2 database provided by default, copy the relevant JDBC driver to the <SP_HOME>/wso2/tools/identity-anonymization-tool/lib directory. Open a new terminal window and navigate to the <SP_HOME>/bin directory. Execute one of the following commands depending on your operating system: On Linux/Mac OS: ./forgetme.sh -U <username> On Windows: forgetme.bat -U <username> Note The commands specified above use only the -U <username> option, which is the only required option to run the tool. There are several other optional command line options that you can specify based on your requirement. The supported options are described in detail below. Command Line Option Description Required Sample Value U The name of the user whose identity references you want to remove. Yes -U john.doe d The configuration directory to use when the tool is run. If you do not specify a value for this option, the <SP_HOME>/wso2/tools/identity-anonymization-tool-x.x.x/conf directory (which is the default configuration directory of the tool) is used. No -d <TOOL_HOME>/conf T The tenant domain of the user whose identity references you want to remove. If you specify a tenant domain via this option, use the TID option to specify the ID of which the references must be removed. No -T acme-company The default value is carbon.super TID The tenant ID of the user whose identity references you want to remove. It is required to specify a tenant ID if you have specified a tenant domain via the TID option. No -TID 2346 D The user store domain name of the user whose identity references you want to remove. No -D Finance-Domain The default value is PRIMARY . pu The pseudonym with which the user name of the user whose identity references you want to remove should be replaced. If you do not specify a pseudonym when you run the tool, a random UUID value is generated as the pseudonym by default. No -pu \u201c123-343-435-545-dfd-4\u201d carbon The CARBON HOME. This should be replaced with the variable $CARBON_HOME in directories configured in the main configuration file. No -carbon \u201c/usr/bin/wso2sp/wso2sp4.1.0 Creating GDPR compliant Siddhi applications \u00b6 The obfuscation/removal of such PII (Personally Identifiable Information) can be handled in the Streaming Integrator via Siddhi Applications that can either modify or remove records that contain the PII. These Siddhi Applications can be written in a way to match the original queries that captured data for persistence so that the same data can be modified or removed as required. For more information about writing Siddhi Queries, see Siddhi Query Guide . The following sections explain how obfuscation/deletion of sensitive data can be managed via Siddhi queries in a custom Siddhi application developed based on a specific user case. Obfuscating PII Deleting PII Obfuscating PII \u00b6 Let's consider a Siddhi application that includes the following store definition to persist streaming data. define table customerTable (customerId string, customerName string, entryVector int); In this example, the customer ID is considered PII, and a customer with the XXX ID wants that ID to be hidden in the system so that he/she cannot be personally identified with it. Therefore, you need to obfuscate the value for the customerId attribute. This can be done by creating an algorithm to create a hashed value or a pseudonym to replace a specific value for the customerId attribute. Let's consider that such an algorithm exists (e.g., as a function named anonymize ). To invoke this function, you need to add a new query to the Siddhi application as shown in the sample below. define table customerTable (customerId string, customerName string, entryVector int); define stream UpdateStream (customerId string); from UpdateStream select * update customerTable set customerTable.customerName = anonymize(customerTable.customerName) on customerTable.customerId == XXX; In the above Siddhi application, the query in bold is triggered when a new event is received in the UpdateStream stream where the value for the customerId attribute is XXX . Once it is triggered, the XXX customer ID is replaced with a pseudonym. For more information about writing custom functions, see Siddhi Query Guide - Writing Custom Extensions . Deleting PII \u00b6 Let's assume that the customer ID in the scenario described above needs to be deleted. To do this, you can write a Siddhi query to delete the value for the customerId attribute when is equal to XXX as shown below. define table customerTable (customerId string, customerName string, entryVector int); define stream DeleteStream (customerId string); from DeleteStream delete customerTable on customerTable.customerId == customerId; In the above Siddhi application, the query in bold is triggered when a new event is received in the DeleteStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is deleted. For more information about the Delete operator used here, see Siddhi Query Guide - Delete . Forget-me tool overview \u00b6 The Forget-me tool is shipped with the Streaming Integrator by default in the <SI_HOME>/wso2/tools/identity-anonymization-tool-x.x.x directory. If required, you can change the default location of the configurations of this tool or make changes to the default configurations. You can also run the Forget-me tool in the standalone mode. Changing the default configurations location \u00b6 You can change the default location of the tool configurations if required. You may want to do this if you are working with a multi-product environment where you want to manage configurations in a single location for ease of use. Note that this is optional . To change the default configurations location for the embedded tool, do the following: Open the forgetme.sh file found inside the <SI_HOME>/bin directory. The location path is the value given after -d within the following line. Modify the value after -d to change the location. Info The default location path is $CARBON_HOME/repository/components/tools/forget-me/conf . sh $CARBON_HOME/repository/components/tools/identity-anonymization-tool/bin/forget-me -d $CARBON_HOME/repository/components/tools/identity-anonymization-tool/conf -carbon $CARBON_HOME $@ Changing the default configurations of the tool \u00b6 All configurations related to this tool can be found inside the <SI_HOME>/wso2/tools/identity-anonymization-tool/conf directory. The default configurations are set up as follows: Read Logs : <SI_HOME>/wso2/server/logs , <SI_TOOLING_HOME>/wso2/server/logs Read Datasource : <SI_HOME>/conf/server/deployment.yaml file, <SI_TOOLING_HOME>/conf/server/deployment.yaml file Default datasources : WSO2_CARBON_DB, WSO2_METRICS_DB , WSO2_PERMISSIONS_DB , WSO2_DASHBOARD_DB , BUSINESS_RULES_DB , SAMPLE_DB , WSO2_STATUS_DASHBOARD_DB Log file name regex : The regex patterns defined in all the files in the <SI_HOME>/wso2/tools/identity-anonymization-tool/conf/log-config directory are considered. For information on changing these configurations, see Configuring the config.json file in the Product Administration Guide. Running the Forget-me tool in the standalone mode \u00b6 This tool can run standalone and therefore cater to multiple products. This means that if you are using multiple WSO2 products and need to delete the user's identity from all products at once, you can do so by running the tool in standalone mode. For information on how to build and run the Forget-Me tool, see Removing References to Deleted User Identities in WSO2 Products in the WSO2 Administration Guide.","title":"General Data Protection Regulations"},{"location":"admin/general-data-protection-regulations/#general-data-protection-regulations-gdpr-for-streaming-integrator","text":"The General Data Protection Regulation (GDPR) is a new legal framework formalized by the European Union (EU) in 2016. This regulation is effective since 28, May 2018, and can affect any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our article series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here .","title":"General Data Protection Regulations (GDPR) for Streaming Integrator"},{"location":"admin/general-data-protection-regulations/#removing-personally-identifiable-information-via-the-forget-me-tool","text":"In the Streaming Integrator, streams specify the schema for events to be selected into the streaming integration event flow to be processed. This schema can include user IDs and other PII (Personally Identifiable Information) that you want to delete from log files and such. This can be done via the Forget-me Tool . Step 1: Configure the config.json file The <SI_HOME>/wso2/tools/identity-anonymization-tool-x.x.x/conf/config.json file specifies the locations from which persisted data needs to be removed. The log-file processor is specified in the configuration file of the Forget-Me tool as shown on the sample below in order to remove data with PII from the logs. If you have configured logs with PII to be saved in another location, you can add it to this list of processors. { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"logs\", \"log-file-name-regex\" : \"(.)*\" } ] } This extract shows the default configuration of the Streaming Integrator. The Streaming Integrator only saves PII in log files by default. Therefore, this configuration allows the Forget-me tool to delete these logs that are saved in the <SI_HOME>/wso2/server/logs directory. Step 2: Execute the Forget-me tool To execute the Forget-me tool, issue the following command pointing to the <SP_HOME> directory. forget-me -U <USERNAME> -d <CONF_DIR> -carbon <SP_HOME>","title":"Removing personally identifiable information via the Forget-me tool"},{"location":"admin/general-data-protection-regulations/#removing-references-to-deleted-user-identities","text":"This section covers how to remove references to deleted user identities in the Streaming Integrator by running the Forget-me tool . Before you begin: Note that this tool is designed to run in offline mode (i.e., the server should be shut down or run on another machine) in order to prevent unnecessary load to the server. If this tool runs in online mode (i.e., when the server is running), DB lock situations on the H2 databases may occur. If you have configured any JDBC database other than the H2 database provided by default, copy the relevant JDBC driver to the <SP_HOME>/wso2/tools/identity-anonymization-tool/lib directory. Open a new terminal window and navigate to the <SP_HOME>/bin directory. Execute one of the following commands depending on your operating system: On Linux/Mac OS: ./forgetme.sh -U <username> On Windows: forgetme.bat -U <username> Note The commands specified above use only the -U <username> option, which is the only required option to run the tool. There are several other optional command line options that you can specify based on your requirement. The supported options are described in detail below. Command Line Option Description Required Sample Value U The name of the user whose identity references you want to remove. Yes -U john.doe d The configuration directory to use when the tool is run. If you do not specify a value for this option, the <SP_HOME>/wso2/tools/identity-anonymization-tool-x.x.x/conf directory (which is the default configuration directory of the tool) is used. No -d <TOOL_HOME>/conf T The tenant domain of the user whose identity references you want to remove. If you specify a tenant domain via this option, use the TID option to specify the ID of which the references must be removed. No -T acme-company The default value is carbon.super TID The tenant ID of the user whose identity references you want to remove. It is required to specify a tenant ID if you have specified a tenant domain via the TID option. No -TID 2346 D The user store domain name of the user whose identity references you want to remove. No -D Finance-Domain The default value is PRIMARY . pu The pseudonym with which the user name of the user whose identity references you want to remove should be replaced. If you do not specify a pseudonym when you run the tool, a random UUID value is generated as the pseudonym by default. No -pu \u201c123-343-435-545-dfd-4\u201d carbon The CARBON HOME. This should be replaced with the variable $CARBON_HOME in directories configured in the main configuration file. No -carbon \u201c/usr/bin/wso2sp/wso2sp4.1.0","title":"Removing references to deleted user identities"},{"location":"admin/general-data-protection-regulations/#creating-gdpr-compliant-siddhi-applications","text":"The obfuscation/removal of such PII (Personally Identifiable Information) can be handled in the Streaming Integrator via Siddhi Applications that can either modify or remove records that contain the PII. These Siddhi Applications can be written in a way to match the original queries that captured data for persistence so that the same data can be modified or removed as required. For more information about writing Siddhi Queries, see Siddhi Query Guide . The following sections explain how obfuscation/deletion of sensitive data can be managed via Siddhi queries in a custom Siddhi application developed based on a specific user case. Obfuscating PII Deleting PII","title":"Creating GDPR compliant Siddhi applications"},{"location":"admin/general-data-protection-regulations/#obfuscating-pii","text":"Let's consider a Siddhi application that includes the following store definition to persist streaming data. define table customerTable (customerId string, customerName string, entryVector int); In this example, the customer ID is considered PII, and a customer with the XXX ID wants that ID to be hidden in the system so that he/she cannot be personally identified with it. Therefore, you need to obfuscate the value for the customerId attribute. This can be done by creating an algorithm to create a hashed value or a pseudonym to replace a specific value for the customerId attribute. Let's consider that such an algorithm exists (e.g., as a function named anonymize ). To invoke this function, you need to add a new query to the Siddhi application as shown in the sample below. define table customerTable (customerId string, customerName string, entryVector int); define stream UpdateStream (customerId string); from UpdateStream select * update customerTable set customerTable.customerName = anonymize(customerTable.customerName) on customerTable.customerId == XXX; In the above Siddhi application, the query in bold is triggered when a new event is received in the UpdateStream stream where the value for the customerId attribute is XXX . Once it is triggered, the XXX customer ID is replaced with a pseudonym. For more information about writing custom functions, see Siddhi Query Guide - Writing Custom Extensions .","title":"Obfuscating PII"},{"location":"admin/general-data-protection-regulations/#deleting-pii","text":"Let's assume that the customer ID in the scenario described above needs to be deleted. To do this, you can write a Siddhi query to delete the value for the customerId attribute when is equal to XXX as shown below. define table customerTable (customerId string, customerName string, entryVector int); define stream DeleteStream (customerId string); from DeleteStream delete customerTable on customerTable.customerId == customerId; In the above Siddhi application, the query in bold is triggered when a new event is received in the DeleteStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is deleted. For more information about the Delete operator used here, see Siddhi Query Guide - Delete .","title":"Deleting PII"},{"location":"admin/general-data-protection-regulations/#forget-me-tool-overview","text":"The Forget-me tool is shipped with the Streaming Integrator by default in the <SI_HOME>/wso2/tools/identity-anonymization-tool-x.x.x directory. If required, you can change the default location of the configurations of this tool or make changes to the default configurations. You can also run the Forget-me tool in the standalone mode.","title":"Forget-me tool overview"},{"location":"admin/general-data-protection-regulations/#changing-the-default-configurations-location","text":"You can change the default location of the tool configurations if required. You may want to do this if you are working with a multi-product environment where you want to manage configurations in a single location for ease of use. Note that this is optional . To change the default configurations location for the embedded tool, do the following: Open the forgetme.sh file found inside the <SI_HOME>/bin directory. The location path is the value given after -d within the following line. Modify the value after -d to change the location. Info The default location path is $CARBON_HOME/repository/components/tools/forget-me/conf . sh $CARBON_HOME/repository/components/tools/identity-anonymization-tool/bin/forget-me -d $CARBON_HOME/repository/components/tools/identity-anonymization-tool/conf -carbon $CARBON_HOME $@","title":"Changing the default configurations location"},{"location":"admin/general-data-protection-regulations/#changing-the-default-configurations-of-the-tool","text":"All configurations related to this tool can be found inside the <SI_HOME>/wso2/tools/identity-anonymization-tool/conf directory. The default configurations are set up as follows: Read Logs : <SI_HOME>/wso2/server/logs , <SI_TOOLING_HOME>/wso2/server/logs Read Datasource : <SI_HOME>/conf/server/deployment.yaml file, <SI_TOOLING_HOME>/conf/server/deployment.yaml file Default datasources : WSO2_CARBON_DB, WSO2_METRICS_DB , WSO2_PERMISSIONS_DB , WSO2_DASHBOARD_DB , BUSINESS_RULES_DB , SAMPLE_DB , WSO2_STATUS_DASHBOARD_DB Log file name regex : The regex patterns defined in all the files in the <SI_HOME>/wso2/tools/identity-anonymization-tool/conf/log-config directory are considered. For information on changing these configurations, see Configuring the config.json file in the Product Administration Guide.","title":"Changing the default configurations of the tool"},{"location":"admin/general-data-protection-regulations/#running-the-forget-me-tool-in-the-standalone-mode","text":"This tool can run standalone and therefore cater to multiple products. This means that if you are using multiple WSO2 products and need to delete the user's identity from all products at once, you can do so by running the tool in standalone mode. For information on how to build and run the Forget-Me tool, see Removing References to Deleted User Identities in WSO2 Products in the WSO2 Administration Guide.","title":"Running the Forget-me tool in the standalone mode"},{"location":"admin/introduction-to-User-Management/","text":"Introduction to User Management \u00b6 User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components . Users \u00b6 Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed. Permission \u00b6 A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically. User Roles \u00b6 A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually. User Store \u00b6 A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\<SP_HOME>/conf/\\<PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled.","title":"Introduction to User Management"},{"location":"admin/introduction-to-User-Management/#introduction-to-user-management","text":"User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components .","title":"Introduction to User Management"},{"location":"admin/introduction-to-User-Management/#users","text":"Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed.","title":"Users"},{"location":"admin/introduction-to-User-Management/#permission","text":"A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically.","title":"Permission"},{"location":"admin/introduction-to-User-Management/#user-roles","text":"A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually.","title":"User Roles"},{"location":"admin/introduction-to-User-Management/#user-store","text":"A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\<SP_HOME>/conf/\\<PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled.","title":"User Store"},{"location":"admin/monitoring-the-streaming-integrator/","text":"Monitoring the Streaming Integrator \u00b6 The Status Dashboard allows you to monitor the metrics of a stand-alone Streaming Integrator instance or a Streaming Integrator cluster. This involves monitoring whether all processes of the Streaming Integrator setup are working in a healthy manner, monitoring the current status of a Streaming Integrator node, and viewing metrics relating to the history of a node or the cluster. Both JVM level metrics or Siddhi application level metrics can be viewed from the Status Dashboard. The following sections cover how to configure the Status Dashboard and analyze statistics relating to your Streaming Integrator deployment in it. Configuring the Status Dashboard \u00b6 The following sections cover the configurations that need to be done in order to view statistics relating to the performance of your Streaming Integrator deployment in the Status Dashboard. Assigning unique carbon IDs to nodes \u00b6 Carbon metrics uses the carbon ID as the source ID for metrics. Therefore, all the worker nodes are required to have a unique carbon ID defined in the wso2.carbon: section of the <SI_HOME>/conf/server/deployment.yaml file as shown in the extract below. Info You need to ensure that the carbon ID of each node is unique because it is required for the Status dashboard to identify the worker nodes and display their statistics accordingly. wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # ports used by this server Setting up the database \u00b6 To monitor statistics in the Status Dashboard, you need a shared metrics database that stores the metrics of all the nodes. Set up a database of the required type by following the steps below. In this section, a MySQL database is created as an example. Info The Status Dashboard is only supported with H2, MySQL, MSSQL and Oracle database types. It is configured with the H2 database type by default. If you want to continue to use H2, skip this step. Download and install the required database type. For this example, let's download and install MySQL Server . Download the required database driver. For this example, download the MySQL JDBC driver . Unzip the database driver you downloaded, copy its JAR file ( mysql-connector-java-x.x.xx-bin.jar in this example), and place it in the <DASHBOARD_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Create two databases named WSO2_METRICS_DB (to store metrics data) and WSO2_STATUS_DASHBOARD_DB (to store statistics) with tables. To create MySQL databases and tables for this example, run the following commands. mysql> create database WSO2_METRICS_DB; mysql> use WSO2_METRICS_DB; mysql> source <SI_TOOLING_HOME>/wso2/server/dbscripts/metrics/mysql.sql; mysql> grant all on WSO2_METRICS_DB.* TO username@localhost identified by \"password\"; mysql> create database WSO2_STATUS_DASHBOARD_DB; mysql> use WSO2_STATUS_DASHBOARD_DB; mysql> source <SI_TOOLING_HOME>/wso2/server/dbscripts/metrics/mysql.sql; mysql> grant all on WSO2_STATUS_DASHBOARD_DB.* TO username@localhost identified by \"password\"; Create two datasources named WSO2_METRICS_DB and WSO2_STATUS_DASHBOARD_DB by adding the following datasource configurations under the wso2.datasources: section of the <SI_HOME>/conf/server/deployment.yaml file. Info The names of the data sources must be the same as the names of the database tables you created for metrics and statistics. You need to change the values for the username and password parameters to the username and password that you are using to access the MySQL database. For detailed information about datasources, see carbon-datasources . WSO2_METRICS_DB - name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_METRICS_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false WSO2_STATUS_DASHBOARD_DB - name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_STATUS_DASHBOARD_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following are sample configurations for database tables when you use other database types supported. Click here to view the sample data source configurations. Database Type Metrics Datasource Dashboard Datasource MSSQL name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:sqlserver://localhost;databaseName=wso2_metrics' username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:sqlserver://localhost;databaseName=monitoring' username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:oracle:thin:@localhost:1521/xe' username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:oracle:thin:@localhost:1521/xe' username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Configuring metrics \u00b6 This section explains how to configure metrics for your status dashboard. Configuring worker metrics To enable metrics and to configure metric-related properties, do the following configurations in the \\< SI_HOME>/conf/server/deployment.yaml file for the required nodes. To enable Carbon metrics, set the enabled property to true under wso2.metrics as shown below. wso2.metrics: enabled: true To enable JDBC reporting, set the Enable JDBC parameter to true in the wso2.metrics.jdbc: => reporting: subsection as shown below. If JDBC reporting is not enabled, only real-time metrics are displayed in the first page of the Status dashboard, and information relating to metrics history is not displayed in the other pages of the dashboard. To render the first entry of the graph, you need to wait for the time duration specified as the pollingPeriod . # Enable JDBC Reporter name: JDBC enabled: true pollingPeriod: 60 Under wso2.metrics.jdbc , configure the following properties to clean up the database entries. wso2.metrics.jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - &JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined in a *-datasources.xml file in conf/datasources directory. dataSourceName: java:comp/env/jdbc/WSO2MetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 Parameter Default Value Description dataSource &JDBC01 dataSource > dataSourceName java:comp/env/jdbc/WSO2MetricsDB The name of the datasource used to store metric data. dataSource > scheduledCleanup > enabled false If this is set to true , metrics data stored in the database is cleared at a specific time interval as scheduled. dataSource > scheduledCleanup > daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. dataSource > scheduledCleanup > scheduledCleanupPeriod 86400 This parameter specifies the time interval in seconds at which all metric data stored in the database must be cleared. JVM metrics of which the log level is set to OFF are not measured by default. If you need to monitor one or more of them, add the relevant metric name(s) under the wso2.metrics: => levels subsection as shown in the extract below. As shown below, you also need to mention log4j mode in which the metrics need to be monitored (i.e., OFF , INFO , DEBUG , TRACE , or ALL ). wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Click here to view the default metric levels supported... Class loading Property Garbage collector Property Memory Property Operating system load Property Threads | Property | Default Level | Description | |------------------------------------------------------------------------|----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | ` jvm.threads.count ` | ` Debug ` | The gauge for showing the number of active and idle threads currently available in the JVM thread pool. | | ` jvm.threads.daemon.count ` | ` Debug ` | The gauge for showing the number of active daemon threads currently available in the JVM thread pool. | | ` jvm.threads.blocked.count ` | ` OFF ` | The gauge for showing the number of threads that are currently blocked in the JVM thread pool. | | ` jvm.threads.deadlock.count ` | ` OFF ` | The gauge for showing the number of threads that are currently deadlocked in the JVM thread pool. | | ` jvm.threads.new.count ` | ` OFF ` | The gauge for showing the number of new threads generated in the JVM thread pool. | | ` jvm.threads.runnable.count ` | ` OFF ` | The gauge for showing the number of runnable threads currently available in the JVM thread pool. | | ` jvm.threads.terminated.count ` | ` OFF ` | The gauge for showing the number of threads terminated from the JVM thread pool since user started running the WSO2 API Manager instance. | | ` jvm.threads.timed_waiting.count ` | ` OFF ` | The gauge for showing the number of threads in the Timed\\_Waiting state. | | ` jvm.threads.waiting.count ` | ` OFF ` | The gauge for showing the number of threads in the Waiting state in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions. | File descriptor details Property Swap space Property Configuring Siddhi application metrics To enable Siddhi application level metrics for a Siddhi application, you need to add the @app:statistics annotation bellow the Siddhi application name in the Siddhi file as shown in the example below. @App:name('TestMetrics') @app:statistics(reporter = 'jdbc') define stream TestStream (message string); The following are the metrics measured for a Siddhi application. Info The default level after enabling metrics is INFO for all the meytrics listed in the following table. Metric Components to which the metric is applied Latency Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains) Throughput Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains ) Memory Queries (per query) Buffered Events Count Number of events at disruptor Streams (per stream) Number of events produced/received after restart Sources (per source) Sinks (per sink) Configuring cluster credentials \u00b6 In order to access the nodes in a cluster and derive statistics, you need to maintain and share a user name and a password for each node in a SI cluster. This user name and password must be specified in the <DASHBOARD_HOME>/conf/server/deployment.yaml file. If you want to secure sensitive information such as the user name and the password, you can encrypt them via WSO2 Secure Vault. To specify the user name and the password to access a node, define them under the wso2.status.dashboard section as shown in the following example. wso2.status.dashboard: workerAccessCredentials: username: 'admin' password: 'admin' To encrypt the user name and the password you defined, define aliases for them as described in Protecting Sensitive Data via the Secure Vault . Info This functionality is currently supported only for single tenant environments. Configuring permissions \u00b6 The following are the three levels of permissions that can be granted for the users of the Status Dashboard. Permission Level Granted permissions SysAdmin Enabling/disabling metrics Adding workers Deleting workers Viewing workers Developers Adding workers Deleting workers Viewing workers Viewers Viewing workers The admin user in the userstore is assigned the SysAdmin permission level by default. To assign different permission levels to different roles, you can list the required roles under the relevant permission level in the wso2.status.dashboard section of the DASHBOARD_HOME>/conf/dashboard/deployment.yaml file as shown in the extract below. wso2.status.dashboard: sysAdminRoles: - role_1 developerRoles: - role_2 viewerRoles: - role_3 Info The display name of the roles given in the configuration must be present in the user store. To configure user store check, User Management . Downloading and accessing the Status Dashboard \u00b6 To download and access the Status Dashboard, follow the procedure below: Download the Status Dashboard from TODO. Unzip the downloaded fine. The unzipped directory is referred to as <DASHBOARD_HOME> for the rest iof this section. In the terminal, navigate to the <DASHBOARD_HOME>/bin directory and issue the following command. For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost:<DASHBOARD_PORT>/si-status-dashboard e.g., https://0.0.0.0:9643/si-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. If no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Adding a node to the dashboard . Node overview \u00b6 Once you login to the status dashboard, the nodes that are already added to the Status Dashboard are displayed as shown in the following example: Adding a node to the dashboard \u00b6 If no nodes are displayed, you can add the nodes for which you want to view the status by following the procedure below: Click ADD NEW NODE . This opens the following dialog box. Enter the following information in the dialog box and click ADD NODE to add a gadget for the required node in the Node Overview page. In the Host parameter, enter the host ID of the node you want to add. In the Port parameter, enter the port number of the node you want to add. If the node you added is currently unreachable, the following dialog box is displayed. Click either WORKER or MANAGER. If you click WORKER , the node is displayed under Never Reached . If you click Manager , the node is displayed under Distributed Deployments as shown below. Info The following basic details are displayed for each node. CPU Usage : The CPU resources consumed by the Streaming Integrator node out of the available CPU resources in the machine in which it is deployed is expressed as a percentage. Memory Usage : The memory consumed by the node as a percentage of the total memory available in the system. Load Average : Siddhi Apps : The total number of Siddhi applications deployed in the node. Viewing status details \u00b6 The following is a list of sections displayed in the Node Overview page to provide information relating to the status of the nodes. Distributed Deployments View (Example) Description The nodes that are connected in the distributed deployment are displayed under the relevant group ID in the status dashboard (e.g., sp in the above example). Both managers and workers are displayed under separate labels. Managers : The active manager node in the cluster is indicated by a green dot that is displayed with the host name and the port of the node. Similarly, a grey dot is displayed for passive manager nodes in the cluster. Workers : When you add an active manager node, it automatically retrieves the worker node details that are connected with that particular deployment. If the worker node is already registered in the Status Dashboard, you can view the metrics of that node as follows: Purpose To determine whether the request load is efficiently allocated between the nodes of a cluster. To determine whether the cluster has sufficient resources to handle the load of requests. To identify the nodes connected with the particular deployment. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Clustered nodes View (Example) Description The nodes that are clustered together in a high-availability deployment are displayed under the relevant cluster ID in the Status Dashboard (e.g., under WSO2_A_1 in the above example). The active node in the cluster (i.e., the active worker in a minimum HA cluster or the active manager in a fully distributed cluster) are indicated by a green dot that is displayed with the hostname and the port of the node. Similarly, a grey dot is displayed for passive nodes in the cluster. Purpose This allows you to determine the following: Whether the request load is efficiently allocated between the nodes of a cluster. Whether the cluster has sufficient resources to handle the load of requests. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Single nodes View (Example) Description This section displays statistics for Streaming Integrator servers that operate as single node setups. Purpose This allows you to compare the performance of single nodes agaisnt each other. Recommended Action If the CPU usage of a node is too high, investigate the causes for it and take corrective action (e.g., undeploy unnecessary Siddhi applications). If any underutilized single nodes are identified, you can either deploy more Siddhi applications thatare currrently deployed in other nodes with a high request load. Alternatively, you can redeploy the Siddhi applications of the underutilized node to other nodes, and then shut it down. Nodes that cannot be reached View (Example) Description When a node is newly added to the Status dashboard and it is unavailable, it is displayed as shown in the above examples. Purpose This allows you to identify nodes that cannot be reached at specific hosts and ports. Recommended Action Check whether the host and port of the node you added is correct. Check whether any authentication errors have occured for the node. Nodes that are currently unavailable View (Example) Description When a node that could be viewed previously is no longer available, its status is displayed in red as shown in the example above. The status displayed for such nodes is applicable for the last time at which the node had been reachable. Purpose This allows you to identify previously available nodes that have become unreachable. Recommended Action Check whether the node is inactive. Check whether any authentication errors have occured for the node. Nodes for which metrics is disabled View (Example) Description When a node for which metrics is disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which metrics is not enabled. Recommended Action Enable metrics for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable metrics, see Monitoring the Stream Processor - Configuring the Status Dashboard . Nodes with JMX reporting disabled View (Example) Description When a node with JMX reporting disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which JMX reporting is disabled Recommended Action Enable JMX reporting for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable JMX reporting, see Monitoring the Stream Processor - Configuring the Status Dashboard . Statistics trends View (Example) Description This dispalys the change that has taken taken place in the CPU usage, memory usage and the load average of nodes since the status was last viewed in the status dashboard. Positive changes are indicated in green (e.g., a decrease in the CPU usage in the above example), and egative changes are indicated in red (an increase in the memory usage and the load average in the above example). Purpose This allows you to view a summary of the performance trends of your Streaming Integrator clusters and single nodes. Recommended Action Based on the performance trend observed, add more resources to your Streaming Integrator clusters/single nodes to handle more events, or shutdown one or more nodes if there is excess resources. Viewing node-specific pages \u00b6 When you open the Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant widget. This opens a separate page for the worker node as shown in the example below. Status indicators \u00b6 The following gadgets can be viewed for the selected worker. Server General Details View (Example) Description This gadget displays general information relating to the selected worker node. Purpose This allows you to understand the distribution of nodes in terms of the location, the time zone, operating system used etc., and to locate them. Recommended Action In a distributed set up, you can use this information to evaluate the clustered setup and make changes to optimize the benefits of deploying the Streaming Integrator as a cluster (e.g., making them physically available in different locations to minimize the risk of all the nodes failing at the same time etc.). CPU Usage View (Example) Description This displays the CPU usage of the selected node. Purpose This allows you to observe the CPU usage of a selected node over time. Recommended Action Identify sudden slumps in the CPU usage, and investigate the reasons (e.g., such as authentication errors that result in requests not reaching the Streaming Integrator server). Identify continuous increases in the CPU usage and check whether the node is overloaded. If so, reallocate some of the Siddhi applications deployed in the node. Memory Used View (Example) Description This displays the memory usage of the selected node. Purpose This allows you to observe the memory usage of a selected node over time. Recommended Action Identify sudden slumps in the memory usage, and investigate the reasons (e.g., a reduction in the requests recived due to system failure). If there are continous increases in the memory usage, check whether there is an increase in the requests handled, and whether you have enough memory resources to handle the increased demand. If not, add more memory to the node or reallocate some of the Siddhi applications deployed in the node to other nodes. System Load Average View (Example) Description This displays the system load average for the selected node. Purpose This allows you to observe the system load of the node over time. Recommended Action Observe the trends of the node's system load, and adjust the allocation of resources (e.g., memory) and work load (i.e., the number of Siddhi applications deployed) accordingly. Overall Throughput View (Example) Description This displays the overall throughput of the selected node. Purpose This allows you to assess the performance of the selected node in terms of the throughput over time. Recommended Action Compare the throughput of the node against that of other nodes with the same amount of CPU and memory resources. If there are significant variations, investigate the causes (e.g., the differences in the number of requests received by different Siddhi applications deployed in the nodes). Observe changes in the throughput over time. If there are significant variances, investigate the causes (e.g., whether the node has been unavaialable to receive requests during a given time). Siddhi Applications View (Example) Description This table displays the complete list of Siddhi applications deployed in the selected node. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Age : The age of the Siddhi application in milliseconds. Latency : The time (in milliseconds) taken by the Siddhi application to process one request. Throughput: The number of requests processed by the Siddhi application since it has been active. Memory : The amount of memory consumed by the Siddhi application during its current active session, expressed in milliseconds. Purpose This allows you to assess the performance of each Siddhi application deployed in the selected node. Recommended Action Identify the inactive Siddhi applications that are required to be active and take the appropriate corrective action. Identify Siddhi applications that consume too much memory, and identify ways in which the memory usage can be optimized (e.g., use incremental processing). Viewing worker history \u00b6 This section explains how to view statistics relating to the performance of a selected node for a specific time interval. Log in to the Status Dashboard. In the Node Overview page, click on the required node to view information specific to that node. In the page displayed with node-specific information, click one of the following gadgets to open the Metrics page. CPU Usage Memory Used System Load Average Overall Throughput In the Metrics page, click the required time interval. Then the page displays statistics relating to the performance of the selected node applicable to that time period. If you want to view more details, click More Details . As a result, the following additional information is displayed for the node for the selected time period. CPU Usage JVM OS as CPU JVM Physical Memory JVM Threads JVM Swap Space Viewing statistics for Siddhi applications \u00b6 When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant gadget. This opens the page specific to the worker . To view information specific to a Siddhi application deployed in the Siddhi node, click on the relevant Siddhi application in the Siddhi Applications table. This opens a page with information specific to the selected Siddhi application as shown in the example below. The following statistics can be viewed for an individual Siddhi Application. Latency View (Example) Description This displays the latency of the selected Siddhi application. Latency is the time taken to complete processing a single event in the event flow. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the latency of the Siddhi application is too high, check the Siddhi queries and rewrite them to optimise performance. Overall Throughput View (Example) Description This shows the overall throughput of a selected Siddhi application over time. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the throughput of a Siddhi application varies greatly overtime, investigate reasons for any slumps in the throughput (e.g., errors in the deployment of the application). If the throughput of the Siddhi application is lower than expected, investigate reasons, and take corrective action to improve the throughput (e.g., check the Siddhi queries in the application and rewrite them with best practices to achieve greater efficiency in the processing of events. Memory Used View (Example) Description This displays the memory usage (In MB) of a selected Siddhi application over time. Purpose This allows you to monitor the memory consumption of individual Siddhi applications. Recommended Action If there are major fluctuations in the memory consumption of a Siddhi application, investigate the reasons (e.g., Whether the Siddhi application has been inactive at any point of time). Code View View (Example) Description This displays the queries defined in the Siddhi file of the application. Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of its latency, throughput and the memory consumption. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Creating a Siddhi Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Design View View (Example) Description This displays the graphical view for queries defined in the Siddhi file of the application. Purpose This allows you to check the flow of the queries of the Siddhi application in the graphical way. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. Siddhi App Component Statistics View (Example) Description This table displays performance statistics related to dfferent components within a selected Siddhi application (e.g., queries). The columns displayed are as follows: Type : The type of the Siddhi application component to which the information displayed in the row applies. The component type can be queries, streams, tables, windows and aggregations. For more information, see Siddhi Application Overview - Common components of a Siddhi application . Name : The name of the Siddhi component within the application to which the information displayed in the row apply. Metric Type : The metric type for which the statistics are displayed. This can be either the latency (in milliseconds), throughput the number of events per second), or the amount of memory consumed (in bytes). The metric types based on which the performance of a Siddhi component is measured depends on the component type. Attribute : The attribute to which the given value applies. Value : The value for the metric type given in the row. Purpose This allows you to carry out a detailed analysis of the performance of a selected Siddhi application and identify components that have a negative impact on the overall performance of the Siddhi application. Recommended Action Identify the componets in a Siddhi application that have a negative impact on the performance, and rewrite them to improve performance. To understand Siddhi concepts in order to rewrite the components, see the Siddhi Query Guide . Viewing statistics for parent Siddhi applications \u00b6 When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. To view information specific to an active manager, click on the required active manager node in the Distributed Deployments section. This opens a page with parent Siddhi applications deployed in that manager node as shown in the example below. This page provides a summary of information relating to each parent Siddhi application as described in the table below. If a parent Siddhi application is active, it is indicated with a green dot that appears before the name of the Siddhi application. Similarly, an orange dot is displayed for inactive parent Siddhi applications. Detail Description Groups This indicates the number of execution groups of the parent Siddhi application. In the above example, the Testing Siddhi application has only one execution group. Child Apps This indicates the number of child applications of the parent Siddhi application. The number of active child applications is displayed in green, and the number of inactive child applications are displayed in red. Worker Nodes The number displayed in yellow indicates the total number of worker nodes in the resource cluster. In the above example, there are two worker nodes in the cluster. The number displayed in green indicates the number of worker nodes in which the Siddhi application is deployed. In the above example, the Testing parent Siddhi application is deployed only in one worker node although there are two worker nodes in the resource cluster. If you click on a parent Siddhi application, detailed information is displayed as shown below. The following are the widgets displayed. Code View View (Example) Description This displays the queries defined in the Parent Siddhi file of the application. This allows you to check the queries of the Siddhi application if any further investigations are needed based on the kafka diagrams and performance. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of the performance of the distributed cluster to which it belongs. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Distributed Siddhi App Deployment View (Example) Description This is a graphical representation of how Kafka topics are connected to the child Siddhi applications of the selected parent Siddhi application. Kafka topics are represented by boxes with red margins, and the child applications are represented by boxes with blue margins. Purpose This is displayed for you to understand how the flow of information takes place. Child App Details View (Example) Description This table displays the complete list of child Siddhi applications of the selected parent Siddhi application. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Group Name : The name of the execution group to which the child application belongs. Child App Status : This indicates whether the child application is currently active or not. Worker Node : The HTTPS host and The HTTPS port of the worker node in which the child siddhi application is deployed. Purpose To identify the currently active child applications. Application overview \u00b6 When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your Streaming Integrator setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your Streaming Integrator setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Scalable Deployment ). The following information is displayed for each Siddhi application. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the Streaming Integrator setup. Deployed Node : The host and the port of the Streaming Integrator node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deployed in the Streaming Integrator setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages .","title":"Monitoring the Streaming Integrator"},{"location":"admin/monitoring-the-streaming-integrator/#monitoring-the-streaming-integrator","text":"The Status Dashboard allows you to monitor the metrics of a stand-alone Streaming Integrator instance or a Streaming Integrator cluster. This involves monitoring whether all processes of the Streaming Integrator setup are working in a healthy manner, monitoring the current status of a Streaming Integrator node, and viewing metrics relating to the history of a node or the cluster. Both JVM level metrics or Siddhi application level metrics can be viewed from the Status Dashboard. The following sections cover how to configure the Status Dashboard and analyze statistics relating to your Streaming Integrator deployment in it.","title":"Monitoring the Streaming Integrator"},{"location":"admin/monitoring-the-streaming-integrator/#configuring-the-status-dashboard","text":"The following sections cover the configurations that need to be done in order to view statistics relating to the performance of your Streaming Integrator deployment in the Status Dashboard.","title":"Configuring the Status Dashboard"},{"location":"admin/monitoring-the-streaming-integrator/#assigning-unique-carbon-ids-to-nodes","text":"Carbon metrics uses the carbon ID as the source ID for metrics. Therefore, all the worker nodes are required to have a unique carbon ID defined in the wso2.carbon: section of the <SI_HOME>/conf/server/deployment.yaml file as shown in the extract below. Info You need to ensure that the carbon ID of each node is unique because it is required for the Status dashboard to identify the worker nodes and display their statistics accordingly. wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # ports used by this server","title":"Assigning unique carbon IDs to nodes"},{"location":"admin/monitoring-the-streaming-integrator/#setting-up-the-database","text":"To monitor statistics in the Status Dashboard, you need a shared metrics database that stores the metrics of all the nodes. Set up a database of the required type by following the steps below. In this section, a MySQL database is created as an example. Info The Status Dashboard is only supported with H2, MySQL, MSSQL and Oracle database types. It is configured with the H2 database type by default. If you want to continue to use H2, skip this step. Download and install the required database type. For this example, let's download and install MySQL Server . Download the required database driver. For this example, download the MySQL JDBC driver . Unzip the database driver you downloaded, copy its JAR file ( mysql-connector-java-x.x.xx-bin.jar in this example), and place it in the <DASHBOARD_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Create two databases named WSO2_METRICS_DB (to store metrics data) and WSO2_STATUS_DASHBOARD_DB (to store statistics) with tables. To create MySQL databases and tables for this example, run the following commands. mysql> create database WSO2_METRICS_DB; mysql> use WSO2_METRICS_DB; mysql> source <SI_TOOLING_HOME>/wso2/server/dbscripts/metrics/mysql.sql; mysql> grant all on WSO2_METRICS_DB.* TO username@localhost identified by \"password\"; mysql> create database WSO2_STATUS_DASHBOARD_DB; mysql> use WSO2_STATUS_DASHBOARD_DB; mysql> source <SI_TOOLING_HOME>/wso2/server/dbscripts/metrics/mysql.sql; mysql> grant all on WSO2_STATUS_DASHBOARD_DB.* TO username@localhost identified by \"password\"; Create two datasources named WSO2_METRICS_DB and WSO2_STATUS_DASHBOARD_DB by adding the following datasource configurations under the wso2.datasources: section of the <SI_HOME>/conf/server/deployment.yaml file. Info The names of the data sources must be the same as the names of the database tables you created for metrics and statistics. You need to change the values for the username and password parameters to the username and password that you are using to access the MySQL database. For detailed information about datasources, see carbon-datasources . WSO2_METRICS_DB - name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_METRICS_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false WSO2_STATUS_DASHBOARD_DB - name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_STATUS_DASHBOARD_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following are sample configurations for database tables when you use other database types supported. Click here to view the sample data source configurations. Database Type Metrics Datasource Dashboard Datasource MSSQL name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:sqlserver://localhost;databaseName=wso2_metrics' username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:sqlserver://localhost;databaseName=monitoring' username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:oracle:thin:@localhost:1521/xe' username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:oracle:thin:@localhost:1521/xe' username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false","title":"Setting up the database"},{"location":"admin/monitoring-the-streaming-integrator/#configuring-metrics","text":"This section explains how to configure metrics for your status dashboard. Configuring worker metrics To enable metrics and to configure metric-related properties, do the following configurations in the \\< SI_HOME>/conf/server/deployment.yaml file for the required nodes. To enable Carbon metrics, set the enabled property to true under wso2.metrics as shown below. wso2.metrics: enabled: true To enable JDBC reporting, set the Enable JDBC parameter to true in the wso2.metrics.jdbc: => reporting: subsection as shown below. If JDBC reporting is not enabled, only real-time metrics are displayed in the first page of the Status dashboard, and information relating to metrics history is not displayed in the other pages of the dashboard. To render the first entry of the graph, you need to wait for the time duration specified as the pollingPeriod . # Enable JDBC Reporter name: JDBC enabled: true pollingPeriod: 60 Under wso2.metrics.jdbc , configure the following properties to clean up the database entries. wso2.metrics.jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - &JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined in a *-datasources.xml file in conf/datasources directory. dataSourceName: java:comp/env/jdbc/WSO2MetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 Parameter Default Value Description dataSource &JDBC01 dataSource > dataSourceName java:comp/env/jdbc/WSO2MetricsDB The name of the datasource used to store metric data. dataSource > scheduledCleanup > enabled false If this is set to true , metrics data stored in the database is cleared at a specific time interval as scheduled. dataSource > scheduledCleanup > daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. dataSource > scheduledCleanup > scheduledCleanupPeriod 86400 This parameter specifies the time interval in seconds at which all metric data stored in the database must be cleared. JVM metrics of which the log level is set to OFF are not measured by default. If you need to monitor one or more of them, add the relevant metric name(s) under the wso2.metrics: => levels subsection as shown in the extract below. As shown below, you also need to mention log4j mode in which the metrics need to be monitored (i.e., OFF , INFO , DEBUG , TRACE , or ALL ). wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO Click here to view the default metric levels supported... Class loading Property Garbage collector Property Memory Property Operating system load Property Threads | Property | Default Level | Description | |------------------------------------------------------------------------|----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | ` jvm.threads.count ` | ` Debug ` | The gauge for showing the number of active and idle threads currently available in the JVM thread pool. | | ` jvm.threads.daemon.count ` | ` Debug ` | The gauge for showing the number of active daemon threads currently available in the JVM thread pool. | | ` jvm.threads.blocked.count ` | ` OFF ` | The gauge for showing the number of threads that are currently blocked in the JVM thread pool. | | ` jvm.threads.deadlock.count ` | ` OFF ` | The gauge for showing the number of threads that are currently deadlocked in the JVM thread pool. | | ` jvm.threads.new.count ` | ` OFF ` | The gauge for showing the number of new threads generated in the JVM thread pool. | | ` jvm.threads.runnable.count ` | ` OFF ` | The gauge for showing the number of runnable threads currently available in the JVM thread pool. | | ` jvm.threads.terminated.count ` | ` OFF ` | The gauge for showing the number of threads terminated from the JVM thread pool since user started running the WSO2 API Manager instance. | | ` jvm.threads.timed_waiting.count ` | ` OFF ` | The gauge for showing the number of threads in the Timed\\_Waiting state. | | ` jvm.threads.waiting.count ` | ` OFF ` | The gauge for showing the number of threads in the Waiting state in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions. | File descriptor details Property Swap space Property Configuring Siddhi application metrics To enable Siddhi application level metrics for a Siddhi application, you need to add the @app:statistics annotation bellow the Siddhi application name in the Siddhi file as shown in the example below. @App:name('TestMetrics') @app:statistics(reporter = 'jdbc') define stream TestStream (message string); The following are the metrics measured for a Siddhi application. Info The default level after enabling metrics is INFO for all the meytrics listed in the following table. Metric Components to which the metric is applied Latency Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains) Throughput Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains ) Memory Queries (per query) Buffered Events Count Number of events at disruptor Streams (per stream) Number of events produced/received after restart Sources (per source) Sinks (per sink)","title":"Configuring metrics"},{"location":"admin/monitoring-the-streaming-integrator/#configuring-cluster-credentials","text":"In order to access the nodes in a cluster and derive statistics, you need to maintain and share a user name and a password for each node in a SI cluster. This user name and password must be specified in the <DASHBOARD_HOME>/conf/server/deployment.yaml file. If you want to secure sensitive information such as the user name and the password, you can encrypt them via WSO2 Secure Vault. To specify the user name and the password to access a node, define them under the wso2.status.dashboard section as shown in the following example. wso2.status.dashboard: workerAccessCredentials: username: 'admin' password: 'admin' To encrypt the user name and the password you defined, define aliases for them as described in Protecting Sensitive Data via the Secure Vault . Info This functionality is currently supported only for single tenant environments.","title":"Configuring cluster credentials"},{"location":"admin/monitoring-the-streaming-integrator/#configuring-permissions","text":"The following are the three levels of permissions that can be granted for the users of the Status Dashboard. Permission Level Granted permissions SysAdmin Enabling/disabling metrics Adding workers Deleting workers Viewing workers Developers Adding workers Deleting workers Viewing workers Viewers Viewing workers The admin user in the userstore is assigned the SysAdmin permission level by default. To assign different permission levels to different roles, you can list the required roles under the relevant permission level in the wso2.status.dashboard section of the DASHBOARD_HOME>/conf/dashboard/deployment.yaml file as shown in the extract below. wso2.status.dashboard: sysAdminRoles: - role_1 developerRoles: - role_2 viewerRoles: - role_3 Info The display name of the roles given in the configuration must be present in the user store. To configure user store check, User Management .","title":"Configuring permissions"},{"location":"admin/monitoring-the-streaming-integrator/#downloading-and-accessing-the-status-dashboard","text":"To download and access the Status Dashboard, follow the procedure below: Download the Status Dashboard from TODO. Unzip the downloaded fine. The unzipped directory is referred to as <DASHBOARD_HOME> for the rest iof this section. In the terminal, navigate to the <DASHBOARD_HOME>/bin directory and issue the following command. For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost:<DASHBOARD_PORT>/si-status-dashboard e.g., https://0.0.0.0:9643/si-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. If no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Adding a node to the dashboard .","title":"Downloading and accessing the Status Dashboard"},{"location":"admin/monitoring-the-streaming-integrator/#node-overview","text":"Once you login to the status dashboard, the nodes that are already added to the Status Dashboard are displayed as shown in the following example:","title":"Node overview"},{"location":"admin/monitoring-the-streaming-integrator/#adding-a-node-to-the-dashboard","text":"If no nodes are displayed, you can add the nodes for which you want to view the status by following the procedure below: Click ADD NEW NODE . This opens the following dialog box. Enter the following information in the dialog box and click ADD NODE to add a gadget for the required node in the Node Overview page. In the Host parameter, enter the host ID of the node you want to add. In the Port parameter, enter the port number of the node you want to add. If the node you added is currently unreachable, the following dialog box is displayed. Click either WORKER or MANAGER. If you click WORKER , the node is displayed under Never Reached . If you click Manager , the node is displayed under Distributed Deployments as shown below. Info The following basic details are displayed for each node. CPU Usage : The CPU resources consumed by the Streaming Integrator node out of the available CPU resources in the machine in which it is deployed is expressed as a percentage. Memory Usage : The memory consumed by the node as a percentage of the total memory available in the system. Load Average : Siddhi Apps : The total number of Siddhi applications deployed in the node.","title":"Adding a node to the dashboard"},{"location":"admin/monitoring-the-streaming-integrator/#viewing-status-details","text":"The following is a list of sections displayed in the Node Overview page to provide information relating to the status of the nodes. Distributed Deployments View (Example) Description The nodes that are connected in the distributed deployment are displayed under the relevant group ID in the status dashboard (e.g., sp in the above example). Both managers and workers are displayed under separate labels. Managers : The active manager node in the cluster is indicated by a green dot that is displayed with the host name and the port of the node. Similarly, a grey dot is displayed for passive manager nodes in the cluster. Workers : When you add an active manager node, it automatically retrieves the worker node details that are connected with that particular deployment. If the worker node is already registered in the Status Dashboard, you can view the metrics of that node as follows: Purpose To determine whether the request load is efficiently allocated between the nodes of a cluster. To determine whether the cluster has sufficient resources to handle the load of requests. To identify the nodes connected with the particular deployment. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Clustered nodes View (Example) Description The nodes that are clustered together in a high-availability deployment are displayed under the relevant cluster ID in the Status Dashboard (e.g., under WSO2_A_1 in the above example). The active node in the cluster (i.e., the active worker in a minimum HA cluster or the active manager in a fully distributed cluster) are indicated by a green dot that is displayed with the hostname and the port of the node. Similarly, a grey dot is displayed for passive nodes in the cluster. Purpose This allows you to determine the following: Whether the request load is efficiently allocated between the nodes of a cluster. Whether the cluster has sufficient resources to handle the load of requests. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Single nodes View (Example) Description This section displays statistics for Streaming Integrator servers that operate as single node setups. Purpose This allows you to compare the performance of single nodes agaisnt each other. Recommended Action If the CPU usage of a node is too high, investigate the causes for it and take corrective action (e.g., undeploy unnecessary Siddhi applications). If any underutilized single nodes are identified, you can either deploy more Siddhi applications thatare currrently deployed in other nodes with a high request load. Alternatively, you can redeploy the Siddhi applications of the underutilized node to other nodes, and then shut it down. Nodes that cannot be reached View (Example) Description When a node is newly added to the Status dashboard and it is unavailable, it is displayed as shown in the above examples. Purpose This allows you to identify nodes that cannot be reached at specific hosts and ports. Recommended Action Check whether the host and port of the node you added is correct. Check whether any authentication errors have occured for the node. Nodes that are currently unavailable View (Example) Description When a node that could be viewed previously is no longer available, its status is displayed in red as shown in the example above. The status displayed for such nodes is applicable for the last time at which the node had been reachable. Purpose This allows you to identify previously available nodes that have become unreachable. Recommended Action Check whether the node is inactive. Check whether any authentication errors have occured for the node. Nodes for which metrics is disabled View (Example) Description When a node for which metrics is disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which metrics is not enabled. Recommended Action Enable metrics for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable metrics, see Monitoring the Stream Processor - Configuring the Status Dashboard . Nodes with JMX reporting disabled View (Example) Description When a node with JMX reporting disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which JMX reporting is disabled Recommended Action Enable JMX reporting for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable JMX reporting, see Monitoring the Stream Processor - Configuring the Status Dashboard . Statistics trends View (Example) Description This dispalys the change that has taken taken place in the CPU usage, memory usage and the load average of nodes since the status was last viewed in the status dashboard. Positive changes are indicated in green (e.g., a decrease in the CPU usage in the above example), and egative changes are indicated in red (an increase in the memory usage and the load average in the above example). Purpose This allows you to view a summary of the performance trends of your Streaming Integrator clusters and single nodes. Recommended Action Based on the performance trend observed, add more resources to your Streaming Integrator clusters/single nodes to handle more events, or shutdown one or more nodes if there is excess resources.","title":"Viewing status details"},{"location":"admin/monitoring-the-streaming-integrator/#viewing-node-specific-pages","text":"When you open the Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant widget. This opens a separate page for the worker node as shown in the example below.","title":"Viewing node-specific pages"},{"location":"admin/monitoring-the-streaming-integrator/#status-indicators","text":"The following gadgets can be viewed for the selected worker. Server General Details View (Example) Description This gadget displays general information relating to the selected worker node. Purpose This allows you to understand the distribution of nodes in terms of the location, the time zone, operating system used etc., and to locate them. Recommended Action In a distributed set up, you can use this information to evaluate the clustered setup and make changes to optimize the benefits of deploying the Streaming Integrator as a cluster (e.g., making them physically available in different locations to minimize the risk of all the nodes failing at the same time etc.). CPU Usage View (Example) Description This displays the CPU usage of the selected node. Purpose This allows you to observe the CPU usage of a selected node over time. Recommended Action Identify sudden slumps in the CPU usage, and investigate the reasons (e.g., such as authentication errors that result in requests not reaching the Streaming Integrator server). Identify continuous increases in the CPU usage and check whether the node is overloaded. If so, reallocate some of the Siddhi applications deployed in the node. Memory Used View (Example) Description This displays the memory usage of the selected node. Purpose This allows you to observe the memory usage of a selected node over time. Recommended Action Identify sudden slumps in the memory usage, and investigate the reasons (e.g., a reduction in the requests recived due to system failure). If there are continous increases in the memory usage, check whether there is an increase in the requests handled, and whether you have enough memory resources to handle the increased demand. If not, add more memory to the node or reallocate some of the Siddhi applications deployed in the node to other nodes. System Load Average View (Example) Description This displays the system load average for the selected node. Purpose This allows you to observe the system load of the node over time. Recommended Action Observe the trends of the node's system load, and adjust the allocation of resources (e.g., memory) and work load (i.e., the number of Siddhi applications deployed) accordingly. Overall Throughput View (Example) Description This displays the overall throughput of the selected node. Purpose This allows you to assess the performance of the selected node in terms of the throughput over time. Recommended Action Compare the throughput of the node against that of other nodes with the same amount of CPU and memory resources. If there are significant variations, investigate the causes (e.g., the differences in the number of requests received by different Siddhi applications deployed in the nodes). Observe changes in the throughput over time. If there are significant variances, investigate the causes (e.g., whether the node has been unavaialable to receive requests during a given time). Siddhi Applications View (Example) Description This table displays the complete list of Siddhi applications deployed in the selected node. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Age : The age of the Siddhi application in milliseconds. Latency : The time (in milliseconds) taken by the Siddhi application to process one request. Throughput: The number of requests processed by the Siddhi application since it has been active. Memory : The amount of memory consumed by the Siddhi application during its current active session, expressed in milliseconds. Purpose This allows you to assess the performance of each Siddhi application deployed in the selected node. Recommended Action Identify the inactive Siddhi applications that are required to be active and take the appropriate corrective action. Identify Siddhi applications that consume too much memory, and identify ways in which the memory usage can be optimized (e.g., use incremental processing).","title":"Status indicators"},{"location":"admin/monitoring-the-streaming-integrator/#viewing-worker-history","text":"This section explains how to view statistics relating to the performance of a selected node for a specific time interval. Log in to the Status Dashboard. In the Node Overview page, click on the required node to view information specific to that node. In the page displayed with node-specific information, click one of the following gadgets to open the Metrics page. CPU Usage Memory Used System Load Average Overall Throughput In the Metrics page, click the required time interval. Then the page displays statistics relating to the performance of the selected node applicable to that time period. If you want to view more details, click More Details . As a result, the following additional information is displayed for the node for the selected time period. CPU Usage JVM OS as CPU JVM Physical Memory JVM Threads JVM Swap Space","title":"Viewing worker history"},{"location":"admin/monitoring-the-streaming-integrator/#viewing-statistics-for-siddhi-applications","text":"When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant gadget. This opens the page specific to the worker . To view information specific to a Siddhi application deployed in the Siddhi node, click on the relevant Siddhi application in the Siddhi Applications table. This opens a page with information specific to the selected Siddhi application as shown in the example below. The following statistics can be viewed for an individual Siddhi Application. Latency View (Example) Description This displays the latency of the selected Siddhi application. Latency is the time taken to complete processing a single event in the event flow. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the latency of the Siddhi application is too high, check the Siddhi queries and rewrite them to optimise performance. Overall Throughput View (Example) Description This shows the overall throughput of a selected Siddhi application over time. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the throughput of a Siddhi application varies greatly overtime, investigate reasons for any slumps in the throughput (e.g., errors in the deployment of the application). If the throughput of the Siddhi application is lower than expected, investigate reasons, and take corrective action to improve the throughput (e.g., check the Siddhi queries in the application and rewrite them with best practices to achieve greater efficiency in the processing of events. Memory Used View (Example) Description This displays the memory usage (In MB) of a selected Siddhi application over time. Purpose This allows you to monitor the memory consumption of individual Siddhi applications. Recommended Action If there are major fluctuations in the memory consumption of a Siddhi application, investigate the reasons (e.g., Whether the Siddhi application has been inactive at any point of time). Code View View (Example) Description This displays the queries defined in the Siddhi file of the application. Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of its latency, throughput and the memory consumption. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Creating a Siddhi Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Design View View (Example) Description This displays the graphical view for queries defined in the Siddhi file of the application. Purpose This allows you to check the flow of the queries of the Siddhi application in the graphical way. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. Siddhi App Component Statistics View (Example) Description This table displays performance statistics related to dfferent components within a selected Siddhi application (e.g., queries). The columns displayed are as follows: Type : The type of the Siddhi application component to which the information displayed in the row applies. The component type can be queries, streams, tables, windows and aggregations. For more information, see Siddhi Application Overview - Common components of a Siddhi application . Name : The name of the Siddhi component within the application to which the information displayed in the row apply. Metric Type : The metric type for which the statistics are displayed. This can be either the latency (in milliseconds), throughput the number of events per second), or the amount of memory consumed (in bytes). The metric types based on which the performance of a Siddhi component is measured depends on the component type. Attribute : The attribute to which the given value applies. Value : The value for the metric type given in the row. Purpose This allows you to carry out a detailed analysis of the performance of a selected Siddhi application and identify components that have a negative impact on the overall performance of the Siddhi application. Recommended Action Identify the componets in a Siddhi application that have a negative impact on the performance, and rewrite them to improve performance. To understand Siddhi concepts in order to rewrite the components, see the Siddhi Query Guide .","title":"Viewing statistics for Siddhi applications"},{"location":"admin/monitoring-the-streaming-integrator/#viewing-statistics-for-parent-siddhi-applications","text":"When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. To view information specific to an active manager, click on the required active manager node in the Distributed Deployments section. This opens a page with parent Siddhi applications deployed in that manager node as shown in the example below. This page provides a summary of information relating to each parent Siddhi application as described in the table below. If a parent Siddhi application is active, it is indicated with a green dot that appears before the name of the Siddhi application. Similarly, an orange dot is displayed for inactive parent Siddhi applications. Detail Description Groups This indicates the number of execution groups of the parent Siddhi application. In the above example, the Testing Siddhi application has only one execution group. Child Apps This indicates the number of child applications of the parent Siddhi application. The number of active child applications is displayed in green, and the number of inactive child applications are displayed in red. Worker Nodes The number displayed in yellow indicates the total number of worker nodes in the resource cluster. In the above example, there are two worker nodes in the cluster. The number displayed in green indicates the number of worker nodes in which the Siddhi application is deployed. In the above example, the Testing parent Siddhi application is deployed only in one worker node although there are two worker nodes in the resource cluster. If you click on a parent Siddhi application, detailed information is displayed as shown below. The following are the widgets displayed. Code View View (Example) Description This displays the queries defined in the Parent Siddhi file of the application. This allows you to check the queries of the Siddhi application if any further investigations are needed based on the kafka diagrams and performance. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of the performance of the distributed cluster to which it belongs. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Distributed Siddhi App Deployment View (Example) Description This is a graphical representation of how Kafka topics are connected to the child Siddhi applications of the selected parent Siddhi application. Kafka topics are represented by boxes with red margins, and the child applications are represented by boxes with blue margins. Purpose This is displayed for you to understand how the flow of information takes place. Child App Details View (Example) Description This table displays the complete list of child Siddhi applications of the selected parent Siddhi application. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Group Name : The name of the execution group to which the child application belongs. Child App Status : This indicates whether the child application is currently active or not. Worker Node : The HTTPS host and The HTTPS port of the worker node in which the child siddhi application is deployed. Purpose To identify the currently active child applications.","title":"Viewing statistics for parent Siddhi applications"},{"location":"admin/monitoring-the-streaming-integrator/#application-overview","text":"When you open the WSO2 Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your Streaming Integrator setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your Streaming Integrator setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Scalable Deployment ). The following information is displayed for each Siddhi application. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the Streaming Integrator setup. Deployed Node : The host and the port of the Streaming Integrator node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deployed in the Streaming Integrator setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages .","title":"Application overview"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/","text":"Protecting Sensitive Data via the Secure Vault \u00b6 The Streaming Integrator uses several artifacts for its functionality including deployment configurations for tuning its operation as well as deployable artifacts for extending its functionality. In each of these scenarios, there can be situations where the data specified is of a sensitive nature e.g., access tokens, passwords, etc. Securing sensitive data in deployment configurations \u00b6 The Streaming Integrator offers the Cipher tool to encrypt sensitive data in deployment configurations. This tool works in conjunction with WSO2 Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see WSO2 Secure Vault . Open the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/secrets.properties file and enter the following information: Enter the required sensitive data element with the value in plain text as shown in the example below. wso2.sample.password1=plainText ABC@123 Enter the alias to be used in the required configuration file instead of the actual value of sensitive data you specified in the previous step as shown in the example below. password: ${sec:wso2.sample.password1} To encrypt the sensitive data element and store it in the secure vault, run the Cipher tool by issuing the following command. sh <SI_HOME>|<SI_TOOLING_HOME>/bin/ciphertool.sh -runtime server Protecting sensitive data in Siddhi applications \u00b6 A parameter named ref is used to secure sensitive information in Siddhi applications that are deployed in the Streaming Integrator. For Siddhi applications that use storage technologies supported by Carbon Data sources , it is also possible to use Carbon data sources instead of specifying the connection parameters directly on the Siddhi file. Using the ref parameter \u00b6 Siddhi 4.0 supports the ref parameter that enables the user to specify parts of their definition outside the Siddhi App. Extensions that support this functionality include: Stores Sources Sinks This method of securing sensitive data involves defining the store parameters required via a connection instance in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file, and referring to those from Siddhi applications via the ref parameter. Example In the <SI_HOME>/conf/server/deployment.yaml file, some connection parameters are defined for a store named store1 as follows: siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' The Siddhi application includes the following configuration: @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Here @Store(ref='store1') refers to store1 defined in the deployment.yaml file. As a result, the properties defined for this store are applicable to the Siddhi application when it is connected to the store. Using carbon datasources \u00b6 Currently, Carbon Data sources only support relational data source definitions. You can also define RDBMS Store artifacts using Carbon Data sources or JNDI instead of directly specifying the connection parameters. Then the datasource definitions defined in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file can be secured via the process described under Securing sensitive data in deployment configurations .","title":"Protecting Sensitive Data via the Secure Vault"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#protecting-sensitive-data-via-the-secure-vault","text":"The Streaming Integrator uses several artifacts for its functionality including deployment configurations for tuning its operation as well as deployable artifacts for extending its functionality. In each of these scenarios, there can be situations where the data specified is of a sensitive nature e.g., access tokens, passwords, etc.","title":"Protecting Sensitive Data via the Secure Vault"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#securing-sensitive-data-in-deployment-configurations","text":"The Streaming Integrator offers the Cipher tool to encrypt sensitive data in deployment configurations. This tool works in conjunction with WSO2 Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see WSO2 Secure Vault . Open the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/secrets.properties file and enter the following information: Enter the required sensitive data element with the value in plain text as shown in the example below. wso2.sample.password1=plainText ABC@123 Enter the alias to be used in the required configuration file instead of the actual value of sensitive data you specified in the previous step as shown in the example below. password: ${sec:wso2.sample.password1} To encrypt the sensitive data element and store it in the secure vault, run the Cipher tool by issuing the following command. sh <SI_HOME>|<SI_TOOLING_HOME>/bin/ciphertool.sh -runtime server","title":"Securing sensitive data in deployment configurations"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#protecting-sensitive-data-in-siddhi-applications","text":"A parameter named ref is used to secure sensitive information in Siddhi applications that are deployed in the Streaming Integrator. For Siddhi applications that use storage technologies supported by Carbon Data sources , it is also possible to use Carbon data sources instead of specifying the connection parameters directly on the Siddhi file.","title":"Protecting sensitive data in Siddhi applications"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#using-the-ref-parameter","text":"Siddhi 4.0 supports the ref parameter that enables the user to specify parts of their definition outside the Siddhi App. Extensions that support this functionality include: Stores Sources Sinks This method of securing sensitive data involves defining the store parameters required via a connection instance in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file, and referring to those from Siddhi applications via the ref parameter. Example In the <SI_HOME>/conf/server/deployment.yaml file, some connection parameters are defined for a store named store1 as follows: siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' The Siddhi application includes the following configuration: @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Here @Store(ref='store1') refers to store1 defined in the deployment.yaml file. As a result, the properties defined for this store are applicable to the Siddhi application when it is connected to the store.","title":"Using the ref parameter"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#using-carbon-datasources","text":"Currently, Carbon Data sources only support relational data source definitions. You can also define RDBMS Store artifacts using Carbon Data sources or JNDI instead of directly specifying the connection parameters. Then the datasource definitions defined in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file can be secured via the process described under Securing sensitive data in deployment configurations .","title":"Using carbon datasources"},{"location":"admin/supporting-different-transports/","text":"Supporting Different Transports \u00b6 Follow the relevant section for the steps that need to be carried out before using the required transport to receive and publish events via the Streaming Integrator. Kafka transport \u00b6 To enable the Streaming Integrator to receive and publish events via the Kafka transport, follow the steps below: Download the Kafka broker from the Kafka site . Convert and copy the Kafka client jars from the <KAFKA_HOME>/libs directory to the <SI_TOOLING_HOME>/lib directory as follows: Create a directory in a preferred location in your machine and copy the following JARs to it from the <KAFKA_HOME>/libs directory. Info This directory is referred to as the <SOURCE_DIRECTORY> in the next steps. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory in a preferred location in your machine. Info This directory is referred to as the <DESTINATION_DIRECTORY> in the next steps. To convert all the Kafka jars you copied into the <SOURCE_DIRECTORY> , issue one of the following commands. For Windows: : <SI_TOOLING_HOME>/bin/jartobundle.bat <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> Copy the converted files from the <DESTINATION_DIRECTORY> to the <SI_TOOLING_HOME>/lib directory. Copy the jars that are not converted from the <SOURCE_DIRECTORY> to the <SI_TOOLING_HOME>/samples/sample-clients/lib directory. The Kafka server should be started before sending events from the Streaming Integrator to a Kafka consumer. JMS transport \u00b6 To configure the Apache ActiveMQ message broker, follow the steps below: Install Apache ActiveMQ JMS . Info This guide uses ActiveMQ versions 5.7.0 - 5.9.0. If you want to use a later version, for instructions on the necessary changes to the configuration steps, go to Apache ActiveMQ Documentation . Download the activemq-client-5.x.x.jar from here . Register the InitialContextFactory implementation according to the OSGi JNDI spec and copy the client jar to the <SI_TOOLING_HOME>/lib directory as follows. Navigate to the SI_TOOLING_HOME>/bin directory and issue the following command. For Linux : ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> For Windows : ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> Info If required, you can provide privileges via chmod +x icf-provider.(sh|bat) . Once the client jar is successfully converted, the activemq-client-5.x.x directory is created. This directory contains the following: activemq-client-5.x.x.jar (original jar) activemq-client-5.x.x_1.0.0.jar (OSGi-converted jar) In addition, the following messages are logged in the terminal. INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar Copy activemq-client-5.x.x/activemq-client-5.x.x.jar and place it in the <SI_TOOLING_HOME>/samples/sample-clients/lib directory. Copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar and place it in the <SI_TOOLING_HOME>/lib directory. Create a directory in a preferred location in your machine and copy the following JARs to it from the <ActiveMQ_HOME>/libs directory. Info This directory is referred to as the <SOURCE_DIRECTORY> in the next steps. hawtbuf-1.9.jar geronimo-jms_1.1_spec-1.1.1.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar Create another directory in a preferred location in your machine. Info This directory will be referred to as the <DESTINATION_DIRECTORY> in the next steps. To convert all the Kafka jars you copied into the <SOURCE_DIRECTORY> , issue the following command. For Windows : <SI_TOOLING_HOME>/bin/jartobundle.bat <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> Copy the converted files from the <DESTINATION_DIRECTORY> to the <SI_TOOLING_HOME>/lib directory. Copy the jars that are not converted from the <SOURCE_DIRECTORY> to the <SI_TOOLING_HOME>/samples/sample-clients/lib directory. MQTT transport \u00b6 To configure the MQTT message broker, follow the steps below: Download the org.eclipse.paho.client.mqttv3-1.1.1.jar file from here. Place the file you downloaded in the <SI_TOOLING_HOME>/lib directory. RabbitMQ transport \u00b6 To configure the RabbitMQ message broker, follow the steps below: Download RabbitMQ from here. Create a directory in a preferred location in your machine. This directory is referred to as at the <SOURCE_DIRECTORY> in the rest of the procedure. Copy the following files from the <RabbitMQ_HOME>/plugins directory to the <SOURCE_DIRECTORY> you created. Create another directory in a preferred location in your machine. This directory is referred to as the <DESTINATION_DIRECTORY> in this procedure.","title":"Supporting Different Transports"},{"location":"admin/supporting-different-transports/#supporting-different-transports","text":"Follow the relevant section for the steps that need to be carried out before using the required transport to receive and publish events via the Streaming Integrator.","title":"Supporting Different Transports"},{"location":"admin/supporting-different-transports/#kafka-transport","text":"To enable the Streaming Integrator to receive and publish events via the Kafka transport, follow the steps below: Download the Kafka broker from the Kafka site . Convert and copy the Kafka client jars from the <KAFKA_HOME>/libs directory to the <SI_TOOLING_HOME>/lib directory as follows: Create a directory in a preferred location in your machine and copy the following JARs to it from the <KAFKA_HOME>/libs directory. Info This directory is referred to as the <SOURCE_DIRECTORY> in the next steps. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory in a preferred location in your machine. Info This directory is referred to as the <DESTINATION_DIRECTORY> in the next steps. To convert all the Kafka jars you copied into the <SOURCE_DIRECTORY> , issue one of the following commands. For Windows: : <SI_TOOLING_HOME>/bin/jartobundle.bat <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> Copy the converted files from the <DESTINATION_DIRECTORY> to the <SI_TOOLING_HOME>/lib directory. Copy the jars that are not converted from the <SOURCE_DIRECTORY> to the <SI_TOOLING_HOME>/samples/sample-clients/lib directory. The Kafka server should be started before sending events from the Streaming Integrator to a Kafka consumer.","title":"Kafka transport"},{"location":"admin/supporting-different-transports/#jms-transport","text":"To configure the Apache ActiveMQ message broker, follow the steps below: Install Apache ActiveMQ JMS . Info This guide uses ActiveMQ versions 5.7.0 - 5.9.0. If you want to use a later version, for instructions on the necessary changes to the configuration steps, go to Apache ActiveMQ Documentation . Download the activemq-client-5.x.x.jar from here . Register the InitialContextFactory implementation according to the OSGi JNDI spec and copy the client jar to the <SI_TOOLING_HOME>/lib directory as follows. Navigate to the SI_TOOLING_HOME>/bin directory and issue the following command. For Linux : ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> For Windows : ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> Info If required, you can provide privileges via chmod +x icf-provider.(sh|bat) . Once the client jar is successfully converted, the activemq-client-5.x.x directory is created. This directory contains the following: activemq-client-5.x.x.jar (original jar) activemq-client-5.x.x_1.0.0.jar (OSGi-converted jar) In addition, the following messages are logged in the terminal. INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar Copy activemq-client-5.x.x/activemq-client-5.x.x.jar and place it in the <SI_TOOLING_HOME>/samples/sample-clients/lib directory. Copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar and place it in the <SI_TOOLING_HOME>/lib directory. Create a directory in a preferred location in your machine and copy the following JARs to it from the <ActiveMQ_HOME>/libs directory. Info This directory is referred to as the <SOURCE_DIRECTORY> in the next steps. hawtbuf-1.9.jar geronimo-jms_1.1_spec-1.1.1.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar Create another directory in a preferred location in your machine. Info This directory will be referred to as the <DESTINATION_DIRECTORY> in the next steps. To convert all the Kafka jars you copied into the <SOURCE_DIRECTORY> , issue the following command. For Windows : <SI_TOOLING_HOME>/bin/jartobundle.bat <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <SOURCE_DIRECTORY_PATH> <DESTINATION_DIRECTORY_PATH> Copy the converted files from the <DESTINATION_DIRECTORY> to the <SI_TOOLING_HOME>/lib directory. Copy the jars that are not converted from the <SOURCE_DIRECTORY> to the <SI_TOOLING_HOME>/samples/sample-clients/lib directory.","title":"JMS transport"},{"location":"admin/supporting-different-transports/#mqtt-transport","text":"To configure the MQTT message broker, follow the steps below: Download the org.eclipse.paho.client.mqttv3-1.1.1.jar file from here. Place the file you downloaded in the <SI_TOOLING_HOME>/lib directory.","title":"MQTT transport"},{"location":"admin/supporting-different-transports/#rabbitmq-transport","text":"To configure the RabbitMQ message broker, follow the steps below: Download RabbitMQ from here. Create a directory in a preferred location in your machine. This directory is referred to as at the <SOURCE_DIRECTORY> in the rest of the procedure. Copy the following files from the <RabbitMQ_HOME>/plugins directory to the <SOURCE_DIRECTORY> you created. Create another directory in a preferred location in your machine. This directory is referred to as the <DESTINATION_DIRECTORY> in this procedure.","title":"RabbitMQ transport"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/","text":"User Management via the IdP Client Interface \u00b6 In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client Local IdP Client \u00b6 The local IdP Client interacts with the file-based user store that is defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default. Parameters \u00b6 The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privileges. userManager > userStore > users > user > username admin The username of the user. userManager > userStore > users > user > password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager > userStore > users > user > roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager > userStore > roles > role > id 1 The unique ID for the role. userManager > userStore > roles > role > admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties > sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties > refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin External IdP Client \u00b6 External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store. Writing custom IdP Client \u00b6 When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"User Management via the IdP Client Interface"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#user-management-via-the-idp-client-interface","text":"In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client","title":"User Management via the IdP Client Interface"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#local-idp-client","text":"The local IdP Client interacts with the file-based user store that is defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default.","title":"Local IdP Client"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#parameters","text":"The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privileges. userManager > userStore > users > user > username admin The username of the user. userManager > userStore > users > user > password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager > userStore > users > user > roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager > userStore > roles > role > id 1 The unique ID for the role. userManager > userStore > roles > role > admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties > sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties > refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Parameters"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#external-idp-client","text":"External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store.","title":"External IdP Client"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#writing-custom-idp-client","text":"When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"Writing custom IdP Client"},{"location":"admin/user-management/","text":"User Management \u00b6 Info User management in Stream Processor has the following features, The concept of single user store, which is either local or external. File based user store as the default embedded store. Ability to connect to an external Identity Provider using SCIM2 and OAuth2 protocols. Ability to extend user authentication as per the scenario Introduction to user management \u00b6 User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components . Users \u00b6 Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed. Permission \u00b6 A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically. User Roles \u00b6 A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually. User Store \u00b6 A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\<SP_HOME>/conf/\\<PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled. User Management via the IdP Client Interface \u00b6 In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client Local IdP Client \u00b6 The local IdP Client interacts with the file-based user store that is defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default. Parameters \u00b6 The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privileges. userManager > userStore > users > user > username admin The username of the user. userManager > userStore > users > user > password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager > userStore > users > user > roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager > userStore > roles > role > id 1 The unique ID for the role. userManager > userStore > roles > role > admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties > sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties > refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin External IdP Client \u00b6 External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store. Writing custom IdP Client \u00b6 When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"User Management"},{"location":"admin/user-management/#user-management","text":"Info User management in Stream Processor has the following features, The concept of single user store, which is either local or external. File based user store as the default embedded store. Ability to connect to an external Identity Provider using SCIM2 and OAuth2 protocols. Ability to extend user authentication as per the scenario","title":"User Management"},{"location":"admin/user-management/#introduction-to-user-management","text":"User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components .","title":"Introduction to user management"},{"location":"admin/user-management/#users","text":"Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed.","title":"Users"},{"location":"admin/user-management/#permission","text":"A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically.","title":"Permission"},{"location":"admin/user-management/#user-roles","text":"A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually.","title":"User Roles"},{"location":"admin/user-management/#user-store","text":"A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\<SP_HOME>/conf/\\<PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled.","title":"User Store"},{"location":"admin/user-management/#user-management-via-the-idp-client-interface","text":"In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client","title":"User Management via the IdP Client Interface"},{"location":"admin/user-management/#local-idp-client","text":"The local IdP Client interacts with the file-based user store that is defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default.","title":"Local IdP Client"},{"location":"admin/user-management/#parameters","text":"The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privileges. userManager > userStore > users > user > username admin The username of the user. userManager > userStore > users > user > password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager > userStore > users > user > roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager > userStore > roles > role > id 1 The unique ID for the role. userManager > userStore > roles > role > admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties > sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties > refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Parameters"},{"location":"admin/user-management/#external-idp-client","text":"External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager > adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store.","title":"External IdP Client"},{"location":"admin/user-management/#writing-custom-idp-client","text":"When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"Writing custom IdP Client"},{"location":"admin/working-with-Keystores/","text":"Working with Keystores \u00b6 A keystore is a repository that stores the cryptographic keys and certificates that are used for various security purposes, such as encrypting sensitive information and for establishing trust between your server and outside parties that connect to your server. The usage of keys and certificates contained in a keystore are explained below. Key pairs : According to public-key cryptography, a key pair (private key and the corresponding public key) is used for encrypting sensitive information and for authenticating the identity of parties that communicate with your server. For example, information that is encrypted in your server using the public key can only be decrypted using the corresponding private key. Therefore, if any party wants to decrypt this encrypted data, they should have the corresponding private key, which is usually kept as a secret (not publicly shared). Digital certificate : When there is a key pair, it is also necessary to have a digital certificate to verify the identity of the keys. Typically, the public key of a key pair is embedded in this digital certificate, which also contains additional information such as the owner, validity, etc. of the keys. For example, if an external party wants to verify the integrity of data or validate the identity of the signer (by validating the digital signature), it is necessary for them to have this digital certificate. Trusted certificates : To establish trust, the digital certificate containing the public key should be signed by a trusted certifying authority (CA). You can generate self-signed certificates for the public key (thereby creating your own certifying authority), or you can get the certificates signed by an external CA. Both types of trusted certificates can be effectively used depending on the sensitivity of the information that is protected by the keys. When the certificate is signed by a reputed CA, all the parties who trust this CA also trust the certificates signed by them. Identity and Trust The key pair and the CA-signed certificates in a keystore establishes two security functions in your server: The key pair with the digital certificate is an indication of identity and the CA-signed certificate provides trust to the identity. Since the public key is used to encrypt information, the keystore containing the corresponding private key should always be protected, as it can decrypt the sensitive information. Furthermore, the privacy of the private key is important as it represents its own identity and protects the integrity of data. However, the CA-signed digital certificates should be accessible to outside parties that require to decrypt and use the information. To facilitate this requirement, the certificates must be copied to a separate keystore (called a Truststore), which can then be shared with outside parties. Therefore, in a typical setup, you have one keystore for identity (containing the private key) that is protected, and a separate keystore for trust (containing CA certificates) that is shared with outside parties. Setting up keystores in the Streaming Integrator \u00b6 The Streaming Integrator uses keystores mainly for the following purposes: Authenticating the communication over Secure Sockets Layer (SSL)/Transport Layer Security (TLS) protocols. Communication over Secure Sockets Layer (SSL) when invoking Rest APIs. Protecting sensitive information via Cipher Tool. Default keystore settings in WSO2 products \u00b6 The Streaming Integrator is shipped with the following default keystore files stored in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. wso2carbon.jks : This keystore contains a key pair and is used by default in your Streaming Integrator and Streaming Integrator Tooling servers for all of the purposes explained above, except protecting sensitive information via Cipher tool. securevault.jks : This is the default keystore used by the secure vault to protect sensitive information via Cipher tool. client-truststore.jks : This is the default trust store that contains the trusted certificates of the keystore used in SSL communication. By default, the following files provide paths to these keystores: <SI_HOME>|<SI_TOOLING_HOME>/wso2/server/bin/carbon.sh file This script is run when you start an Streaming Integrator server. It contains the following parameters, and makes references to the two files mentioned above by default. Parameter Default Value Description keyStore \"$CARBON_HOME/resources/security/wso2carbon.jks\" \\ This specifies the path to the keystore to be used when running the Streaming Integrator server on a secure network. keyStorePassword \"wso2carbon\" \\ The password to access the keystore. trustStore \"$CARBON_HOME/resources/security/client-truststore.jks\" \\ This specifies the path to the trust store to be used when running the server on a secure network. trustStorePassword \"wso2carbon\" \\ The password to access the trust store. <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file refers to the above keystore and trust store by default for the following configurations: Listener configurations This specifies the key store to be used when the Streaming Integrator is receiving events via a secure network, and the password to access the key store. Databridge configurations This specifies the key store to be used when the Streaming Integrator is publishing events via databrige using a secure network, and the password to access the key store. Secure vault configurations This specifies the key store to be used when you are configuring a secure vault to protect sensitive information. Note It is recommended to replace this default keystore with a new keystore that has self-signed or CA signed certificates when the products are deployed in production environments. This is because wso2carbon.jks is available with open source WSO2 products, which means anyone can have access to the private key of the default keystore. Managing keystores \u00b6 You can view the default keystores and truststores in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. Once you create your own keystore, you can delete the default keystores, but you need to ensure that they are no longer referenced in the <SI_HOME>|<SI_TOOLING_HOME>/wso2/server/bin/carbon.sh file or the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. Creating new keystores \u00b6 The Streaming Integrator is shipped with two default keystores named wso2carbon.jks and securevault.jks . These keystores are stored in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. They come with a private/public key pair that is used to encrypt sensitive information for communication over SSL and for encryption/signature purposes in WS-Security. However, note that because these keystores are available with open source WSO2 products, anyone can have access to the private keys of the default keystores. It is therefore recommended to replace these with keystores that have self-signed or CA signed certificates when the Streaming Integrator is deployed in production environments. Creating a keystore using an existing certificate \u00b6 Secure Sockets Layer (SSL) is a protocol that is used to secure communication between systems. This protocol uses a public key, a private key and a random symmetric key to encrypt data. As SSL is widely used in many systems, certificates may already exist that can be reused. In such situations, you can use the CA-signed certificates to generate a Java keystore using OpenSSL and the Java keytool. First, export certificates to the PKCS12/PFX format. Give strong passwords whenever required. Info It is required to have same password for both the keystore and key. Execute the following command to export the certificates: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx Convert the PKCS12 to a Java keystore using the following command: keytool -importkeystore -srckeystore <pkcs12 file name>.pfx -srcstoretype pkcs12 -destkeystore <JKS name>.jks -deststoretype JKS Now you have a keystore with CA-signed certificates. Creating a keystore using a new certificate \u00b6 If there are no certificates signed by a Certification Authority, you can follow the steps in this section to create a keystore with keys and a new certificate. In the following steps, you are using the keytool that is available with your JDK installation. Step 1: Creating keystore with private key and public certificate Open a command prompt and go to the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. All the keystores should be stored here. Create the keystore that includes the private key by executing the following command: keytool -genkey -alias certalias -keyalg RSA -keysize 2048 -keystore newkeystore.jks -dname \"CN=<testdomain.org>,OU=Home,O=Home,L=SL,S=WS,C=LK\" -storepass mypassword -keypass mypassword This command creates a keystore with the following details: Keystore name : newkeystore.jks Alias of public certificate : certalias Keystore password : mypassword Private key password : mypassword (this is required to be the same as keystore password) Tip If you did not specify values for the '-keypass' and the '-storepass' in the above command, you are requested to give a value for the '-storepass' (password of the keystore). As a best practice, use a password generator to generate a strong password. You are then asked to enter a value for -keypass . Click Enter , because we need the same password for both the keystore and the key. Also, if you did not specify values for -dname , you will be asked to provide those details individually. Open the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory and check if the new keystore file is created. Make a backup of it and move it to a secure location. This is important because if not your private key is available only on one location. Step 2: Creating CA-signed certificates for public key Now we have a .jks file. This keystore ( .jks file) can be used to generate a certificate signing request (CSR). This CSR file must be certified by a certificate authority or certification authority (CA), which is an entity that issues digital certificates. These certificates can certify the ownership of a public key. Execute the following command to generate the CSR: keytool -certreq -alias certalias -file newcertreq.csr -keystore newkeystore.jks Tip As mentioned before, use the same alias that you used during the keystore creation process. You are asked to give the keystore password. Once the password is given, the command outputs the newcertreq.csr file to the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. This is the CSR that you must submit to a CA. You must provide this CSR file to the CA. For testing purposes, try the 90 days trial SSL certificate from Comodo . Info It is preferable to have a wildcard certificate or multiple domain certificates if you wish to have multiple sub-domains like gateway.sampledomain.org , publisher.sampledomain.org , identity.sampledomain.org , etc., for the deployment. For such requirements you must modify the CSR request by adding subject alternative names. Most of the SSL providers give instructions to generate the CSR in such cases. After accepting the request, a signed certificate is provided along with several intermediate certificates (depending on the CA) as a bundle (.zip file). !!!info \"The following is a sample certificate by the CA (Comodo) ```text The Root certificate of the CA: AddTrustExternalCARoot.crt Intermediate certificates: COMODORSAAddTrustCA.crt , COMODORSADomainValidationSecureServerCA.crt SSL Certificate signed by CA: test_sampleapp_org.crt ``` Step 3: Importing CA-signed certificates to keystore Before importing the CA-signed certificate to the keystore, add the root CA certificate and the two intermediate certificates by executing the commands given below. Note that the sample certificates given above are used as examples. keytool -import -v -trustcacerts -alias ExternalCARoot -file AddTrustExternalCARoot.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias TrustCA -file COMODORSAAddTrustCA.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias SecureServerCA -file COMODORSADomainValidationSecureServerCA.crt -keystore newkeystore.jks -storepass mypassword Info Optionally, you can append the -storepass <keystore password> option to avoid having to enter the password when prompted later in the interactive mode. After you add the root certificate and all other intermediate certificates, add the CA-signed SSL certificate to the keystore by executing the following command: keytool -import -v -alias <certalias> -file <test_sampleapp_org.crt> -keystore newkeystore.jks -keypass myppassword -storepass mykpassword Tip In this command, use the same alias that you used when you created the keystore. Now you have a Java keystore including a CA-signed certificate that can be used in a production environment. Next, you must add its public key to the client-truststore.jks file to enable backend communication and inter-system communication via SSL. Adding the public key to client-truststore.jks In SSL handshake, the client needs to verify the certificate presented by the server. For this purpose, the client usually stores the certificates it trusts in a trust store. The Streaming Integrator is shipped with the trust store named client-truststore.jks that resides in the same directory as the keystore (i.e., <SI_HOME>|<SI_TOOLING_HOME>/resources/ ). Therefore, you need to import the new public certificate into this trust store for front-end and backend communication of the Streaming Integrator to take place in the required manner over SSL. Tip In this example, you are using the default client-truststore.jks file in the Streaming Integrator as the trust store. To add the public key of the signed certificate to the client trust store: Get a copy of the client-truststore.jks file from the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. Export the public key from your .jks file using the by issuing the following command. keytool -export -alias certalias -keystore newkeystore.jks -file <public key name>.pem Import the public key you extracted in the previous step to the client-truststore.jks file by issuing the following command. keytool -import -alias certalias -file <public key name>.pem -keystore client-truststore.jks -storepass wso2carbon Tip wso2carbon is the keystore password of the default client-truststore.jks file. Now, you have an SSL certificate stored in a Java keystore and a public key added to the client-truststore.jks file. Note that both these files should be placed in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. You can now replace the default wso2carbon.jks keystore in your product with the newly created keystore by updating the relevant configuration files in your product. For more information, see Configuring Keystores . Configuring keystores \u00b6 Once you have created a new key store and updated the <SI_HOME>|<SI_TOOLING_HOME>/resources/security/client-truststore.jks file, you must update the <SI_HOME>|<SI_TOOLING_HOME>/conf/<PROFILE/deployment.yaml file to make that keystore work for the required functions. Keystores are used for multiple functions in the Streaming Integrator including securing the servlet transport, databridge communication, encrypting confidential information in configuration files etc. Tip The wso2carbon.jks keystore file that is shipped with the Streaming Integrator is used as the default keystore for all functions. However, in a production environment, it is recommended to create new keystores with keys and certificates because the Streaming Integrator is an open source integrator, and anyone who downloads it has access to the default keystore. To find all the functions that require a keystore, you can search for .jks in the deployment.yaml file. e.g., If you want to secure the listener configured for the Streaming Integrator using a keystore, you can enter details relating to the keystore as shown below. In this example, the details of the default key is used. listenerConfigurations: - id: \"msf4j-https\" host: \"0.0.0.0\" port: 9743 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon Parameter Description keyStoreFile The path to the keystore file. keyStorePassword The password with which the keystore can be accessed. certPass The alias of the public certificate issued by the certification authority.","title":"Working with Keystores"},{"location":"admin/working-with-Keystores/#working-with-keystores","text":"A keystore is a repository that stores the cryptographic keys and certificates that are used for various security purposes, such as encrypting sensitive information and for establishing trust between your server and outside parties that connect to your server. The usage of keys and certificates contained in a keystore are explained below. Key pairs : According to public-key cryptography, a key pair (private key and the corresponding public key) is used for encrypting sensitive information and for authenticating the identity of parties that communicate with your server. For example, information that is encrypted in your server using the public key can only be decrypted using the corresponding private key. Therefore, if any party wants to decrypt this encrypted data, they should have the corresponding private key, which is usually kept as a secret (not publicly shared). Digital certificate : When there is a key pair, it is also necessary to have a digital certificate to verify the identity of the keys. Typically, the public key of a key pair is embedded in this digital certificate, which also contains additional information such as the owner, validity, etc. of the keys. For example, if an external party wants to verify the integrity of data or validate the identity of the signer (by validating the digital signature), it is necessary for them to have this digital certificate. Trusted certificates : To establish trust, the digital certificate containing the public key should be signed by a trusted certifying authority (CA). You can generate self-signed certificates for the public key (thereby creating your own certifying authority), or you can get the certificates signed by an external CA. Both types of trusted certificates can be effectively used depending on the sensitivity of the information that is protected by the keys. When the certificate is signed by a reputed CA, all the parties who trust this CA also trust the certificates signed by them. Identity and Trust The key pair and the CA-signed certificates in a keystore establishes two security functions in your server: The key pair with the digital certificate is an indication of identity and the CA-signed certificate provides trust to the identity. Since the public key is used to encrypt information, the keystore containing the corresponding private key should always be protected, as it can decrypt the sensitive information. Furthermore, the privacy of the private key is important as it represents its own identity and protects the integrity of data. However, the CA-signed digital certificates should be accessible to outside parties that require to decrypt and use the information. To facilitate this requirement, the certificates must be copied to a separate keystore (called a Truststore), which can then be shared with outside parties. Therefore, in a typical setup, you have one keystore for identity (containing the private key) that is protected, and a separate keystore for trust (containing CA certificates) that is shared with outside parties.","title":"Working with Keystores"},{"location":"admin/working-with-Keystores/#setting-up-keystores-in-the-streaming-integrator","text":"The Streaming Integrator uses keystores mainly for the following purposes: Authenticating the communication over Secure Sockets Layer (SSL)/Transport Layer Security (TLS) protocols. Communication over Secure Sockets Layer (SSL) when invoking Rest APIs. Protecting sensitive information via Cipher Tool.","title":"Setting up keystores in the Streaming Integrator"},{"location":"admin/working-with-Keystores/#default-keystore-settings-in-wso2-products","text":"The Streaming Integrator is shipped with the following default keystore files stored in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. wso2carbon.jks : This keystore contains a key pair and is used by default in your Streaming Integrator and Streaming Integrator Tooling servers for all of the purposes explained above, except protecting sensitive information via Cipher tool. securevault.jks : This is the default keystore used by the secure vault to protect sensitive information via Cipher tool. client-truststore.jks : This is the default trust store that contains the trusted certificates of the keystore used in SSL communication. By default, the following files provide paths to these keystores: <SI_HOME>|<SI_TOOLING_HOME>/wso2/server/bin/carbon.sh file This script is run when you start an Streaming Integrator server. It contains the following parameters, and makes references to the two files mentioned above by default. Parameter Default Value Description keyStore \"$CARBON_HOME/resources/security/wso2carbon.jks\" \\ This specifies the path to the keystore to be used when running the Streaming Integrator server on a secure network. keyStorePassword \"wso2carbon\" \\ The password to access the keystore. trustStore \"$CARBON_HOME/resources/security/client-truststore.jks\" \\ This specifies the path to the trust store to be used when running the server on a secure network. trustStorePassword \"wso2carbon\" \\ The password to access the trust store. <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file refers to the above keystore and trust store by default for the following configurations: Listener configurations This specifies the key store to be used when the Streaming Integrator is receiving events via a secure network, and the password to access the key store. Databridge configurations This specifies the key store to be used when the Streaming Integrator is publishing events via databrige using a secure network, and the password to access the key store. Secure vault configurations This specifies the key store to be used when you are configuring a secure vault to protect sensitive information. Note It is recommended to replace this default keystore with a new keystore that has self-signed or CA signed certificates when the products are deployed in production environments. This is because wso2carbon.jks is available with open source WSO2 products, which means anyone can have access to the private key of the default keystore.","title":"Default keystore settings in WSO2 products"},{"location":"admin/working-with-Keystores/#managing-keystores","text":"You can view the default keystores and truststores in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. Once you create your own keystore, you can delete the default keystores, but you need to ensure that they are no longer referenced in the <SI_HOME>|<SI_TOOLING_HOME>/wso2/server/bin/carbon.sh file or the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file.","title":"Managing keystores"},{"location":"admin/working-with-Keystores/#creating-new-keystores","text":"The Streaming Integrator is shipped with two default keystores named wso2carbon.jks and securevault.jks . These keystores are stored in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. They come with a private/public key pair that is used to encrypt sensitive information for communication over SSL and for encryption/signature purposes in WS-Security. However, note that because these keystores are available with open source WSO2 products, anyone can have access to the private keys of the default keystores. It is therefore recommended to replace these with keystores that have self-signed or CA signed certificates when the Streaming Integrator is deployed in production environments.","title":"Creating new keystores"},{"location":"admin/working-with-Keystores/#creating-a-keystore-using-an-existing-certificate","text":"Secure Sockets Layer (SSL) is a protocol that is used to secure communication between systems. This protocol uses a public key, a private key and a random symmetric key to encrypt data. As SSL is widely used in many systems, certificates may already exist that can be reused. In such situations, you can use the CA-signed certificates to generate a Java keystore using OpenSSL and the Java keytool. First, export certificates to the PKCS12/PFX format. Give strong passwords whenever required. Info It is required to have same password for both the keystore and key. Execute the following command to export the certificates: openssl pkcs12 -export -in <certificate file>.crt -inkey <private>.key -name \"<alias>\" -certfile <additional certificate file> -out <pfx keystore name>.pfx Convert the PKCS12 to a Java keystore using the following command: keytool -importkeystore -srckeystore <pkcs12 file name>.pfx -srcstoretype pkcs12 -destkeystore <JKS name>.jks -deststoretype JKS Now you have a keystore with CA-signed certificates.","title":"Creating a keystore using an existing certificate"},{"location":"admin/working-with-Keystores/#creating-a-keystore-using-a-new-certificate","text":"If there are no certificates signed by a Certification Authority, you can follow the steps in this section to create a keystore with keys and a new certificate. In the following steps, you are using the keytool that is available with your JDK installation. Step 1: Creating keystore with private key and public certificate Open a command prompt and go to the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. All the keystores should be stored here. Create the keystore that includes the private key by executing the following command: keytool -genkey -alias certalias -keyalg RSA -keysize 2048 -keystore newkeystore.jks -dname \"CN=<testdomain.org>,OU=Home,O=Home,L=SL,S=WS,C=LK\" -storepass mypassword -keypass mypassword This command creates a keystore with the following details: Keystore name : newkeystore.jks Alias of public certificate : certalias Keystore password : mypassword Private key password : mypassword (this is required to be the same as keystore password) Tip If you did not specify values for the '-keypass' and the '-storepass' in the above command, you are requested to give a value for the '-storepass' (password of the keystore). As a best practice, use a password generator to generate a strong password. You are then asked to enter a value for -keypass . Click Enter , because we need the same password for both the keystore and the key. Also, if you did not specify values for -dname , you will be asked to provide those details individually. Open the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory and check if the new keystore file is created. Make a backup of it and move it to a secure location. This is important because if not your private key is available only on one location. Step 2: Creating CA-signed certificates for public key Now we have a .jks file. This keystore ( .jks file) can be used to generate a certificate signing request (CSR). This CSR file must be certified by a certificate authority or certification authority (CA), which is an entity that issues digital certificates. These certificates can certify the ownership of a public key. Execute the following command to generate the CSR: keytool -certreq -alias certalias -file newcertreq.csr -keystore newkeystore.jks Tip As mentioned before, use the same alias that you used during the keystore creation process. You are asked to give the keystore password. Once the password is given, the command outputs the newcertreq.csr file to the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. This is the CSR that you must submit to a CA. You must provide this CSR file to the CA. For testing purposes, try the 90 days trial SSL certificate from Comodo . Info It is preferable to have a wildcard certificate or multiple domain certificates if you wish to have multiple sub-domains like gateway.sampledomain.org , publisher.sampledomain.org , identity.sampledomain.org , etc., for the deployment. For such requirements you must modify the CSR request by adding subject alternative names. Most of the SSL providers give instructions to generate the CSR in such cases. After accepting the request, a signed certificate is provided along with several intermediate certificates (depending on the CA) as a bundle (.zip file). !!!info \"The following is a sample certificate by the CA (Comodo) ```text The Root certificate of the CA: AddTrustExternalCARoot.crt Intermediate certificates: COMODORSAAddTrustCA.crt , COMODORSADomainValidationSecureServerCA.crt SSL Certificate signed by CA: test_sampleapp_org.crt ``` Step 3: Importing CA-signed certificates to keystore Before importing the CA-signed certificate to the keystore, add the root CA certificate and the two intermediate certificates by executing the commands given below. Note that the sample certificates given above are used as examples. keytool -import -v -trustcacerts -alias ExternalCARoot -file AddTrustExternalCARoot.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias TrustCA -file COMODORSAAddTrustCA.crt -keystore newkeystore.jks -storepass mypassword keytool -import -v -trustcacerts -alias SecureServerCA -file COMODORSADomainValidationSecureServerCA.crt -keystore newkeystore.jks -storepass mypassword Info Optionally, you can append the -storepass <keystore password> option to avoid having to enter the password when prompted later in the interactive mode. After you add the root certificate and all other intermediate certificates, add the CA-signed SSL certificate to the keystore by executing the following command: keytool -import -v -alias <certalias> -file <test_sampleapp_org.crt> -keystore newkeystore.jks -keypass myppassword -storepass mykpassword Tip In this command, use the same alias that you used when you created the keystore. Now you have a Java keystore including a CA-signed certificate that can be used in a production environment. Next, you must add its public key to the client-truststore.jks file to enable backend communication and inter-system communication via SSL. Adding the public key to client-truststore.jks In SSL handshake, the client needs to verify the certificate presented by the server. For this purpose, the client usually stores the certificates it trusts in a trust store. The Streaming Integrator is shipped with the trust store named client-truststore.jks that resides in the same directory as the keystore (i.e., <SI_HOME>|<SI_TOOLING_HOME>/resources/ ). Therefore, you need to import the new public certificate into this trust store for front-end and backend communication of the Streaming Integrator to take place in the required manner over SSL. Tip In this example, you are using the default client-truststore.jks file in the Streaming Integrator as the trust store. To add the public key of the signed certificate to the client trust store: Get a copy of the client-truststore.jks file from the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. Export the public key from your .jks file using the by issuing the following command. keytool -export -alias certalias -keystore newkeystore.jks -file <public key name>.pem Import the public key you extracted in the previous step to the client-truststore.jks file by issuing the following command. keytool -import -alias certalias -file <public key name>.pem -keystore client-truststore.jks -storepass wso2carbon Tip wso2carbon is the keystore password of the default client-truststore.jks file. Now, you have an SSL certificate stored in a Java keystore and a public key added to the client-truststore.jks file. Note that both these files should be placed in the <SI_HOME>|<SI_TOOLING_HOME>/resources/security directory. You can now replace the default wso2carbon.jks keystore in your product with the newly created keystore by updating the relevant configuration files in your product. For more information, see Configuring Keystores .","title":"Creating a keystore using a new certificate"},{"location":"admin/working-with-Keystores/#configuring-keystores","text":"Once you have created a new key store and updated the <SI_HOME>|<SI_TOOLING_HOME>/resources/security/client-truststore.jks file, you must update the <SI_HOME>|<SI_TOOLING_HOME>/conf/<PROFILE/deployment.yaml file to make that keystore work for the required functions. Keystores are used for multiple functions in the Streaming Integrator including securing the servlet transport, databridge communication, encrypting confidential information in configuration files etc. Tip The wso2carbon.jks keystore file that is shipped with the Streaming Integrator is used as the default keystore for all functions. However, in a production environment, it is recommended to create new keystores with keys and certificates because the Streaming Integrator is an open source integrator, and anyone who downloads it has access to the default keystore. To find all the functions that require a keystore, you can search for .jks in the deployment.yaml file. e.g., If you want to secure the listener configured for the Streaming Integrator using a keystore, you can enter details relating to the keystore as shown below. In this example, the details of the default key is used. listenerConfigurations: - id: \"msf4j-https\" host: \"0.0.0.0\" port: 9743 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon Parameter Description keyStoreFile The path to the keystore file. keyStorePassword The password with which the keystore can be accessed. certPass The alias of the public certificate issued by the certification authority.","title":"Configuring keystores"},{"location":"admin/writing-Custom-Siddhi-Extensions/","text":"Writing Custom Siddhi Extensions \u00b6 Custom extensions can be written in order to apply use case specific logic that is not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension archetypes are explained below with their related maven archetypes. You can use these archetypes to generate maven projects for each extension type. siddhi-execution \u00b6 Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Siddhi Query Guide - Extensions . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.execution -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'executionType': ML To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory. siddhi-io \u00b6 Siddhi-io provides following extension types: sink source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: The Source extension type gets inputs to your Siddhi application. The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.io -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory. siddhi-map \u00b6 Siddhi-map provides following extension types: Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints(such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.map -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory. siddhi-script \u00b6 Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.script -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOfScript': To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory. siddhi-store \u00b6 Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.store -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'storeType': RDBMS To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"Writing Custom Siddhi Extensions"},{"location":"admin/writing-Custom-Siddhi-Extensions/#writing-custom-siddhi-extensions","text":"Custom extensions can be written in order to apply use case specific logic that is not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension archetypes are explained below with their related maven archetypes. You can use these archetypes to generate maven projects for each extension type.","title":"Writing Custom Siddhi Extensions"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-execution","text":"Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Siddhi Query Guide - Extensions . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.execution -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'executionType': ML To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"siddhi-execution"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-io","text":"Siddhi-io provides following extension types: sink source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: The Source extension type gets inputs to your Siddhi application. The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.io -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"siddhi-io"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-map","text":"Siddhi-map provides following extension types: Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints(such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.map -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"siddhi-map"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-script","text":"Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.script -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOfScript': To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"siddhi-script"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-store","text":"Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.store -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'storeType': RDBMS To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the <SI_HOME>/lib directory.","title":"siddhi-store"},{"location":"concepts/concepts/","text":"Key Concepts \u00b6 Concept Description Streaming Data A continuous flow of data generated by one or more sources. A data stream is often made of discrete data bundles that are transmitted one after the other. These data bundles are often referred to as events or messages. Stream Processing A paradigm for processing streaming data. Stream processing lets users process continuous streams of data on the fly and derive results faster in near-real-time. This contrasts with conventional processing methods that require data to be stored prior to processing. Streaming Data Integration The process that makes streaming data available for downstream destinations. Each consumer may expect data in different formats, protocols, or mediums (DB, file, Network, etc.). These requirements are addressed via Streaming Data Integration that involves converting data into different formats and publishing data in different protocols through different mediums. The data data is often processed using Stream Processing and other techniques to add value before integrating data with its consumers. Streaming Integration In contrast to Streaming Data Integration that only focuses on making streaming data available for downstream, Streaming Integration involves integrating streaming data as well as trigger action based on data streams. The action can be a single request to a service or a complex enterprise integration flow. Data Transformation This refers to converting data from one format to another (e.g, JSON to XML) or altering the original structure of data. Data Cleansing This refers to filtering out corrupted, inaccurate or irrelevant data from a data stream based on one or more conditions. Data cleansing also involves modifying/replacing content to hide/remove unwanted data parts from a message (e.g., obscuring). Data Correlation This refers to combining different data items based on relationships between data. In the context of streaming data, it is often required to correlate data across multiple streams received from different sources. The relationship can be expressed in terms of a boolean condition, sequence or a pattern of occurrences of data items. Data Enrichment The process of adding value to raw data by enhancing and improving data by adding more information that is more consumable for users. Data Summarization This refers to aggregating data to produce a summarized representation of raw data. In the context of Stream Processing, such aggregations can be done based on temporal criteria in real-time with moving time windows. Real-time ETL This refers to performing data extraction, transformation, and loading in real-time. In contrast to traditional ETL that uses batch processing, Real-time ETL extracts data as and when it is available, transforms it on the fly, and integrates it. Change Data Capture A technique or a process that makes it possible to capture changes in the data in real-time. This enables real-time ETL with databases because it allows users to receive real-time notifications when data in the database tables are changing.","title":"Key Concepts"},{"location":"concepts/concepts/#key-concepts","text":"Concept Description Streaming Data A continuous flow of data generated by one or more sources. A data stream is often made of discrete data bundles that are transmitted one after the other. These data bundles are often referred to as events or messages. Stream Processing A paradigm for processing streaming data. Stream processing lets users process continuous streams of data on the fly and derive results faster in near-real-time. This contrasts with conventional processing methods that require data to be stored prior to processing. Streaming Data Integration The process that makes streaming data available for downstream destinations. Each consumer may expect data in different formats, protocols, or mediums (DB, file, Network, etc.). These requirements are addressed via Streaming Data Integration that involves converting data into different formats and publishing data in different protocols through different mediums. The data data is often processed using Stream Processing and other techniques to add value before integrating data with its consumers. Streaming Integration In contrast to Streaming Data Integration that only focuses on making streaming data available for downstream, Streaming Integration involves integrating streaming data as well as trigger action based on data streams. The action can be a single request to a service or a complex enterprise integration flow. Data Transformation This refers to converting data from one format to another (e.g, JSON to XML) or altering the original structure of data. Data Cleansing This refers to filtering out corrupted, inaccurate or irrelevant data from a data stream based on one or more conditions. Data cleansing also involves modifying/replacing content to hide/remove unwanted data parts from a message (e.g., obscuring). Data Correlation This refers to combining different data items based on relationships between data. In the context of streaming data, it is often required to correlate data across multiple streams received from different sources. The relationship can be expressed in terms of a boolean condition, sequence or a pattern of occurrences of data items. Data Enrichment The process of adding value to raw data by enhancing and improving data by adding more information that is more consumable for users. Data Summarization This refers to aggregating data to produce a summarized representation of raw data. In the context of Stream Processing, such aggregations can be done based on temporal criteria in real-time with moving time windows. Real-time ETL This refers to performing data extraction, transformation, and loading in real-time. In contrast to traditional ETL that uses batch processing, Real-time ETL extracts data as and when it is available, transforms it on the fly, and integrates it. Change Data Capture A technique or a process that makes it possible to capture changes in the data in real-time. This enables real-time ETL with databases because it allows users to receive real-time notifications when data in the database tables are changing.","title":"Key Concepts"},{"location":"develop/creating-a-Siddhi-Application/","text":"Creating Siddhi Applications \u00b6 Siddhi applications are files that define the Siddhi logic to process the events sent to the Streaming Integrator. They are written in the Siddhi Query Language using the Streaming Integrator Tooling . A Siddhi file contains the following configurations: Configuration Description Stream A logical series of events ordered in time with a uniquely identifiable name, and set of defined attributes with specific data types defining its schema. Source This consumes data from external sources (such as TCP , Kafka , HTTP , etc) in the form of events, then converts each event (that can be in XML , JSON , binary , etc. format) to a Siddhi event, and passes that to a stream for processing. Sink This takes events arriving at a stream, maps them to a predefined data format (such as XML , JSON, binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Executional Element An executional element can be one of the following: Stateless query: Queries that only consider currently incoming events when generating an output. e.g., filters Stateful query: Queries that consider both currently incoming events as well as past events when generating an output. e.g., windows, sequences, patterns, etc. Partitions: Collections of stream definitions and Siddhi queries separated from each other within a Siddhi application for the purpose of processing events in parallel and in isolation A Siddhi application can be created from the source view or the design view of the Streaming Integrator Tooling. Creating a Siddhi application in the source view \u00b6 To create a Siddhi application via the source view of the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by navigating to the <SI_TOOLING_HOME>/bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Streaming Integrator Tooling opens as shown below. ![Welcome Page](../images/Creating-Siddhi-Applications/Welcome-Page.png) Click New to start defining a new Siddhi application. A new file opens as shown below. Add the following sample Siddhi application to the file. @App:name(\"SweetProductionAnalysis\") @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', @map(type='json')) define stream ProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into ProductionAlertStream; Info Note the following in this Siddhi application Configuration Description Stream This stream contains two stream configurations: SweetProductionStream define stream SweetProductionStream (name string, amount double); This is the input stream that defines the schema based on which events are selected to be processed by the SweetProductionAnalysis Siddhi application. Events received via the source in this application are directed to this stream. ProductionAlertStream define stream ProductionAlertStream (name string, amount double); This is the output stream from which the sink configured in this application takes events to be published as the output. Source @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) This source configuration has the following sections: @Source(type = \u2018tcp\u2019, context='SweetProductionData' This configuration defines tcp as the transport via which events are received to be processed by the SweetProductionAnalysis Siddhi application. @map(type='binary')) This configuration defines the input mapping. In this scenario, Binary Mapper is used which converts input events into binary events and feeds them into siddhi. The source types and map types are available as Siddhi extensions, and you can find via the operator finder as follows: Click the Operator Finder icon to open the Operator Finder. Move the cursor to the location in the Siddhi application where you want to add the source. Search for the required transport type. Once it appears in the search results, click the Add to Source icon on it. Similarly, search for the mapping type you want to include in the source configuration, and add it. The source annotation is now displayed as follows. You can add the other properties as required, and save your changes. Sink @sink(type='log', @map(type='json')) This sink configuration has the following sections: @sink(type='log') This configuration defines log as the transport via which the processed events are published from the ProductionAlertStream output stream. Log sink simply publishes events into the console. @map(type='json')) This configuration defines the output mapping. Events are published with the json mapping type. Json mapper converts the events in the ProductionAlertStream to the Json format. You can select the sink type and the map type from the Operator Finder . Executional Elements from SweetProductionStream select * insert into ProductionAlertStream; This is where the logic of the siddhi app is defined. In this scenario, all the events received in the SweetProductionStream input stream are inserted into the ProductionAlertStream output stream. To save this Siddhi application, click File , and then click Save . By default siddhi applications are saved in the <SI_HOME>/wso2/editor/deployment/workspace directory. To export the Siddhi application to your preferred location, click File , and then click Export File . To see a graphical view of the event flow you defined in your Siddhi application, click Design View . The event flow is displayed as follows. {width=\"900\"} Creating a Siddhi application in the design view \u00b6 To create a Siddhi application via the design view of the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Studio by navigating to the <SI_TOOLING_HOME>/bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. To open the design view, click Design View . To define the input stream into which the events to be processed via the Siddhi application should be received, drag and drop the stream icon (shown below) into the grid. Once the stream component is added to the grid, move the cursor over it, and then click on the settings icon as shown below. As as result, the Stream Configuration form opens as follows. Fill this form as follows to define a stream named SweetProductionStream with two attributes named name and amount : In the Name field, enter SweetProductionStream . In the Attributes table, enter two attributes as follows. You can click +Attribute to add a new row in the table to define a new attribute. Attribute Name Attribute Type name string amount double Click Submit to save the new stream definition. As a result, the stream is displayed on the grid with the SweetProductionStream label as shown below. To define the output stream to which the processed events need to be directed, drag and drop the stream icon again. Place it after the SweetProductionStream stream. This stream should be named ProductionAlertStream and have the following attributes. Attribute Name Attribute Type name string totalProduction long To add the source from which events are received, drag and drop the source icon (shown below) into the grid. The source is an input to the SweetProductionStream input stream component. Therefore, place this source component to the left of the input stream component in the grid. Once you add the source component, draw a line from it to the SweetProductionStream input stream component by dragging the cursor as demonstrated below. Click the settings icon on the source component you added to open the Source Configuration form. Then enter information as follows. In the Source Type field, select tcp . For this example, assume that events are received in the binary format. To indicate that events are expected to be converted from this format, select binary in the Map Type field. To indicate the context, select the context check box and enter SweetProductionData in the field that appears below. Click Submit. To add a query that defines the execution logic, drag and drop the projection query icon (shown below) to the grid. The query uses the events in the SweetProductionStream input stream as inputs and directs the processed events (which are its output) to the ProductionAlertStream output stream. Therefore, create two connections as demonstrated below. To define the execution logic, move the cursor over the query in the grid, and click on the settings icon that appears. This opens the Query Configuration form. Enter information in it as follows: Enter a name for the query in the Name field. In this example, let's enter query as the name. In order to specify how each user defined attribute in the input stream is converted to generate the output events, select User Defined Attributes in the Select field. As a result, the User Defined Attributes table appears. The As column of this table displays the attributes of the output stream. To derive the value for each attribute, enter required expressions/values in the Expression column as explained below. The value for name can be derived from the input stream without any further processing. Therefore, enter name as the expression for the name attribute. To derive the value for the totalProduction attribute, the sum of the values for the amount attribute of input events need to be calculated. Therefore, enter the expression as follows to apply the sum() Siddhi function to the amount attribute. sum(amount) Leave the default values of the Output section unchanged. Click Submit to save the information. To add a sink to publish the output events that are directed to the ProductionAlertStream output stream, drag and drop the sink icon (shown below) into the grid. Draw an arrow from the ProductionAlertStream output stream to the sink component to connect them. Click the settings icon on the sink component you added to open the Sink Configuration form. Then enter information as follows. 1. In this example, let's assume that output needs to be generated as logs in the console. To indicate this, select log in the Sink Type field. In the Map Type field, select the format in which the output must be generated. For this example, let's select json . Click Submit to save the information. To align the Siddhi components that you have added to the grid, click Edit and then click Auto-Align . As a result, all the components are horizontally aligned as shown below. Click Source View . The siddhi application is displayed as follows. Click File and then click Save as . The Save to Workspace dialog box appears. In the File Name field, enter SweetProductionAnalysis and click Save .","title":"Creating Siddhi Applications"},{"location":"develop/creating-a-Siddhi-Application/#creating-siddhi-applications","text":"Siddhi applications are files that define the Siddhi logic to process the events sent to the Streaming Integrator. They are written in the Siddhi Query Language using the Streaming Integrator Tooling . A Siddhi file contains the following configurations: Configuration Description Stream A logical series of events ordered in time with a uniquely identifiable name, and set of defined attributes with specific data types defining its schema. Source This consumes data from external sources (such as TCP , Kafka , HTTP , etc) in the form of events, then converts each event (that can be in XML , JSON , binary , etc. format) to a Siddhi event, and passes that to a stream for processing. Sink This takes events arriving at a stream, maps them to a predefined data format (such as XML , JSON, binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Executional Element An executional element can be one of the following: Stateless query: Queries that only consider currently incoming events when generating an output. e.g., filters Stateful query: Queries that consider both currently incoming events as well as past events when generating an output. e.g., windows, sequences, patterns, etc. Partitions: Collections of stream definitions and Siddhi queries separated from each other within a Siddhi application for the purpose of processing events in parallel and in isolation A Siddhi application can be created from the source view or the design view of the Streaming Integrator Tooling.","title":"Creating Siddhi Applications"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-source-view","text":"To create a Siddhi application via the source view of the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by navigating to the <SI_TOOLING_HOME>/bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Streaming Integrator Tooling opens as shown below. ![Welcome Page](../images/Creating-Siddhi-Applications/Welcome-Page.png) Click New to start defining a new Siddhi application. A new file opens as shown below. Add the following sample Siddhi application to the file. @App:name(\"SweetProductionAnalysis\") @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', @map(type='json')) define stream ProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into ProductionAlertStream; Info Note the following in this Siddhi application Configuration Description Stream This stream contains two stream configurations: SweetProductionStream define stream SweetProductionStream (name string, amount double); This is the input stream that defines the schema based on which events are selected to be processed by the SweetProductionAnalysis Siddhi application. Events received via the source in this application are directed to this stream. ProductionAlertStream define stream ProductionAlertStream (name string, amount double); This is the output stream from which the sink configured in this application takes events to be published as the output. Source @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) This source configuration has the following sections: @Source(type = \u2018tcp\u2019, context='SweetProductionData' This configuration defines tcp as the transport via which events are received to be processed by the SweetProductionAnalysis Siddhi application. @map(type='binary')) This configuration defines the input mapping. In this scenario, Binary Mapper is used which converts input events into binary events and feeds them into siddhi. The source types and map types are available as Siddhi extensions, and you can find via the operator finder as follows: Click the Operator Finder icon to open the Operator Finder. Move the cursor to the location in the Siddhi application where you want to add the source. Search for the required transport type. Once it appears in the search results, click the Add to Source icon on it. Similarly, search for the mapping type you want to include in the source configuration, and add it. The source annotation is now displayed as follows. You can add the other properties as required, and save your changes. Sink @sink(type='log', @map(type='json')) This sink configuration has the following sections: @sink(type='log') This configuration defines log as the transport via which the processed events are published from the ProductionAlertStream output stream. Log sink simply publishes events into the console. @map(type='json')) This configuration defines the output mapping. Events are published with the json mapping type. Json mapper converts the events in the ProductionAlertStream to the Json format. You can select the sink type and the map type from the Operator Finder . Executional Elements from SweetProductionStream select * insert into ProductionAlertStream; This is where the logic of the siddhi app is defined. In this scenario, all the events received in the SweetProductionStream input stream are inserted into the ProductionAlertStream output stream. To save this Siddhi application, click File , and then click Save . By default siddhi applications are saved in the <SI_HOME>/wso2/editor/deployment/workspace directory. To export the Siddhi application to your preferred location, click File , and then click Export File . To see a graphical view of the event flow you defined in your Siddhi application, click Design View . The event flow is displayed as follows. {width=\"900\"}","title":"Creating a Siddhi application in the source view"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-design-view","text":"To create a Siddhi application via the design view of the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Studio by navigating to the <SI_TOOLING_HOME>/bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. To open the design view, click Design View . To define the input stream into which the events to be processed via the Siddhi application should be received, drag and drop the stream icon (shown below) into the grid. Once the stream component is added to the grid, move the cursor over it, and then click on the settings icon as shown below. As as result, the Stream Configuration form opens as follows. Fill this form as follows to define a stream named SweetProductionStream with two attributes named name and amount : In the Name field, enter SweetProductionStream . In the Attributes table, enter two attributes as follows. You can click +Attribute to add a new row in the table to define a new attribute. Attribute Name Attribute Type name string amount double Click Submit to save the new stream definition. As a result, the stream is displayed on the grid with the SweetProductionStream label as shown below. To define the output stream to which the processed events need to be directed, drag and drop the stream icon again. Place it after the SweetProductionStream stream. This stream should be named ProductionAlertStream and have the following attributes. Attribute Name Attribute Type name string totalProduction long To add the source from which events are received, drag and drop the source icon (shown below) into the grid. The source is an input to the SweetProductionStream input stream component. Therefore, place this source component to the left of the input stream component in the grid. Once you add the source component, draw a line from it to the SweetProductionStream input stream component by dragging the cursor as demonstrated below. Click the settings icon on the source component you added to open the Source Configuration form. Then enter information as follows. In the Source Type field, select tcp . For this example, assume that events are received in the binary format. To indicate that events are expected to be converted from this format, select binary in the Map Type field. To indicate the context, select the context check box and enter SweetProductionData in the field that appears below. Click Submit. To add a query that defines the execution logic, drag and drop the projection query icon (shown below) to the grid. The query uses the events in the SweetProductionStream input stream as inputs and directs the processed events (which are its output) to the ProductionAlertStream output stream. Therefore, create two connections as demonstrated below. To define the execution logic, move the cursor over the query in the grid, and click on the settings icon that appears. This opens the Query Configuration form. Enter information in it as follows: Enter a name for the query in the Name field. In this example, let's enter query as the name. In order to specify how each user defined attribute in the input stream is converted to generate the output events, select User Defined Attributes in the Select field. As a result, the User Defined Attributes table appears. The As column of this table displays the attributes of the output stream. To derive the value for each attribute, enter required expressions/values in the Expression column as explained below. The value for name can be derived from the input stream without any further processing. Therefore, enter name as the expression for the name attribute. To derive the value for the totalProduction attribute, the sum of the values for the amount attribute of input events need to be calculated. Therefore, enter the expression as follows to apply the sum() Siddhi function to the amount attribute. sum(amount) Leave the default values of the Output section unchanged. Click Submit to save the information. To add a sink to publish the output events that are directed to the ProductionAlertStream output stream, drag and drop the sink icon (shown below) into the grid. Draw an arrow from the ProductionAlertStream output stream to the sink component to connect them. Click the settings icon on the sink component you added to open the Sink Configuration form. Then enter information as follows. 1. In this example, let's assume that output needs to be generated as logs in the console. To indicate this, select log in the Sink Type field. In the Map Type field, select the format in which the output must be generated. For this example, let's select json . Click Submit to save the information. To align the Siddhi components that you have added to the grid, click Edit and then click Auto-Align . As a result, all the components are horizontally aligned as shown below. Click Source View . The siddhi application is displayed as follows. Click File and then click Save as . The Save to Workspace dialog box appears. In the File Name field, enter SweetProductionAnalysis and click Save .","title":"Creating a Siddhi application in the design view"},{"location":"develop/debugging_siddhi_applications/","text":"Debugging Siddhi Applications \u00b6 WSO2 Streaming Integrator Studio allows debugging tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run on an actual production environment. To debug a Siddhi application, you can run it in the debug mode, apply debug point and then run event simulation so that the specific debug points are analyzed. To run a Siddhi application in the debug mode, follow the procedure below: Info A Siddhi application can be run in the debug mode only in the source view. Start the Streaming Integrator Studio following the instructions in Starting Stream Integration Studio . You are directed to the welcome-page. In this scenario, let's use the existing sample Siddhi application named ReceiveAndCount to demostrate the debugging functionality. To open this Siddhi application, click on the sample. The ReceiveAndCount Siddhi application opens inca new tab. In order to debug the Siddhi file, you need to first save it in the workspace directory. To do this, click File => Save . In the Save to Workspace dialog box that appears, click Save . To run the Siddhi application in the debug mode, click Run => Debug . !!! info This menu option is enabled only when the Siddhi file is saved in the workspace directory as specified in the previous step. As a result, the following log is printed in the console. Also, another console tab is opened with debug options as shown below. Apply debug points for the required queries. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into statements as debug points. Simulate one or more events for the SweetProductionStream stream in the Siddhi application. The first line that is marked as a debug point is highlighted as shown below when they are hit. Info For detailed instructions to simulate events, see the following sections: Simulating a Single Event Simulating Multiple Events via CS Files Simulating Multiple Events via Databases [Generating Random Data](testing-a-Siddhi-Application.md#generating-random-events Two viewing options are provided under both Event State and the Query State sections of the Debug tab for each debug point hit as shown above. To expand the tree and understand the details of the processed attributes and their values etc., click the following icon for the relevant query. When you observe the details, note that the value for outputData in the Event State section is null. This is because the debug point is still at beginning of the query. Also note that the value calculated via the count() function is still displayed as 0 in the Query State section. The following icons are displayed in the Debug tab of the console: Icon Description Click this to proceed from the current debug point to the next available debug point. If there is no debug point marked after the current debug point, the existing debug point continues to be displayed in the tab. Click this to proceed from the current debug point even if no debug point exists after it. Once you navigate to next debug point and see the details by clicking the plus signs as mentioned above you can further analyze the processed attributes and its values as shown below. Note that after the count() aggregate function, a value of 1 has been calculated.","title":"Debugging Siddhi Applications"},{"location":"develop/debugging_siddhi_applications/#debugging-siddhi-applications","text":"WSO2 Streaming Integrator Studio allows debugging tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run on an actual production environment. To debug a Siddhi application, you can run it in the debug mode, apply debug point and then run event simulation so that the specific debug points are analyzed. To run a Siddhi application in the debug mode, follow the procedure below: Info A Siddhi application can be run in the debug mode only in the source view. Start the Streaming Integrator Studio following the instructions in Starting Stream Integration Studio . You are directed to the welcome-page. In this scenario, let's use the existing sample Siddhi application named ReceiveAndCount to demostrate the debugging functionality. To open this Siddhi application, click on the sample. The ReceiveAndCount Siddhi application opens inca new tab. In order to debug the Siddhi file, you need to first save it in the workspace directory. To do this, click File => Save . In the Save to Workspace dialog box that appears, click Save . To run the Siddhi application in the debug mode, click Run => Debug . !!! info This menu option is enabled only when the Siddhi file is saved in the workspace directory as specified in the previous step. As a result, the following log is printed in the console. Also, another console tab is opened with debug options as shown below. Apply debug points for the required queries. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into statements as debug points. Simulate one or more events for the SweetProductionStream stream in the Siddhi application. The first line that is marked as a debug point is highlighted as shown below when they are hit. Info For detailed instructions to simulate events, see the following sections: Simulating a Single Event Simulating Multiple Events via CS Files Simulating Multiple Events via Databases [Generating Random Data](testing-a-Siddhi-Application.md#generating-random-events Two viewing options are provided under both Event State and the Query State sections of the Debug tab for each debug point hit as shown above. To expand the tree and understand the details of the processed attributes and their values etc., click the following icon for the relevant query. When you observe the details, note that the value for outputData in the Event State section is null. This is because the debug point is still at beginning of the query. Also note that the value calculated via the count() function is still displayed as 0 in the Query State section. The following icons are displayed in the Debug tab of the console: Icon Description Click this to proceed from the current debug point to the next available debug point. If there is no debug point marked after the current debug point, the existing debug point continues to be displayed in the tab. Click this to proceed from the current debug point even if no debug point exists after it. Once you navigate to next debug point and see the details by clicking the plus signs as mentioned above you can further analyze the processed attributes and its values as shown below. Note that after the count() aggregate function, a value of 1 has been calculated.","title":"Debugging Siddhi Applications"},{"location":"develop/deploying-Streaming-Applications/","text":"Deploying Siddhi Applications \u00b6 After creating and testing and debugging a Siddhi application, you need to deploy it in the Streaming Integrator server. You can also deploy it in Docker and Kubernetes. To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below: Info To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <SI_HOME> and the home directory of Streaming Integrator Tooling is referred to as <SI_TOOLING_HOME> . Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing one of the following commands: On Windows: server.bat --run On Linux/Mac OS: ./server.sh In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as follows. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the Siddhi applications that you want to deploy as shown below. Then select the check boxes for the servers in which you want to deploy them. Click Deploy . As a result, the Siddhi application(s) yoiu selected is saved in the <SI_HOME>/deployment/siddhi-files directory, and the following is message displayed in the dialog box.","title":"Deploying Siddhi Applications"},{"location":"develop/deploying-Streaming-Applications/#deploying-siddhi-applications","text":"After creating and testing and debugging a Siddhi application, you need to deploy it in the Streaming Integrator server. You can also deploy it in Docker and Kubernetes. To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below: Info To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <SI_HOME> and the home directory of Streaming Integrator Tooling is referred to as <SI_TOOLING_HOME> . Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing one of the following commands: On Windows: server.bat --run On Linux/Mac OS: ./server.sh In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as follows. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the Siddhi applications that you want to deploy as shown below. Then select the check boxes for the servers in which you want to deploy them. Click Deploy . As a result, the Siddhi application(s) yoiu selected is saved in the <SI_HOME>/deployment/siddhi-files directory, and the following is message displayed in the dialog box.","title":"Deploying Siddhi Applications"},{"location":"develop/developing-streaming-integration-solutions/","text":"Developing Streaming Integrator Solutions \u00b6 This section provides an overview of the development flow in the Streaming Integrator. Developing a Streaming Integrator solution involves the following four steps. Step Description Step 1: Installing SI Tooling This involves downloading and installing the Streaming Integration Tooling in which Siddhi applications are designed. For more information, see the following topics: - Installing the Streaming Integrator in a Virtual Machine - Installing the Streaming Integrator in Docker - Installing the Streaming Integrator in Kubernetes Step 2: Creating Siddhi Applications Siddhi applications can be designed in the Streaming Integrator Tooling via the source view or the design view. For detailed instructions, see Creating Siddhi Applications . Step 3: Testing Siddhi Applications Once a Siddhi application is created, you can test it before using it in a production environmenty by simulating events to it. For more information, see Testing Siddhi Applications . Step 4: Debugging Siddhi Applications If there are errors in your Siddhi application design that prevent it from functioning as expected, you can debug it via the Streaming Integrator Tooling to identify them so that you can correct them before deploying and running the Siddhi Application. For more information, see Debugging Siddhi Applications . Step 5: Deploying Siddhi Applications Once your Siddhi application is created and verified via the testing and debugging fiunctionality in the Streaming Integrator Tooling, you can deploy it in the Streaming Integrator server, or deploy it in a Docker/Kubernetes environment. For more information about, see the following topics: - Deploying Siddhi Applications - Exporting Siddhi Applications Step 6: Running Siddhi Applications This involves running the Siddhi application in the server where you deployed them. To try this out, you can follow the Streaming Integrator Tutorials .","title":"Developing Streaming Integrator Solutions"},{"location":"develop/developing-streaming-integration-solutions/#developing-streaming-integrator-solutions","text":"This section provides an overview of the development flow in the Streaming Integrator. Developing a Streaming Integrator solution involves the following four steps. Step Description Step 1: Installing SI Tooling This involves downloading and installing the Streaming Integration Tooling in which Siddhi applications are designed. For more information, see the following topics: - Installing the Streaming Integrator in a Virtual Machine - Installing the Streaming Integrator in Docker - Installing the Streaming Integrator in Kubernetes Step 2: Creating Siddhi Applications Siddhi applications can be designed in the Streaming Integrator Tooling via the source view or the design view. For detailed instructions, see Creating Siddhi Applications . Step 3: Testing Siddhi Applications Once a Siddhi application is created, you can test it before using it in a production environmenty by simulating events to it. For more information, see Testing Siddhi Applications . Step 4: Debugging Siddhi Applications If there are errors in your Siddhi application design that prevent it from functioning as expected, you can debug it via the Streaming Integrator Tooling to identify them so that you can correct them before deploying and running the Siddhi Application. For more information, see Debugging Siddhi Applications . Step 5: Deploying Siddhi Applications Once your Siddhi application is created and verified via the testing and debugging fiunctionality in the Streaming Integrator Tooling, you can deploy it in the Streaming Integrator server, or deploy it in a Docker/Kubernetes environment. For more information about, see the following topics: - Deploying Siddhi Applications - Exporting Siddhi Applications Step 6: Running Siddhi Applications This involves running the Siddhi application in the server where you deployed them. To try this out, you can follow the Streaming Integrator Tutorials .","title":"Developing Streaming Integrator Solutions"},{"location":"develop/exporting-Siddhi-Applications/","text":"Exporting Siddhi Applications \u00b6 The Streaming Integrator Tooling allows you to export one or more Siddhi files as a Docker or Kubernetes artifact in order to run those Siddhi applications within a Docker or Kubernetes environment. To export Siddhi files as Docker or Kubernetes artifacts, follow the steps given below. Exporting Siddhi applications as a Docker Image \u00b6 To export one or more Siddhi applications as a Docker image, follow the procedure below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Docker image. For more information, see Creating a Siddhi Application . Click Export menu option and then click For Docker . As a result, the following wizard opens. In Step 1: Select Siddhi Apps , select one or more Siddhi applications to be included in the Docker image. Click Next . In Step 2: Template Siddhi Apps , template the Siddhi application if required. You can define a template within the Siddhi application via the ${...} pattern. In the following example, ${http.port} ) is added as a template within the source configuration. @source(type='http', receiver.url='http://localhost:${http.port}/${http.context}', @map(type = 'json')) define stream StockQuoteStream(symbol string, price double, volume int); Once the templates are defined, click Next . In Step 3: Update Streaming Integrator configurations , update the deployment.yaml file of the Streaming Integrator to enter configurations specific for the Docker image. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., ${...} ) to specify a template within the configuration file. Once the configuration is complete, click Next . In Step 4: Populate arguments template , define values for the template fields defined in Step 2 and Step 3 . This step lists down all the template fields. Therefore you can set the relevant value for each field. Once the template values are set, click Next . In Step 5: Bundle additional dependencies , select the additional JARs to be shipped with the Docker image. Once the everything is complete, click Export to export a ZIP file with the following directory structure. . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u2514\u2500\u2500 siddhi-files \u251c\u2500\u2500 <SIDDHI_FILE>.siddhi \u251c\u2500\u2500 ... \u2514\u2500\u2500 <SIDDHI_FILE>.siddhi For more information on Siddhi Docker artifacts, see Siddhi 5.1 as a Docker Microservice . Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input. Exporting Siddhi Applications for Kubernetes \u00b6 To export one or more Siddhi applications for Kubernetes, follow the procedure below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Kubernetes artifact. For more information, see Creating a Siddhi Application . Click Export menu option and then click For Kubernetes . As a result, the following wizard opens. In Step 1: Select Siddhi Apps , select one or more Siddhi applications to be included in the Kubernetes artifact. Click Next . In Step 2: Template Siddhi Apps , template the Siddhi application if required. You can define a template within the Siddhi application via the ${...} pattern. In the following example, ${http.port} ) is added as a template within the source configuration. @source(type='http', receiver.url='http://localhost:${http.port}/${http.context}', @map(type = 'json')) define stream StockQuoteStream(symbol string, price double, volume int); Once the templates are defined, click Next . In Step 3: Update Streaming Integrator configurations , update the deployment.yaml file of the Streaming Integrator to enter configurations specific for the Kubernetes artifact. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., ${...} ) to specify a template within the configuration file. Once the configuration is complete, click Next . In Step 4: Populate arguments template , define values for the template fields defined in Step 2 and Step 3 . This step lists down all the template fields. Therefore you can set the relevant value for each field. Once the template values are set, click Next . In Step 5: Bundle additional dependencies , select the additional JARs to be shipped with the Docker image. Once the JARs are selected, click Next . In Step 7: Add Kubernetes config , click Export . This downloads the ZIP file with the following directory structure. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 siddhi-files \u2502 \u251c\u2500\u2500 <SIDDHI_FILE>.siddhi \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 <SIDDHI_FILE>.siddhi \u2514\u2500\u2500 siddhi-process.yaml Once the ZIP is downloaded, you can extract and open the <ZIP_HOME>/siddhi-process.yaml via a text editor to modify the SiddhiProcess configuration. For more information on SiddhiProcess Kubernetes configuration, see Siddhi 5.1 as a Kubernetes Microservice documentation. Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input.","title":"Exporting Siddhi Applications"},{"location":"develop/exporting-Siddhi-Applications/#exporting-siddhi-applications","text":"The Streaming Integrator Tooling allows you to export one or more Siddhi files as a Docker or Kubernetes artifact in order to run those Siddhi applications within a Docker or Kubernetes environment. To export Siddhi files as Docker or Kubernetes artifacts, follow the steps given below.","title":"Exporting Siddhi Applications"},{"location":"develop/exporting-Siddhi-Applications/#exporting-siddhi-applications-as-a-docker-image","text":"To export one or more Siddhi applications as a Docker image, follow the procedure below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Docker image. For more information, see Creating a Siddhi Application . Click Export menu option and then click For Docker . As a result, the following wizard opens. In Step 1: Select Siddhi Apps , select one or more Siddhi applications to be included in the Docker image. Click Next . In Step 2: Template Siddhi Apps , template the Siddhi application if required. You can define a template within the Siddhi application via the ${...} pattern. In the following example, ${http.port} ) is added as a template within the source configuration. @source(type='http', receiver.url='http://localhost:${http.port}/${http.context}', @map(type = 'json')) define stream StockQuoteStream(symbol string, price double, volume int); Once the templates are defined, click Next . In Step 3: Update Streaming Integrator configurations , update the deployment.yaml file of the Streaming Integrator to enter configurations specific for the Docker image. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., ${...} ) to specify a template within the configuration file. Once the configuration is complete, click Next . In Step 4: Populate arguments template , define values for the template fields defined in Step 2 and Step 3 . This step lists down all the template fields. Therefore you can set the relevant value for each field. Once the template values are set, click Next . In Step 5: Bundle additional dependencies , select the additional JARs to be shipped with the Docker image. Once the everything is complete, click Export to export a ZIP file with the following directory structure. . \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u2514\u2500\u2500 siddhi-files \u251c\u2500\u2500 <SIDDHI_FILE>.siddhi \u251c\u2500\u2500 ... \u2514\u2500\u2500 <SIDDHI_FILE>.siddhi For more information on Siddhi Docker artifacts, see Siddhi 5.1 as a Docker Microservice . Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input.","title":"Exporting Siddhi applications as a Docker Image"},{"location":"develop/exporting-Siddhi-Applications/#exporting-siddhi-applications-for-kubernetes","text":"To export one or more Siddhi applications for Kubernetes, follow the procedure below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Kubernetes artifact. For more information, see Creating a Siddhi Application . Click Export menu option and then click For Kubernetes . As a result, the following wizard opens. In Step 1: Select Siddhi Apps , select one or more Siddhi applications to be included in the Kubernetes artifact. Click Next . In Step 2: Template Siddhi Apps , template the Siddhi application if required. You can define a template within the Siddhi application via the ${...} pattern. In the following example, ${http.port} ) is added as a template within the source configuration. @source(type='http', receiver.url='http://localhost:${http.port}/${http.context}', @map(type = 'json')) define stream StockQuoteStream(symbol string, price double, volume int); Once the templates are defined, click Next . In Step 3: Update Streaming Integrator configurations , update the deployment.yaml file of the Streaming Integrator to enter configurations specific for the Kubernetes artifact. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., ${...} ) to specify a template within the configuration file. Once the configuration is complete, click Next . In Step 4: Populate arguments template , define values for the template fields defined in Step 2 and Step 3 . This step lists down all the template fields. Therefore you can set the relevant value for each field. Once the template values are set, click Next . In Step 5: Bundle additional dependencies , select the additional JARs to be shipped with the Docker image. Once the JARs are selected, click Next . In Step 7: Add Kubernetes config , click Export . This downloads the ZIP file with the following directory structure. \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 configurations.yaml \u251c\u2500\u2500 siddhi-files \u2502 \u251c\u2500\u2500 <SIDDHI_FILE>.siddhi \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 <SIDDHI_FILE>.siddhi \u2514\u2500\u2500 siddhi-process.yaml Once the ZIP is downloaded, you can extract and open the <ZIP_HOME>/siddhi-process.yaml via a text editor to modify the SiddhiProcess configuration. For more information on SiddhiProcess Kubernetes configuration, see Siddhi 5.1 as a Kubernetes Microservice documentation. Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input.","title":"Exporting Siddhi Applications for Kubernetes"},{"location":"develop/installing-siddhi-extensions/","text":"Installing Siddhi Extensions \u00b6 Streaming Integrator Tooling allows you to install or un-install Siddhi extensions through the Extensions Installer. To install or un-install Siddhi extensions, follow the steps given below. Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Click File menu option and then click Extension Installation . // TODO rename to 'Install Extensions'? \u2192 // TODO insert file menu \u2192 As a result, the following dialog box opens. // TODO insert dialog \u2192 Locate the extension you wish to install/un-install. Each extension's name, installation status, and the action button will be shown. Tip If the extension you are searching for is not shown within the first set of extensions that are listed down, enter the name of the extension in the search bar, which will filter the extension(s) matching the entered keyword. // TODO insert search keyword-filtered extension \u2192 Following are the possible values for an extension's status. | Status | Description | |-------------------------------|---------------------------------------------------| | Installed | The extension has been completely installed. | | Not-Installed | The extension has not been installed. | | Partially Installed | At least one of the dependencies of the extension has to be installed. This can be due to a failure in installing dependencies, or there is a dependency that should be manually installed .| The editor should be restarted after installing or un-installing an extension. Manually Installable Dependencies \u00b6 Certain dependencies of some extensions can not be automatically downloaded through the Siddhi Extensions Installer. These dependencies should be manually downloaded and installed. When there is at least one such dependency for an extension, an icon will be shown as follows, next to the extension's status. // TODO insert Extension row with (i) image \u2192 Clicking this icon will open the following dialog box. The dialog box will show each dependency of the particular extension that should be manually installed, along with the following information. - Instructions - Instructions to follow, in order to download the *.jar file for the dependency. - Installation Locations - The downloaded *.jar file should be placed in its specific location, as given below. | Location | Directory | |-------------------|-----------------------------------------------| | runtime | `<SI_HOME>/lib` | | samples | `<SI_HOME>/samples/sample-clients/lib` | !!! Note When the type is _bundle_, and the downloaded `*.jar` file is not an OSGi bundle, the jar should be converted to an OSGi bundle. For this, navigate to the `<SI_HOME>/bin` directory, and then issue the command `./jartobundle.sh <PATH_TO_DOWNLOADED_NON-OSGi_JAR> ../lib`.","title":"Installing Siddhi Extensions"},{"location":"develop/installing-siddhi-extensions/#installing-siddhi-extensions","text":"Streaming Integrator Tooling allows you to install or un-install Siddhi extensions through the Extensions Installer. To install or un-install Siddhi extensions, follow the steps given below. Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: tooling.bat For Linux: ./tooling.sh The Streaming Integrator Tooling opens as shown below. Click File menu option and then click Extension Installation . // TODO rename to 'Install Extensions'? \u2192 // TODO insert file menu \u2192 As a result, the following dialog box opens. // TODO insert dialog \u2192 Locate the extension you wish to install/un-install. Each extension's name, installation status, and the action button will be shown. Tip If the extension you are searching for is not shown within the first set of extensions that are listed down, enter the name of the extension in the search bar, which will filter the extension(s) matching the entered keyword. // TODO insert search keyword-filtered extension \u2192 Following are the possible values for an extension's status. | Status | Description | |-------------------------------|---------------------------------------------------| | Installed | The extension has been completely installed. | | Not-Installed | The extension has not been installed. | | Partially Installed | At least one of the dependencies of the extension has to be installed. This can be due to a failure in installing dependencies, or there is a dependency that should be manually installed .| The editor should be restarted after installing or un-installing an extension.","title":"Installing Siddhi Extensions"},{"location":"develop/installing-siddhi-extensions/#manually-installable-dependencies","text":"Certain dependencies of some extensions can not be automatically downloaded through the Siddhi Extensions Installer. These dependencies should be manually downloaded and installed. When there is at least one such dependency for an extension, an icon will be shown as follows, next to the extension's status. // TODO insert Extension row with (i) image \u2192 Clicking this icon will open the following dialog box. The dialog box will show each dependency of the particular extension that should be manually installed, along with the following information. - Instructions - Instructions to follow, in order to download the *.jar file for the dependency. - Installation Locations - The downloaded *.jar file should be placed in its specific location, as given below. | Location | Directory | |-------------------|-----------------------------------------------| | runtime | `<SI_HOME>/lib` | | samples | `<SI_HOME>/samples/sample-clients/lib` | !!! Note When the type is _bundle_, and the downloaded `*.jar` file is not an OSGi bundle, the jar should be converted to an OSGi bundle. For this, navigate to the `<SI_HOME>/bin` directory, and then issue the command `./jartobundle.sh <PATH_TO_DOWNLOADED_NON-OSGi_JAR> ../lib`.","title":"Manually Installable Dependencies"},{"location":"develop/siddhi-Application-Overview/","text":"Siddhi Application Overview \u00b6 A Siddhi application (.siddhi) file is the deployment artifact containing the Stream Processing logic for WSO2 Stream Processor. The format of a Siddhi application is as follows: @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') /* Sample Siddhi App block comment */ -- Sample Siddhi App line comment @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream; Basic information about Siddhi applications \u00b6 Following are some important things to note about Siddhi applications: The file name of each Siddhi application must be equal to the name specified via the @App:name() annotation. e.g., In the sample Siddhi application given above, the application name is ReceiveAndCount . Therefore, the Siddhi file name must be ReceiveAndCount.Siddhi . It is optional to provide a description via the @App:description() annotation. The definitions of the required streams, windows, tables, triggers and aggregations need to be included before the Siddhi queries. e.g., In the above sample Siddhi file, the streams (lines 14 and 17) are defined before the queries (lines 21-23). Siddhi can infer the definition of the streams. It is not required to define all the streams. However, if annotations need to be added to a stream, that stream must be defined. In the above sample, lines 4-6 nd 8 demonstrate how to nclude comments within Siddhi applications. For more information about Siddhi applications, see Siddhi Application at Siddhi Streaming SQL Guide . Common elements of a Siddhi application \u00b6 This section explains the common types of definitions and queries that are included in Siddhi application: Queries \u00b6 Queries define the logical processing and selections that must be executed for streaming events. They consume from the pre-defined streams/ windows/ tables/ aggregations, process them in a streaming manner, and insert the output to another stream, window or table. For more information about Siddhi queries, see Queries at Siddhi Streaming SQL Guide . Streams \u00b6 Streams are one of the core elements of a stream processing application. A stream is a logical series of events ordered in time with a uniquely identifiable name and set of defined attributes with specific data types defining its schema. In Siddhi, streams are defined by giving it a name and the set of attributes it contains. Lines 14 and 17 of the above sample are examples of defined streams. For more information on Siddhi streams, see Streams at Siddhi Streaming SQL Guide . Tables \u00b6 A table is a collection of events that can be used to store streaming data. The capability to store events in a table allows you to query for stored events later or process them again with a different stream. The generic table concept holds here as well, however, Siddhi tables also support numerous table specific data manipulations such as defining primary keys, indexing, etc. For more information on Siddhi tables, see Storage Integration and Tables at Siddhi Streaming SQL Guide . Windows \u00b6 Windows allow you to retain a collection of streaming events based on a time duration (time window), or a given number of events (length window). It allows you to process events that fall into the defined window or expire from it. For more information on Siddhi windows, see Windows at Siddhi Streaming SQL Guide . Aggregations \u00b6 Aggregation allows you to aggregate streaming events for different time granularities. The time granularities supported are seconds, minutes, hours, days, months and years. Aggregations such as sum, min, avg can be calculated for the desired duration(s) via Siddhi aggregation. For more information on Siddhi aggregations, see Aggregations at Siddhi Streaming SQL Guide . The elements mentioned above work together in a Siddhi application to form an event flow. To understand how the elements os a Siddhi application are interconnected, you can view the design view of a Siddhi application. For more information, see Stream Processor Studio Overview .","title":"Siddhi Application Overview"},{"location":"develop/siddhi-Application-Overview/#siddhi-application-overview","text":"A Siddhi application (.siddhi) file is the deployment artifact containing the Stream Processing logic for WSO2 Stream Processor. The format of a Siddhi application is as follows: @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') /* Sample Siddhi App block comment */ -- Sample Siddhi App line comment @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream;","title":"Siddhi Application Overview"},{"location":"develop/siddhi-Application-Overview/#basic-information-about-siddhi-applications","text":"Following are some important things to note about Siddhi applications: The file name of each Siddhi application must be equal to the name specified via the @App:name() annotation. e.g., In the sample Siddhi application given above, the application name is ReceiveAndCount . Therefore, the Siddhi file name must be ReceiveAndCount.Siddhi . It is optional to provide a description via the @App:description() annotation. The definitions of the required streams, windows, tables, triggers and aggregations need to be included before the Siddhi queries. e.g., In the above sample Siddhi file, the streams (lines 14 and 17) are defined before the queries (lines 21-23). Siddhi can infer the definition of the streams. It is not required to define all the streams. However, if annotations need to be added to a stream, that stream must be defined. In the above sample, lines 4-6 nd 8 demonstrate how to nclude comments within Siddhi applications. For more information about Siddhi applications, see Siddhi Application at Siddhi Streaming SQL Guide .","title":"Basic information about Siddhi applications"},{"location":"develop/siddhi-Application-Overview/#common-elements-of-a-siddhi-application","text":"This section explains the common types of definitions and queries that are included in Siddhi application:","title":"Common elements of a Siddhi application"},{"location":"develop/siddhi-Application-Overview/#queries","text":"Queries define the logical processing and selections that must be executed for streaming events. They consume from the pre-defined streams/ windows/ tables/ aggregations, process them in a streaming manner, and insert the output to another stream, window or table. For more information about Siddhi queries, see Queries at Siddhi Streaming SQL Guide .","title":"Queries"},{"location":"develop/siddhi-Application-Overview/#streams","text":"Streams are one of the core elements of a stream processing application. A stream is a logical series of events ordered in time with a uniquely identifiable name and set of defined attributes with specific data types defining its schema. In Siddhi, streams are defined by giving it a name and the set of attributes it contains. Lines 14 and 17 of the above sample are examples of defined streams. For more information on Siddhi streams, see Streams at Siddhi Streaming SQL Guide .","title":"Streams"},{"location":"develop/siddhi-Application-Overview/#tables","text":"A table is a collection of events that can be used to store streaming data. The capability to store events in a table allows you to query for stored events later or process them again with a different stream. The generic table concept holds here as well, however, Siddhi tables also support numerous table specific data manipulations such as defining primary keys, indexing, etc. For more information on Siddhi tables, see Storage Integration and Tables at Siddhi Streaming SQL Guide .","title":"Tables"},{"location":"develop/siddhi-Application-Overview/#windows","text":"Windows allow you to retain a collection of streaming events based on a time duration (time window), or a given number of events (length window). It allows you to process events that fall into the defined window or expire from it. For more information on Siddhi windows, see Windows at Siddhi Streaming SQL Guide .","title":"Windows"},{"location":"develop/siddhi-Application-Overview/#aggregations","text":"Aggregation allows you to aggregate streaming events for different time granularities. The time granularities supported are seconds, minutes, hours, days, months and years. Aggregations such as sum, min, avg can be calculated for the desired duration(s) via Siddhi aggregation. For more information on Siddhi aggregations, see Aggregations at Siddhi Streaming SQL Guide . The elements mentioned above work together in a Siddhi application to form an event flow. To understand how the elements os a Siddhi application are interconnected, you can view the design view of a Siddhi application. For more information, see Stream Processor Studio Overview .","title":"Aggregations"},{"location":"develop/simulating-Events/","text":"Simulating Events \u00b6 Simulating events involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for debugging and monitoring the event receivers and publishers, execution plans and event formatters. Function REST API Saving a simulation configuration Single Event Simulation : POST http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: POST http://<SP_HOST>:<API_PORT>/simulation/feed Editing a simulation configuration Single Event Simulation : PUT http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: PUT http://<SP_HOST>:<API_PORT>/simulation/feed Deleting a simulation configuration Single Event Simulation : DELETE http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: DELETE http://<SP_HOST>:<API_PORT>/simulation/feed Retrieving a simulation configuration Single Event Simulation : GET http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: GET http://<SP_HOST>:<API_PORT>/simulation/feed Uploading a CSV file POST http://<SP_HOST>:<API_PORT>/simulation/feed Editing and uploaded CSV file PUT -F 'file=@/{path to csv file}' http://<SP_HOST>:<API_PORT>/simulation/files/{fileName}?fileName={fileName} Deleting an uploaded CSV file DELETE http://<SP_HOST>:<API_PORT>/simulation/files/{fileName} Pausing an event simulation POST http://<SP_HOST>:<API_PORT>/ simulation/feed/{simulationName}/?action=pause Resuming an event simulation POST http://<SP_HOST>:<API_PORT>/ simulation/feed/{simulationName}/?action=resume Stopping an event simulation DELETE http://<SP_HOST>:<API_PORT>/simulation/feed/{simulationName} The following sections cover how events can be simulated. Saving a simulation configuration Editing a simulation configuration Deleting a simulation configuration Retrieving a simulation configuration Uploading a CSV file Editing an uploaded CSV file Deleting an uploaded CSV file Pausing an event simulation Resuming an event simulation Stopping an event simulation Saving a simulation configuration \u00b6 To simulate events for WSO2 SP, you should first save the event simulator configuration in the <SP_HOME>/deployment/simulator/simulationConfigs directory by sending a POST request to a REST API as described below. REST API \u00b6 The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event POST http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events POST http:// <SP_HOST>:<API_PORT> /simulation/feed/ Sample cURL command \u00b6 curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"simulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }' Sample output \u00b6 { \"status\": \"CREATED\", \"message\": \"Successfully uploaded simulation configuration 'simulationPrimitive'\" } REST API response \u00b6 200 if the simulation configuration is successfully saved. 409 if a simulation configuration with the specified name already exists. 400 if the configuration provided is not in a valid JSON format. For descriptions of the HTTP status codes, see HTTP Status Codes . Editing a simulation configuration \u00b6 To edit a simulation configuration that is already saved, a PUT request should be sent to a REST API as explained below. REST API \u00b6 The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event PUT http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events PUT http://<SP_HOST>: <API_PORT> /simulation/feed/{feed name} Sample cURL command \u00b6 curl -X PUT \\ http://localhost:9390/simulation/feed/simulationPrimitive \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"updatedSimulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"10\", \"description\": \"Updating the simulation configuration\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }' Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully updated simulation configuration 'simulationPrimitive'.\" } REST API response \u00b6 200 if the simulation configuration is successfully updated. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. 400 if the file specified is not a CSV file, or if the file does not exist in the path specified. 403 if the size of the file specified exceeds the maximum size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting a simulation configuration \u00b6 To delete an event simulation file that is already saved in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory, a DELETE request should be sent to a REST API as explained below. REST API \u00b6 The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event DELETE http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events DELETE http:// <SP_HOST> : <API_PORT> /simulation/feed/ Sample cURL command \u00b6 curl -X DELETE 'http://localhost:9390/simulation/feed/simulationPrimitive' Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully deleted simulation configuration 'simulationPrimitive'\" } REST API response \u00b6 200 if the simulation configuration is successfully deleted. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Retrieving a simulation configuration \u00b6 To view a simulation configuration saved in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory via the CLI, a GET request should be sent to a REST API as explained below. REST API \u00b6 The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event GET http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events GET http:// <SP_HOST>:<API_PORT> /simulation/feed/ Sample cURL command \u00b6 curl -X GET 'http://localhost:9390/simulation/feed/simulationPrimitive' Sample output \u00b6 { \"status\": \"OK\", \"message\": \"{\\\"Simulation configuration\\\":{\\\"sources\\\":[{\\\"timestampInterval\\\":\\\"1000\\\",\\\"simulationType\\\":\\\"RANDOM_DATA_SIMULATION\\\",\\\"attributeConfiguration\\\":[{\\\"length\\\":\\\"5\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"STRING\\\"},{\\\"min\\\":\\\"0\\\",\\\"max\\\":\\\"999\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"INT\\\"}],\\\"streamName\\\":\\\"FooStream\\\",\\\"siddhiAppName\\\":\\\"TestExecutionPlan\\\"}],\\\"properties\\\":{\\\"simulationName\\\":\\\"simulationPrimitive\\\",\\\"description\\\":\\\"Updating the simulation configuration\\\",\\\"timeInterval\\\":\\\"1000\\\",\\\"endTimestamp\\\":\\\"\\\",\\\"startTimestamp\\\":\\\"\\\",\\\"noOfEvents\\\":\\\"10\\\"}}}\" } REST API Response \u00b6 200 if the simulation configuration is successfully retrieved. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Uploading a CSV file \u00b6 To simulate events from a CSV file, the required CSV file needs to exist in the \\<SP_HOME>/wso2/editor/deployment/csv-files directory. REST API \u00b6 A POST request should be sent to the following API. POST http://<SP_HOST>:<API_PORT>/simulation/feed Sample cURL command \u00b6 curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"FeedSimulationTest\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"CSV_SIMULATION\", \"fileName\": \"myEvents.csv\", \"delimiter\": \",\", \"isOrdered\": true, \"indices\": \"0,1\" } ] }' Sample output \u00b6 { \"status\": \"CREATED\", \"message\": \"Successfully uploaded simulation configuration 'FeedSimulationTest'\" } REST API response \u00b6 200 if the CSV file is successfully uploaded. 409 if a CSV file with the file name specified already exists in the \\<SP_HOME>/wso2/editor/deployment/csv-files directory. 400 if the specified file is not a CSV file or if the specified file path is not valid. 403 if the size of the file specified exceeds the maximum file size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes . Editing an uploaded CSV file \u00b6 This section explains how to edit a CSV file that is already uploaded to the <SP_HOME>/wso2/editor/deployment/csv-files directory. REST API \u00b6 A PUT request should be sent to the following API. PUT -F 'file=@/{path to csv file}' http://<SP_HOST>:<API_PORT>/simulation/files/{fileName}?fileName={fileName} Sample cURL command \u00b6 curl -X PUT -F 'file=@/home/nadeeka/Desktop/editedMyEvents.csv' http://localhost:9390/simulation/files/myEvents.csv?fileName=myEvents.csv Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully updated CSV file 'myEvents.csv' with file ' editedMyEvents.csv'.\" } REST API response \u00b6 200 if the CSV file is successfully updated. 404 if the specified CSV file does not exist in the <SP_HOME>/deployment/simulator/csvFiles directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting an uploaded CSV file \u00b6 This section explains how to delete a CSV file that is already uploaded to the <SP_HOME>/wso2/editor/deployment/csv-files directory. REST API \u00b6 A DELETE request should be sent to the following API. DELETE http://<SP_HOST>:<API_PORT>/simulation/files/{fileName} Sample cURL command \u00b6 curl -X DELETE http://localhost:9390/simulation/files/myEvents.csv Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully deleted file 'myEvents.csv'\" } REST API response \u00b6 200 if the CSV file is successfully deleted. 404 if the specified CSV file does not exist in the <SP_HOME>/wso2/editor/deployment/csv-files directory. Pausing an event simulation \u00b6 This section explains how to pause an event simulation that has already started. REST API \u00b6 A POST request should be sent to the following API. POST http://<SP_HOST>:<API_PORT>/simulation/feed/{simulationName}/?action=pause Sample cURL command \u00b6 curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=pause Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully paused event simulation 'simulationPrimitive'.\" } REST API response \u00b6 200 if the event simulation is successfully paused. 409 if the event simulation is already paused. Resuming an event simulation \u00b6 This section explains how to resume an event simulation that has already paused. REST API \u00b6 A POST request should be sent to the following API POST http://<SP_HOST>:<API_PORT>/simulation/feed/{ simulationName }/?action=resume Sample cURL command \u00b6 curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=resume Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully resumed event simulation 'simulationPrimitive'.\" } REST API response \u00b6 200 if the event simulation is successfully resumed. Stopping an event simulation \u00b6 This section explains how to stop an event simulation. REST API \u00b6 A POST request should be sent to the following API POST http://<SP_HOST>:<API_PORT>/simulation/feed/{ simulationName }/?action=stop Sample cURL command \u00b6 curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=stop Sample output \u00b6 { \"status\": \"OK\", \"message\": \"Successfully stopped event simulation 'simulationPrimitive'.\" } REST API response \u00b6 200 if the event simulation is successfully stoped.","title":"Simulating Events"},{"location":"develop/simulating-Events/#simulating-events","text":"Simulating events involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for debugging and monitoring the event receivers and publishers, execution plans and event formatters. Function REST API Saving a simulation configuration Single Event Simulation : POST http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: POST http://<SP_HOST>:<API_PORT>/simulation/feed Editing a simulation configuration Single Event Simulation : PUT http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: PUT http://<SP_HOST>:<API_PORT>/simulation/feed Deleting a simulation configuration Single Event Simulation : DELETE http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: DELETE http://<SP_HOST>:<API_PORT>/simulation/feed Retrieving a simulation configuration Single Event Simulation : GET http://<SP_HOST>:<API_PORT>/simulation/single Multiple Event Simulation: GET http://<SP_HOST>:<API_PORT>/simulation/feed Uploading a CSV file POST http://<SP_HOST>:<API_PORT>/simulation/feed Editing and uploaded CSV file PUT -F 'file=@/{path to csv file}' http://<SP_HOST>:<API_PORT>/simulation/files/{fileName}?fileName={fileName} Deleting an uploaded CSV file DELETE http://<SP_HOST>:<API_PORT>/simulation/files/{fileName} Pausing an event simulation POST http://<SP_HOST>:<API_PORT>/ simulation/feed/{simulationName}/?action=pause Resuming an event simulation POST http://<SP_HOST>:<API_PORT>/ simulation/feed/{simulationName}/?action=resume Stopping an event simulation DELETE http://<SP_HOST>:<API_PORT>/simulation/feed/{simulationName} The following sections cover how events can be simulated. Saving a simulation configuration Editing a simulation configuration Deleting a simulation configuration Retrieving a simulation configuration Uploading a CSV file Editing an uploaded CSV file Deleting an uploaded CSV file Pausing an event simulation Resuming an event simulation Stopping an event simulation","title":"Simulating Events"},{"location":"develop/simulating-Events/#saving-a-simulation-configuration","text":"To simulate events for WSO2 SP, you should first save the event simulator configuration in the <SP_HOME>/deployment/simulator/simulationConfigs directory by sending a POST request to a REST API as described below.","title":"Saving a simulation configuration"},{"location":"develop/simulating-Events/#rest-api","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event POST http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events POST http:// <SP_HOST>:<API_PORT> /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command","text":"curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"simulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output","text":"{ \"status\": \"CREATED\", \"message\": \"Successfully uploaded simulation configuration 'simulationPrimitive'\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response","text":"200 if the simulation configuration is successfully saved. 409 if a simulation configuration with the specified name already exists. 400 if the configuration provided is not in a valid JSON format. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#editing-a-simulation-configuration","text":"To edit a simulation configuration that is already saved, a PUT request should be sent to a REST API as explained below.","title":"Editing a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_1","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event PUT http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events PUT http://<SP_HOST>: <API_PORT> /simulation/feed/{feed name}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_1","text":"curl -X PUT \\ http://localhost:9390/simulation/feed/simulationPrimitive \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"updatedSimulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"10\", \"description\": \"Updating the simulation configuration\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_1","text":"{ \"status\": \"OK\", \"message\": \"Successfully updated simulation configuration 'simulationPrimitive'.\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_1","text":"200 if the simulation configuration is successfully updated. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. 400 if the file specified is not a CSV file, or if the file does not exist in the path specified. 403 if the size of the file specified exceeds the maximum size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#deleting-a-simulation-configuration","text":"To delete an event simulation file that is already saved in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory, a DELETE request should be sent to a REST API as explained below.","title":"Deleting a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_2","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event DELETE http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events DELETE http:// <SP_HOST> : <API_PORT> /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_2","text":"curl -X DELETE 'http://localhost:9390/simulation/feed/simulationPrimitive'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_2","text":"{ \"status\": \"OK\", \"message\": \"Successfully deleted simulation configuration 'simulationPrimitive'\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_2","text":"200 if the simulation configuration is successfully deleted. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#retrieving-a-simulation-configuration","text":"To view a simulation configuration saved in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory via the CLI, a GET request should be sent to a REST API as explained below.","title":"Retrieving a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_3","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event GET http://<SP_HOST>:<API_PORT>/simulation/single Simulating a multiple events GET http:// <SP_HOST>:<API_PORT> /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_3","text":"curl -X GET 'http://localhost:9390/simulation/feed/simulationPrimitive'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_3","text":"{ \"status\": \"OK\", \"message\": \"{\\\"Simulation configuration\\\":{\\\"sources\\\":[{\\\"timestampInterval\\\":\\\"1000\\\",\\\"simulationType\\\":\\\"RANDOM_DATA_SIMULATION\\\",\\\"attributeConfiguration\\\":[{\\\"length\\\":\\\"5\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"STRING\\\"},{\\\"min\\\":\\\"0\\\",\\\"max\\\":\\\"999\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"INT\\\"}],\\\"streamName\\\":\\\"FooStream\\\",\\\"siddhiAppName\\\":\\\"TestExecutionPlan\\\"}],\\\"properties\\\":{\\\"simulationName\\\":\\\"simulationPrimitive\\\",\\\"description\\\":\\\"Updating the simulation configuration\\\",\\\"timeInterval\\\":\\\"1000\\\",\\\"endTimestamp\\\":\\\"\\\",\\\"startTimestamp\\\":\\\"\\\",\\\"noOfEvents\\\":\\\"10\\\"}}}\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_3","text":"200 if the simulation configuration is successfully retrieved. 404 if the file specified does not exist in the <SP_HOME>/wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API Response"},{"location":"develop/simulating-Events/#uploading-a-csv-file","text":"To simulate events from a CSV file, the required CSV file needs to exist in the \\<SP_HOME>/wso2/editor/deployment/csv-files directory.","title":"Uploading a CSV file"},{"location":"develop/simulating-Events/#rest-api_4","text":"A POST request should be sent to the following API. POST http://<SP_HOST>:<API_PORT>/simulation/feed","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_4","text":"curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"FeedSimulationTest\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"CSV_SIMULATION\", \"fileName\": \"myEvents.csv\", \"delimiter\": \",\", \"isOrdered\": true, \"indices\": \"0,1\" } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_4","text":"{ \"status\": \"CREATED\", \"message\": \"Successfully uploaded simulation configuration 'FeedSimulationTest'\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_4","text":"200 if the CSV file is successfully uploaded. 409 if a CSV file with the file name specified already exists in the \\<SP_HOME>/wso2/editor/deployment/csv-files directory. 400 if the specified file is not a CSV file or if the specified file path is not valid. 403 if the size of the file specified exceeds the maximum file size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#editing-an-uploaded-csv-file","text":"This section explains how to edit a CSV file that is already uploaded to the <SP_HOME>/wso2/editor/deployment/csv-files directory.","title":"Editing an uploaded CSV file"},{"location":"develop/simulating-Events/#rest-api_5","text":"A PUT request should be sent to the following API. PUT -F 'file=@/{path to csv file}' http://<SP_HOST>:<API_PORT>/simulation/files/{fileName}?fileName={fileName}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_5","text":"curl -X PUT -F 'file=@/home/nadeeka/Desktop/editedMyEvents.csv' http://localhost:9390/simulation/files/myEvents.csv?fileName=myEvents.csv","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_5","text":"{ \"status\": \"OK\", \"message\": \"Successfully updated CSV file 'myEvents.csv' with file ' editedMyEvents.csv'.\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_5","text":"200 if the CSV file is successfully updated. 404 if the specified CSV file does not exist in the <SP_HOME>/deployment/simulator/csvFiles directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#deleting-an-uploaded-csv-file","text":"This section explains how to delete a CSV file that is already uploaded to the <SP_HOME>/wso2/editor/deployment/csv-files directory.","title":"Deleting an uploaded CSV file"},{"location":"develop/simulating-Events/#rest-api_6","text":"A DELETE request should be sent to the following API. DELETE http://<SP_HOST>:<API_PORT>/simulation/files/{fileName}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_6","text":"curl -X DELETE http://localhost:9390/simulation/files/myEvents.csv","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_6","text":"{ \"status\": \"OK\", \"message\": \"Successfully deleted file 'myEvents.csv'\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_6","text":"200 if the CSV file is successfully deleted. 404 if the specified CSV file does not exist in the <SP_HOME>/wso2/editor/deployment/csv-files directory.","title":"REST API response"},{"location":"develop/simulating-Events/#pausing-an-event-simulation","text":"This section explains how to pause an event simulation that has already started.","title":"Pausing an event simulation"},{"location":"develop/simulating-Events/#rest-api_7","text":"A POST request should be sent to the following API. POST http://<SP_HOST>:<API_PORT>/simulation/feed/{simulationName}/?action=pause","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_7","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=pause","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_7","text":"{ \"status\": \"OK\", \"message\": \"Successfully paused event simulation 'simulationPrimitive'.\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_7","text":"200 if the event simulation is successfully paused. 409 if the event simulation is already paused.","title":"REST API response"},{"location":"develop/simulating-Events/#resuming-an-event-simulation","text":"This section explains how to resume an event simulation that has already paused.","title":"Resuming an event simulation"},{"location":"develop/simulating-Events/#rest-api_8","text":"A POST request should be sent to the following API POST http://<SP_HOST>:<API_PORT>/simulation/feed/{ simulationName }/?action=resume","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_8","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=resume","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_8","text":"{ \"status\": \"OK\", \"message\": \"Successfully resumed event simulation 'simulationPrimitive'.\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_8","text":"200 if the event simulation is successfully resumed.","title":"REST API response"},{"location":"develop/simulating-Events/#stopping-an-event-simulation","text":"This section explains how to stop an event simulation.","title":"Stopping an event simulation"},{"location":"develop/simulating-Events/#rest-api_9","text":"A POST request should be sent to the following API POST http://<SP_HOST>:<API_PORT>/simulation/feed/{ simulationName }/?action=stop","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_9","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=stop","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_9","text":"{ \"status\": \"OK\", \"message\": \"Successfully stopped event simulation 'simulationPrimitive'.\" }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_9","text":"200 if the event simulation is successfully stoped.","title":"REST API response"},{"location":"develop/streaming-integrator-studio-overview/","text":"Streaming Integrator Tooling Overview \u00b6 The Streaming Integrator Tooling is a developer tool that is shipped with the Streaming Integrator to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications Source View : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. Design View :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Streaming Integrator Tooling to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Siddhi logic. Starting Streaming Integrator Tooling \u00b6 To start and access the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh Access the Streaming Integrator Tooling via the http://localhost:/editor URL. The Streaming Integrator Tooling opens as shown below. Info The default URL is http://localhost:9390/editor . Welcome Page \u00b6 The Welcome to the Streaming Integrator Tooling Tour Guide is open by default. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Streaming Integrator Tooling on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. Open Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is <SI_Home>/wso2/server/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones that are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources. Menu items \u00b6 This section explains the options that are available in the File , Edit and Run menus. File menu Items \u00b6 The File menu includes the following options. New Click this to open a new untitled Siddhi file. Open File Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is <SI_Home>/wso2/server/deployment . ` . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. Extensions Installation Click this to install extensions, or un-install extensions that have been already installed. Once you click on this menu item, the Extension Details dialog box appears. You can install or un-install the shown extensions by clicking the relevant button, which is shown based on the extension's installation status. For detailed information, see Installing Siddhi Extensions . Save Click this to save an edited or new file to the workspace directory. Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker Tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. + Docker + Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Applications . Close File Click this to close a currently active Siddhi application that is already open in a tab. Close All Files Click this to close all the Siddhi files that are currently open. Delete File Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Streaming Integrator Tooling. The default theme is Twilight . Edit menu Items \u00b6 The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. Find Click this to search for a specific string in the currently active Siddhi application tab. Find and Replace Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. Reformat Code Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . Info This menu option is only visible when you are working in the source view . Auto-Align Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . Info This menu option is only visible when you are working in the design view . Run menu Items \u00b6 The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode. Tools menu items \u00b6 The Tools menu provides access to the following tools that are shipped with the Streaming Integrator Tooling. File Explorer The file explorer. This is also avaible in the Side Panel . Event Simulator Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Streaming Integration Tooling. It is accesible from the Side Panel . Sample Event Generator This opens the Sample Event Generator as follows. Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. On-Demand Query This opens the On-Demand Query dialog box. Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Storage Integration - Performing CRUD operations via REST API . Tour Guide This opens a dialog box named Welcome to the Streaming Integrator Tooling Tour Guide which guides you to understand Streaming Integrator Tooling. When you start the Streaming Integrator Tooling and access it, this dialog box is open by default. Deploy menu items \u00b6 The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more Streaming Integrator servers. For more information, see Deploying Siddhi Applications . Export menu items \u00b6 The Export menu has the following options that allow you to export Siddhi applications in a format that can be deployed in a containerized environment. For Docker This opens the Export Siddhi Apps for Docker image wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi applications as a Docker Image . For Kubernetes This opens the Export Siddhi Apps For Kubernetes CRD wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi Applications for Kubernetes . Side Panel \u00b6 File Explorer This provides a view of all the files saved as shown in the example above. Event Simulator \u00b6 Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . Output Console \u00b6 This provides feedback on various user activities carried out on the Streaming Integrator. Operator Finder \u00b6 Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application . Toolbar \u00b6 Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Streaming Integrator Tooling Overview"},{"location":"develop/streaming-integrator-studio-overview/#streaming-integrator-tooling-overview","text":"The Streaming Integrator Tooling is a developer tool that is shipped with the Streaming Integrator to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications Source View : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. Design View :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Streaming Integrator Tooling to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Siddhi logic.","title":"Streaming Integrator Tooling Overview"},{"location":"develop/streaming-integrator-studio-overview/#starting-streaming-integrator-tooling","text":"To start and access the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by issuing one of the following commands from the <SI_HOME>/bin directory. For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh Access the Streaming Integrator Tooling via the http://localhost:/editor URL. The Streaming Integrator Tooling opens as shown below. Info The default URL is http://localhost:9390/editor .","title":"Starting Streaming Integrator Tooling"},{"location":"develop/streaming-integrator-studio-overview/#welcome-page","text":"The Welcome to the Streaming Integrator Tooling Tour Guide is open by default. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Streaming Integrator Tooling on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. Open Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is <SI_Home>/wso2/server/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones that are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources.","title":"Welcome Page"},{"location":"develop/streaming-integrator-studio-overview/#menu-items","text":"This section explains the options that are available in the File , Edit and Run menus.","title":"Menu items"},{"location":"develop/streaming-integrator-studio-overview/#file-menu-items","text":"The File menu includes the following options. New Click this to open a new untitled Siddhi file. Open File Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is <SI_Home>/wso2/server/deployment . ` . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. Extensions Installation Click this to install extensions, or un-install extensions that have been already installed. Once you click on this menu item, the Extension Details dialog box appears. You can install or un-install the shown extensions by clicking the relevant button, which is shown based on the extension's installation status. For detailed information, see Installing Siddhi Extensions . Save Click this to save an edited or new file to the workspace directory. Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker Tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. + Docker + Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Applications . Close File Click this to close a currently active Siddhi application that is already open in a tab. Close All Files Click this to close all the Siddhi files that are currently open. Delete File Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Streaming Integrator Tooling. The default theme is Twilight .","title":"File menu Items"},{"location":"develop/streaming-integrator-studio-overview/#edit-menu-items","text":"The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. Find Click this to search for a specific string in the currently active Siddhi application tab. Find and Replace Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. Reformat Code Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . Info This menu option is only visible when you are working in the source view . Auto-Align Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . Info This menu option is only visible when you are working in the design view .","title":"Edit menu Items"},{"location":"develop/streaming-integrator-studio-overview/#run-menu-items","text":"The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode.","title":"Run menu Items"},{"location":"develop/streaming-integrator-studio-overview/#tools-menu-items","text":"The Tools menu provides access to the following tools that are shipped with the Streaming Integrator Tooling. File Explorer The file explorer. This is also avaible in the Side Panel . Event Simulator Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Streaming Integration Tooling. It is accesible from the Side Panel . Sample Event Generator This opens the Sample Event Generator as follows. Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. On-Demand Query This opens the On-Demand Query dialog box. Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Storage Integration - Performing CRUD operations via REST API . Tour Guide This opens a dialog box named Welcome to the Streaming Integrator Tooling Tour Guide which guides you to understand Streaming Integrator Tooling. When you start the Streaming Integrator Tooling and access it, this dialog box is open by default.","title":"Tools menu items"},{"location":"develop/streaming-integrator-studio-overview/#deploy-menu-items","text":"The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more Streaming Integrator servers. For more information, see Deploying Siddhi Applications .","title":"Deploy menu items"},{"location":"develop/streaming-integrator-studio-overview/#export-menu-items","text":"The Export menu has the following options that allow you to export Siddhi applications in a format that can be deployed in a containerized environment. For Docker This opens the Export Siddhi Apps for Docker image wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi applications as a Docker Image . For Kubernetes This opens the Export Siddhi Apps For Kubernetes CRD wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi Applications for Kubernetes .","title":"Export menu items"},{"location":"develop/streaming-integrator-studio-overview/#side-panel","text":"File Explorer This provides a view of all the files saved as shown in the example above.","title":"Side Panel"},{"location":"develop/streaming-integrator-studio-overview/#event-simulator","text":"Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events .","title":"Event Simulator"},{"location":"develop/streaming-integrator-studio-overview/#output-console","text":"This provides feedback on various user activities carried out on the Streaming Integrator.","title":"Output Console"},{"location":"develop/streaming-integrator-studio-overview/#operator-finder","text":"Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application .","title":"Operator Finder"},{"location":"develop/streaming-integrator-studio-overview/#toolbar","text":"Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Toolbar"},{"location":"develop/testing-a-Siddhi-Application/","text":"Testing Siddhi Applications \u00b6 The Streaming Integrator allows the following tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run in an actual production environment. Validate Siddhi applications that are written in the Streaming Integrator Studio. Run Siddhi applications that were written in the Streaming Integrator Studio in either Run or Debug mode. Simulate events to test the Siddhi applications and analyze events that are received and sent. This allows you to analyze the status of each query within a Siddhi application at different execution points. Validating a Siddhi application \u00b6 To validate a Siddhi application, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . In this example, let's use an existing sample as an example. Click on the ReceiveAndCount sample to open it. Sample opens in a new tab. This sample does not have errors, and therefore, no errors are displayed in the editor. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() as shown below. @info(name='query1') from SweetProductionStream select countNew() as totalCount insert into TotalCountStream; ` Now, the editor indicates that there is a syntax error. If you move the cursor over the error icon, it indicates that countNew is an invalid function name as shown below. Running or debugging a Siddhi application \u00b6 You can run or debug a Siddhi application to verify whether the logic you have written is correct. To start a Siddhi application in the run/debug mode, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . For this example, click the existing sample ReceiveAndCount . It opens in a new untitled tab. Save the Siddhi file so that you can run it in the Run or Debug mode. To save it, click File => Save . Once the file is saved, you can see the Run and Debug menu options enabled as shown below. To start the application in Run mode, click Run => Run . This logs the following output in the console. Start the application in the Debug mode, click Run => Debug . As a result, the following mesage is logged in the console. You can also note that another console tab is opened with debug options. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() , and save. Then click Run => Run . As a result, the following output is logged in the console. Simulating events \u00b6 This section demonstrates how to test a Siddhi application via event simulation. Event simulation involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for testing Siddhi applications in order to evaluate whether they function as expected Events can be simulated in the following methods: Simulating a single event Simulating multiple events via CSV files Simulating multiple events via databases Generating random events Tip Before you simulate events for a Siddhi application, you need to run or debug it. Therefore, before you try this section, see Running or debugging a Siddhi application . Simulating a single event \u00b6 This section demonstrates how to simulate a single event to be processed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate a single event, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor to open the Single Simulation panel. It opens the left panel for event simulation as follows. Enter Information in the Single Simulation panel as described below. In the Siddhi App Name field, select a currently deployed Siddhi application. In the Stream Name field, select the event stream for which you want to simulate events. The list displays all the event streams defined in the selected Siddhi application. If you want to simulate the event for a specific time different to the current time, enter a valid timestamp in the Timestamp field. To select a timestamp, click the time and calendar icon next to the Timestamp field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . Enter values for the attributes of the selected stream. Click Send to start sending the event. The simulated event is logged similar to the sample log given below. Simulating multiple events via CSV files \u00b6 This section explains how to generate multiple events via CSV files to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate multiple events from a CSV file, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. In the event simulation left panel that opens, click on the Feed Simulation tab. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. Click Advanced Configurations if you want to enter detailed specifications to filter events from the CSV file. Then enter information as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select CSV File . Click Add Simulation Source to open the following section. In the Siddhi App Name field, select the required Siddhi application. Then more fields as shown below. Enter details as follows: In the Stream Name field, select the stream for which you want to simulate events. All the streams defined in the Siddhi App you selected are available in the list. In the CSV File field, select an available CSV file. If no CSV files are currently uploaded, select Upload File from the list. This opens the Upload File dialog box. Click Choose File and browse for the CSV file you want to upload. Then click Upload . In the Delimiter field, enter the character you want to use in order to separate the attribute values in each row of the CSV file. If you want to enter more detailed specificiations, click Advanced Configuration . Then enter details as follows. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want to increase the value of the timestamp for each new event, select the Increment event time by(ms) option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. If you want the events to arrive in order based on the timestamp, select Yes under the Timestamp Interval option. Click Save to save the information relating to the CSV file. The name os the CSV file appears in the Feed Simulation tab in the left panel. To simulate a CSV file that is uploaded and visible in the Feed Simulation tab in the left panel, click on the arrow to its right. The simulated events are logged n the output console. Simulating multiple events via databases \u00b6 This section explains how to generate multiple events via databases to be analyzed via the Streaming Integrator. Tip Before simulating events via databases: - A Siddhi application must be created. - The database from which you want to simulate events must be already configured for the Streaming Integrator. To simulate multiple events from a database, follow the procedure below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to simulate events at time intervals of a specific length, enter that length in milliseconds in the Time Interval(ms) field. If you want to enter more advanced conditions to simulate the events, click Advanced Configurations . As a result, the following section is displayed. {width=\"442\" height=\"191\"} Then enter details as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select Database . To connect to a new database, click Add Simulation Source to open the following section. Enter information as follows: Field Description Siddhi App Name Select the Siddhi Application in which the event stream for which you want to simulate events is defined. Stream Name Select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. Data Source The JDBC URL to be used to access the required database. Driver Class The driver class name of the selected database. Username The username that must be used to access the database. Password The password that must be used to access the database. Once you have entered the above information, click Connect to Database . If the datasource is correctly configured, the following is displayed to indicate that Streaming Integrator can successfully connect to the database. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want the vents in the CSV file to be sorted based on the timestamp, select the Yes option under CSV File is Ordered by Timestamp . To increase the timestamp of the published events, select the Timestamp Interval option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. Click Save . This adds the fed simulation you created as an active simulation in the Feed Simulation tab of the left panel as shown below. Click on the play button of this simulation to open the Run or Debug dialog box. If you want to run the Siddhi application you previously selected and simulate events for it, select Run . If you want to simulate events in the Debug mode, select Debug . Once you have selected the required mode, click Start Simulation . A message appears to inform you that the feed simulation started successfully. Similarly, when the simulation is completed, a message appears to inform you that the event simulation has finished. Generating random events \u00b6 This section explains how to generate random data to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate random events, follow the steps given below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. In the Simulation Source field, select Random . If the random simulation source from which you want to simulate events does not already exist in the Feed Simulation pane, click Add New to open the following section. Enter information relating to the random source as follows: In the Siddhi App Name field, s elect the name of the Siddhi App with the event stream for which the events are simulated. In the Stream Name field, select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. In the Timestamp Interval field, enter the number of milliseconds by which you want to increase the timestamp of each event. To enter values for the stream attributes, follow the instructions below. To enter a custom value for a stream attribute, select Custom data based from the list. When you select this value, data field in which the required value can be entered appears as shown in the example below. {width=\"235\" height=\"88\"} To enter a primitive based value, select Primitive based from the list. The information to be entered varies depending on the data type of the attribute. The following table explains the information you need to enter when you select Primitive based for each data type. Data Type Values to enter STRING Specify a length in the Length field that appears. This results in a value of the specified length being auto-generated. FLOAT or DOUBLE The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. Precision : The precise value. The number of decimals included in the auto-generated values are the same as that of the value specified here. INT or LONG The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. BOOL No further information is required because true and false values are randomly generated. To randomly assign values based on a pre-defined set of meaningful values, select Property based from the list. When you select this value, a field in which the set of available values are listed appears as shown in the example below. To assign a regex value, select Regex based from the list. Click Save to save the simulation information. The saved random simulation appears in the Feed tab of the left panel. To simulate events, click the arrow to the right os the saved simulation (shown in the example below). The simulated events are logged in the CLI as shown in the extract below.","title":"Testing Siddhi Applications"},{"location":"develop/testing-a-Siddhi-Application/#testing-siddhi-applications","text":"The Streaming Integrator allows the following tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run in an actual production environment. Validate Siddhi applications that are written in the Streaming Integrator Studio. Run Siddhi applications that were written in the Streaming Integrator Studio in either Run or Debug mode. Simulate events to test the Siddhi applications and analyze events that are received and sent. This allows you to analyze the status of each query within a Siddhi application at different execution points.","title":"Testing Siddhi Applications"},{"location":"develop/testing-a-Siddhi-Application/#validating-a-siddhi-application","text":"To validate a Siddhi application, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . In this example, let's use an existing sample as an example. Click on the ReceiveAndCount sample to open it. Sample opens in a new tab. This sample does not have errors, and therefore, no errors are displayed in the editor. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() as shown below. @info(name='query1') from SweetProductionStream select countNew() as totalCount insert into TotalCountStream; ` Now, the editor indicates that there is a syntax error. If you move the cursor over the error icon, it indicates that countNew is an invalid function name as shown below.","title":"Validating a Siddhi application"},{"location":"develop/testing-a-Siddhi-Application/#running-or-debugging-a-siddhi-application","text":"You can run or debug a Siddhi application to verify whether the logic you have written is correct. To start a Siddhi application in the run/debug mode, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . For this example, click the existing sample ReceiveAndCount . It opens in a new untitled tab. Save the Siddhi file so that you can run it in the Run or Debug mode. To save it, click File => Save . Once the file is saved, you can see the Run and Debug menu options enabled as shown below. To start the application in Run mode, click Run => Run . This logs the following output in the console. Start the application in the Debug mode, click Run => Debug . As a result, the following mesage is logged in the console. You can also note that another console tab is opened with debug options. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() , and save. Then click Run => Run . As a result, the following output is logged in the console.","title":"Running or debugging a Siddhi application"},{"location":"develop/testing-a-Siddhi-Application/#simulating-events","text":"This section demonstrates how to test a Siddhi application via event simulation. Event simulation involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for testing Siddhi applications in order to evaluate whether they function as expected Events can be simulated in the following methods: Simulating a single event Simulating multiple events via CSV files Simulating multiple events via databases Generating random events Tip Before you simulate events for a Siddhi application, you need to run or debug it. Therefore, before you try this section, see Running or debugging a Siddhi application .","title":"Simulating events"},{"location":"develop/testing-a-Siddhi-Application/#simulating-a-single-event","text":"This section demonstrates how to simulate a single event to be processed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate a single event, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor to open the Single Simulation panel. It opens the left panel for event simulation as follows. Enter Information in the Single Simulation panel as described below. In the Siddhi App Name field, select a currently deployed Siddhi application. In the Stream Name field, select the event stream for which you want to simulate events. The list displays all the event streams defined in the selected Siddhi application. If you want to simulate the event for a specific time different to the current time, enter a valid timestamp in the Timestamp field. To select a timestamp, click the time and calendar icon next to the Timestamp field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . Enter values for the attributes of the selected stream. Click Send to start sending the event. The simulated event is logged similar to the sample log given below.","title":"Simulating a single event"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-csv-files","text":"This section explains how to generate multiple events via CSV files to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate multiple events from a CSV file, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. In the event simulation left panel that opens, click on the Feed Simulation tab. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. Click Advanced Configurations if you want to enter detailed specifications to filter events from the CSV file. Then enter information as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select CSV File . Click Add Simulation Source to open the following section. In the Siddhi App Name field, select the required Siddhi application. Then more fields as shown below. Enter details as follows: In the Stream Name field, select the stream for which you want to simulate events. All the streams defined in the Siddhi App you selected are available in the list. In the CSV File field, select an available CSV file. If no CSV files are currently uploaded, select Upload File from the list. This opens the Upload File dialog box. Click Choose File and browse for the CSV file you want to upload. Then click Upload . In the Delimiter field, enter the character you want to use in order to separate the attribute values in each row of the CSV file. If you want to enter more detailed specificiations, click Advanced Configuration . Then enter details as follows. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want to increase the value of the timestamp for each new event, select the Increment event time by(ms) option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. If you want the events to arrive in order based on the timestamp, select Yes under the Timestamp Interval option. Click Save to save the information relating to the CSV file. The name os the CSV file appears in the Feed Simulation tab in the left panel. To simulate a CSV file that is uploaded and visible in the Feed Simulation tab in the left panel, click on the arrow to its right. The simulated events are logged n the output console.","title":"Simulating multiple events via CSV files"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-databases","text":"This section explains how to generate multiple events via databases to be analyzed via the Streaming Integrator. Tip Before simulating events via databases: - A Siddhi application must be created. - The database from which you want to simulate events must be already configured for the Streaming Integrator. To simulate multiple events from a database, follow the procedure below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to simulate events at time intervals of a specific length, enter that length in milliseconds in the Time Interval(ms) field. If you want to enter more advanced conditions to simulate the events, click Advanced Configurations . As a result, the following section is displayed. {width=\"442\" height=\"191\"} Then enter details as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select Database . To connect to a new database, click Add Simulation Source to open the following section. Enter information as follows: Field Description Siddhi App Name Select the Siddhi Application in which the event stream for which you want to simulate events is defined. Stream Name Select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. Data Source The JDBC URL to be used to access the required database. Driver Class The driver class name of the selected database. Username The username that must be used to access the database. Password The password that must be used to access the database. Once you have entered the above information, click Connect to Database . If the datasource is correctly configured, the following is displayed to indicate that Streaming Integrator can successfully connect to the database. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want the vents in the CSV file to be sorted based on the timestamp, select the Yes option under CSV File is Ordered by Timestamp . To increase the timestamp of the published events, select the Timestamp Interval option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. Click Save . This adds the fed simulation you created as an active simulation in the Feed Simulation tab of the left panel as shown below. Click on the play button of this simulation to open the Run or Debug dialog box. If you want to run the Siddhi application you previously selected and simulate events for it, select Run . If you want to simulate events in the Debug mode, select Debug . Once you have selected the required mode, click Start Simulation . A message appears to inform you that the feed simulation started successfully. Similarly, when the simulation is completed, a message appears to inform you that the event simulation has finished.","title":"Simulating multiple events via databases"},{"location":"develop/testing-a-Siddhi-Application/#generating-random-events","text":"This section explains how to generate random data to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate random events, follow the steps given below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. In the Simulation Source field, select Random . If the random simulation source from which you want to simulate events does not already exist in the Feed Simulation pane, click Add New to open the following section. Enter information relating to the random source as follows: In the Siddhi App Name field, s elect the name of the Siddhi App with the event stream for which the events are simulated. In the Stream Name field, select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. In the Timestamp Interval field, enter the number of milliseconds by which you want to increase the timestamp of each event. To enter values for the stream attributes, follow the instructions below. To enter a custom value for a stream attribute, select Custom data based from the list. When you select this value, data field in which the required value can be entered appears as shown in the example below. {width=\"235\" height=\"88\"} To enter a primitive based value, select Primitive based from the list. The information to be entered varies depending on the data type of the attribute. The following table explains the information you need to enter when you select Primitive based for each data type. Data Type Values to enter STRING Specify a length in the Length field that appears. This results in a value of the specified length being auto-generated. FLOAT or DOUBLE The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. Precision : The precise value. The number of decimals included in the auto-generated values are the same as that of the value specified here. INT or LONG The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. BOOL No further information is required because true and false values are randomly generated. To randomly assign values based on a pre-defined set of meaningful values, select Property based from the list. When you select this value, a field in which the set of available values are listed appears as shown in the example below. To assign a regex value, select Regex based from the list. Click Save to save the simulation information. The saved random simulation appears in the Feed tab of the left panel. To simulate events, click the arrow to the right os the saved simulation (shown in the example below). The simulated events are logged in the CLI as shown in the extract below.","title":"Generating random events"},{"location":"develop/working-with-the-Design-View/","text":"Working with the Design View \u00b6 This section provides an overview of the design view of the Streaming Integrator Tooling. Accesing the Design View \u00b6 To open the design view of the Streaming Integrator Tooling: Start the Streaming Integrator Tooling and log in with your credentials. For detailed instructions, see Streaming Integrator Tooling Overview - Starting Streaming Integrator Tooling . Click New and open a new Siddhi file, or click Open and open an existing Siddhi file. Click Design View to open the Design View. The design view opens as shown in the example below. It consists of a grid to which you can drag and drop the Siddhi components represented by icons displayed in the left panel to design a Siddhi application. Adding Siddhi components \u00b6 To add a Siddhi component to the Siddhi application that you are creating/editing in the design view, click on the relevant icon in the left pane, and then drag and drop it to the grid as demonstrated in the example below. Once you add a Siddhi component, you can configure it as required. To configure a Siddhi component, click the settings icon on the component. This opens a form with parameters related to the relevant component. The following is the complete list of Siddhi components that you can add to the grid of the design view when you create a Siddhi application. Stream \u00b6 Icon Description A stream represents a logical series of events ordered in time. For a detailed description of streams, see Siddhi Query Guide - Stream . Form To configure the stream, click the settings icon on the stream component you added to the grid. Then enter values as follows: Stream Name : A unique name for the stream. This should be specified in title caps, and without spaces (e.g., ProductionDataStream ). Attributes : Attributes of streams are specified as name and type pairs in the Attributes table. Example The details entered in the above form creates a stream configuration as follows: define stream SweetProductionStream (amount double , name string); Source Sources Projection queries Filter queries Window queries Join queries Target Sinks Projection queries Filter queries Window queries Join queries Source \u00b6 Icon Description A source receives events in the specified transport and in the specified format. For more information, see Siddhi Query Guide - Source . Form To configure the source, click the settings icon on the source component you added to the grid. This opens a form where you can enter the following information: To access the form in which you can configure a source, you must first connect the source as the source (input) object to a stream component. Source Type : This specifies the transport type via which the events are received. The value should be entered in lower case (e.g., t cp ). The other parameters displayed for the source depends on the source type selected. Map Type : This specifies the format in which you want to receive the events (e.g., xml ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a source configuration as follows: @source (type = 'tcp', @map (type = 'json', @attributes (name = \"$.sweet\" , amount = \"$.batch.count\" ))) Source No connection can start from another Siddhi component and link to a source because a source is the point from which events selected into the event flow of the Siddhi application start. Target Streams Sink \u00b6 Icon Description A sink publishes events via the specified transport and in the specified format. For more information, see Siddhi Query Guide - Sink . Form To configure the sink, click the settings icon on the sink component you added to the grid. !!! info To access the form in which you can configure a sink, you must first connect the sink as the target object to a stream component. Sink Type : This specifies the transport via which the sink publishes processed events. The value should be entered in lower case (e.g., log ). Map Type : This specifies the format in which you want to publish the events (e.g., passThrough ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a sink configuration as follows: @sink(type = 'log', prefix = \"Sweet Totals:\") Source Streams Target N/A A sink cannot be followed by another Siddhi component because it represents the last stage of the event flow where the results of the processing carried out by the Siddhi application are communicated via the required interface. Table \u00b6 Icon Description A table is a stored version of an stream or a table of events. For more information, see Siddhi Query Guide - Table . Form To configure the table, click the settings icon on the table component you added to the grid. Name : This field specifies unique name for the table. This should be specified in title caps, and without spaces (e.g., ProductionDataTable ). Attributes : Attributes of tables are specified as name and type pairs in the Attributes table. To add a new attribute, click +Attribute . Store Type : This specifies the specific database type in which you want to stopre data or whether the data is to be stored in-memory. Once the store type is selected, select an option to indicate whether the datastore needs to be defined inline, whether you want to use a datasource defined in the <SP_HOME>/conf/worker/deployment.yaml file, or connected to a JNDI resource. For more information, see Defining Tables for Physical Stores . The other parameters configured under Store Type depend on the store type you select. Annotations : This section allows you to specify the table attributes you want to use as the primary key and indexes via the @primarykey and @index annotations. For more information, see Defining Data Tables . If you want to add any other custom annotations to your table definition, click +Annotation to define them. Example The details entered in the above form creates a table definition as follows: @store(type = 'rdbms', datasource = \"SweetProductionDB\") define table ShipmentDetails (name string, supplier string, amount double ); Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries Window \u00b6 Icon Description This icon represents a window definition that can be shared across multiple queries. For more information, see Siddhi Query Guide - (Defined) Window . Form To configure the window, click the settings icon on the window component you added to the grid, and update the following information. Name : This field specifies a unique name for the window. PascalCase is used for window names as a convention. Attributes : Attributes of windows are specified as name and type pairs in the Attributes table. Window Type : This specifies the function of the window (i.e., the window type such as time , length , frequent etc.). The window types supported include time , timeBatch , timeLength , length , lengthBatch , sort , frequent , lossyFrequent , cron , externalTime , externalTimeBatch . Parameters : This section allows you to define one or more parameters for the window definition based on the window type you entered in the Window Type field. Annotations : If you want to add any other custom annotations to your window definition, click +Annotation to define them. Example The details entered in the above form creates a window definition as follows: define window FiveMinTempWindow (roomNo int , temp double ) time ( 5 min ) output all events ; Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries Trigger \u00b6 Icon Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger . Form To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information. Name : A unique name for the trigger. Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows: start : Select this to trigger events when the Streaming Integrator server has started. every : Select this to specify a time interval at which events should be triggered. cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the Example The details entered in the above orm creates a trigger definition as follows: define trigger FiveMinTriggerStream at every 5 min ; Source N/A Target Projection queries Window queries Filter queries Join queries Aggregation \u00b6 Icon Description Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. For more information, see Siddhi Query Guide - Incremental Aggregation . !!! tip Before you add an aggregation, make sure that you have already added the stream with the events to which the aggregation is applied is already defined. Form To configure the aggregation, click the settings icon on the aggregation component you added to the grid, and update the following information. Aggregation Meta Information : In this section, define a unique name for the aggregation in the Name field, and specify the stream from which the input information is taken to perform the aggregations. You can also select the optional annotations you want to use in the aggregation definition by selecting the relevant check boxes. For more information about configuring the annotations once you select them, see Incremental Analysis . Projection : This section specifies the attributes to be included in the aggregation query. In the Select field, you can select All attributes to perform the aggregation for all the attributes of the stream specified under Input , or select User Defined Attributes to select specific attributes. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Aggregation Criteria : Here, you can specify the time values based on which the aggregates are calculated. Example The details entered in the above form creates an aggregation definition as follows: define aggregation TradeAggregation from TradeStream select symbol, avg (price) as avgPrice, sum (price) as total group by symbol aggregate by timestamp every seconds .. .years; Source N/A Target Join queries Function \u00b6 Icon Description The function icon represents Script in Siddhi Query Language . It allows you to write functions in other programming languages and execute them within Siddhi queries. A function component in a Siddhi application is not connected to ther Siddhi components in the design UI. However, the configuration of one or more Query components can include a reference to it. Form To configure the function, click the settings icon on the function component you added to the grid, and update the following information. Name : A unique name for the function. Script Type : The language in which the function is written. Return Value : The data format of the value that is generated as the output via the function. Script Body : This is a free text field to write the function in the specified script type. Example The details entered in the above form creates a function definition as follows: define function concatFN[JAVASCRIPT] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3; return responce; }; Projection Query \u00b6 Icon Description !!! tip Before you add a projection query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. This icon represents a query to project the events in an input stream to an output stream. This invoves selectng the attributes to be included in the output, renaming attributes, introducing constant values, and using mathematical and/or logical expressions. For more information, see Siddhi Query Guide - Query Projection . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the projection query, click the settings icon on the projection query component you added to the grid, and update the following information. Query Meta Information : This section specifies the stream to be considered as the input stream with the events to which the query needs to be applied. The input stream connected to the query as the source is automatically displayed. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.| Example The details entered in the above form creates a query as follows: from TradeStream select symbol, avg (price) as averagePrice, sum (volume) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Filter Query \u00b6 Icon Description !!! tip Before you add a filter query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A filter query filters information in an input stream based on a given condition. For more information, see Siddhi Query Guide - Filters . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the filter query, click the settings icon on the filter query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the filter type is available under it to indicate that the query is a filter. Expand this stream handler, and enter the condition based on which the information needs to be filtered. !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above form creates a query with a filter as follows: from TradeStream[ sum (amount) > 10000 ] select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Window Query \u00b6 Icon Description !!! tip Before you add a window query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. Window queries include a window to select a subset of events to be processed based on a specific criterion. For more information, see Siddhi Query Guide - (Defined) Window . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the window query, click the settings icon on the window query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the window type is available under it to indicate that the query is a filter. Expand this stream handler, and enter details to determine the window including the window type and the basis on which the subset of events considered by the window is determined (i.e., based on the window type selected). !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above Query Configuration form creates a query with a window as follows: from TradeStream#window. time ( 1 month ) select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source !!! info A window query can have only one source at a given time. Streams Tables Triggers Windows Target Streams Tables Windows Join Query \u00b6 Icon Description A join query derives a combined result from two streams in real-time based on a specified condition. For more information, see Siddhi Query Guide - Join . Form Once you connect two Siddhi components to the join query as sources and another Siddhi component as the target, you can configure the join query. To configure the join query, click the settings icon on the join query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : Here, you can specify the input sources, the references, the join type, join condition, and stream handlers for the left source and right source of the join. For a detailed explanation of the join concept, see Siddhi Query Guide - Joins . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example A join query is configured as follows: The above configurations result in creating the following join query. from TempStream[temp > 30.0 ]#window. time ( 1 min ) as T join RegulatorStream[isOn == false ]#window. length ( 1 ) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Source !!! info A join query must always be connected to two sources, and at least one of them must be a defined stream/trigger/window. Streams Tables Aggregations Windows Target !!! info A join query must always be connected to a single target. Streams Tables Windows Pattern Query \u00b6 Icon Description !!! tip Before you add a pattern query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A pattern query detects patterns in events that arrive overtime. For more information, see Siddhi Query Guide - Patterns . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the pattern query, click the settings icon on the pattern query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which patterns are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Pattern concept, see Siddhi Query Guide - Patterns . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every (e1 = MaterialSupplyStream) -> not MaterialConsumptionStream[name == e1.name and amount == e1.amount] for 15 sec select e1.name, e1.amount insert into ProductionDelayAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Sequence Query \u00b6 Icon Description !!! tip Before you add a sequence query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A sequence query detects sequences in event occurrences over time. For more information, see Siddhi Query Guide - Sequence . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the sequence query, click the settings icon on the sequence query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which sequences are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Sequence concept, see Siddhi Query Guide - Sequences . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every e1 = SweetProductionStream, e2 = SweetProductionStream[e1.amount > amount and ( timestamp - e1. timestamp ) < 10 * 60000 ] * , e3 = SweetProductionStream[ timestamp - e1. timestamp > 10 * 60000 and e1.amount > amount] select e1.name, e1.amount as initialAmount, e2.amount as finalAmount, e2. timestamp insert into DecreasingTrendAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Partitions \u00b6 Icon Description !!! tip Before you add a partition: You need to add the stream to be partitioned. Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. For more information, see Siddhi Query Guide - Partition . Form Once the stream to be partitioned is connected as a source to the partition, you can configure the partition. In order to do so, move the cursor over the partition and click the settings icon on the partition component. This opens the Partition Configuration form. In this form, you can enter expressions to convert the attributes of the stream that is selected to be partitioned. Example The above configuration creates the following partition query. partition with ( roomNo >= 1030 as 'serverRoom' or roomNo < 1030 and roomNo >= 330 as 'officeRoom' or roomNo < 330 as 'lobby' of TempStream) begin from TempStream#window. time ( 10 min ) select roomNo, deviceID, avg (temp) as avgTemp insert into AreaTempStream end ; Source Streams Target N/A Connecting Siddhi components \u00b6 In order to define how the Siddhi components in a Siddhi application interact with each other to process events, you need to define connections between Siddhi components. A connection is defined by drawing an arrow from one component to another by dragging the cursor as demonstrated below. Saving, running and debugging Siddhi applications \u00b6 To save a Siddhi application that you created in the design view, you need to switch to the source view. You also need to switch to the source view to run or debug a Siddhi application. For more information, see the following sections: Streaming Integrator Tiooling Overview Debugging a Siddhi Application","title":"Working with the Design View"},{"location":"develop/working-with-the-Design-View/#working-with-the-design-view","text":"This section provides an overview of the design view of the Streaming Integrator Tooling.","title":"Working with the Design View"},{"location":"develop/working-with-the-Design-View/#accesing-the-design-view","text":"To open the design view of the Streaming Integrator Tooling: Start the Streaming Integrator Tooling and log in with your credentials. For detailed instructions, see Streaming Integrator Tooling Overview - Starting Streaming Integrator Tooling . Click New and open a new Siddhi file, or click Open and open an existing Siddhi file. Click Design View to open the Design View. The design view opens as shown in the example below. It consists of a grid to which you can drag and drop the Siddhi components represented by icons displayed in the left panel to design a Siddhi application.","title":"Accesing the Design View"},{"location":"develop/working-with-the-Design-View/#adding-siddhi-components","text":"To add a Siddhi component to the Siddhi application that you are creating/editing in the design view, click on the relevant icon in the left pane, and then drag and drop it to the grid as demonstrated in the example below. Once you add a Siddhi component, you can configure it as required. To configure a Siddhi component, click the settings icon on the component. This opens a form with parameters related to the relevant component. The following is the complete list of Siddhi components that you can add to the grid of the design view when you create a Siddhi application.","title":"Adding Siddhi components"},{"location":"develop/working-with-the-Design-View/#stream","text":"Icon Description A stream represents a logical series of events ordered in time. For a detailed description of streams, see Siddhi Query Guide - Stream . Form To configure the stream, click the settings icon on the stream component you added to the grid. Then enter values as follows: Stream Name : A unique name for the stream. This should be specified in title caps, and without spaces (e.g., ProductionDataStream ). Attributes : Attributes of streams are specified as name and type pairs in the Attributes table. Example The details entered in the above form creates a stream configuration as follows: define stream SweetProductionStream (amount double , name string); Source Sources Projection queries Filter queries Window queries Join queries Target Sinks Projection queries Filter queries Window queries Join queries","title":"Stream"},{"location":"develop/working-with-the-Design-View/#source","text":"Icon Description A source receives events in the specified transport and in the specified format. For more information, see Siddhi Query Guide - Source . Form To configure the source, click the settings icon on the source component you added to the grid. This opens a form where you can enter the following information: To access the form in which you can configure a source, you must first connect the source as the source (input) object to a stream component. Source Type : This specifies the transport type via which the events are received. The value should be entered in lower case (e.g., t cp ). The other parameters displayed for the source depends on the source type selected. Map Type : This specifies the format in which you want to receive the events (e.g., xml ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a source configuration as follows: @source (type = 'tcp', @map (type = 'json', @attributes (name = \"$.sweet\" , amount = \"$.batch.count\" ))) Source No connection can start from another Siddhi component and link to a source because a source is the point from which events selected into the event flow of the Siddhi application start. Target Streams","title":"Source"},{"location":"develop/working-with-the-Design-View/#sink","text":"Icon Description A sink publishes events via the specified transport and in the specified format. For more information, see Siddhi Query Guide - Sink . Form To configure the sink, click the settings icon on the sink component you added to the grid. !!! info To access the form in which you can configure a sink, you must first connect the sink as the target object to a stream component. Sink Type : This specifies the transport via which the sink publishes processed events. The value should be entered in lower case (e.g., log ). Map Type : This specifies the format in which you want to publish the events (e.g., passThrough ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a sink configuration as follows: @sink(type = 'log', prefix = \"Sweet Totals:\") Source Streams Target N/A A sink cannot be followed by another Siddhi component because it represents the last stage of the event flow where the results of the processing carried out by the Siddhi application are communicated via the required interface.","title":"Sink"},{"location":"develop/working-with-the-Design-View/#table","text":"Icon Description A table is a stored version of an stream or a table of events. For more information, see Siddhi Query Guide - Table . Form To configure the table, click the settings icon on the table component you added to the grid. Name : This field specifies unique name for the table. This should be specified in title caps, and without spaces (e.g., ProductionDataTable ). Attributes : Attributes of tables are specified as name and type pairs in the Attributes table. To add a new attribute, click +Attribute . Store Type : This specifies the specific database type in which you want to stopre data or whether the data is to be stored in-memory. Once the store type is selected, select an option to indicate whether the datastore needs to be defined inline, whether you want to use a datasource defined in the <SP_HOME>/conf/worker/deployment.yaml file, or connected to a JNDI resource. For more information, see Defining Tables for Physical Stores . The other parameters configured under Store Type depend on the store type you select. Annotations : This section allows you to specify the table attributes you want to use as the primary key and indexes via the @primarykey and @index annotations. For more information, see Defining Data Tables . If you want to add any other custom annotations to your table definition, click +Annotation to define them. Example The details entered in the above form creates a table definition as follows: @store(type = 'rdbms', datasource = \"SweetProductionDB\") define table ShipmentDetails (name string, supplier string, amount double ); Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries","title":"Table"},{"location":"develop/working-with-the-Design-View/#window","text":"Icon Description This icon represents a window definition that can be shared across multiple queries. For more information, see Siddhi Query Guide - (Defined) Window . Form To configure the window, click the settings icon on the window component you added to the grid, and update the following information. Name : This field specifies a unique name for the window. PascalCase is used for window names as a convention. Attributes : Attributes of windows are specified as name and type pairs in the Attributes table. Window Type : This specifies the function of the window (i.e., the window type such as time , length , frequent etc.). The window types supported include time , timeBatch , timeLength , length , lengthBatch , sort , frequent , lossyFrequent , cron , externalTime , externalTimeBatch . Parameters : This section allows you to define one or more parameters for the window definition based on the window type you entered in the Window Type field. Annotations : If you want to add any other custom annotations to your window definition, click +Annotation to define them. Example The details entered in the above form creates a window definition as follows: define window FiveMinTempWindow (roomNo int , temp double ) time ( 5 min ) output all events ; Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries","title":"Window"},{"location":"develop/working-with-the-Design-View/#trigger","text":"Icon Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger . Form To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information. Name : A unique name for the trigger. Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows: start : Select this to trigger events when the Streaming Integrator server has started. every : Select this to specify a time interval at which events should be triggered. cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the Example The details entered in the above orm creates a trigger definition as follows: define trigger FiveMinTriggerStream at every 5 min ; Source N/A Target Projection queries Window queries Filter queries Join queries","title":"Trigger"},{"location":"develop/working-with-the-Design-View/#aggregation","text":"Icon Description Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. For more information, see Siddhi Query Guide - Incremental Aggregation . !!! tip Before you add an aggregation, make sure that you have already added the stream with the events to which the aggregation is applied is already defined. Form To configure the aggregation, click the settings icon on the aggregation component you added to the grid, and update the following information. Aggregation Meta Information : In this section, define a unique name for the aggregation in the Name field, and specify the stream from which the input information is taken to perform the aggregations. You can also select the optional annotations you want to use in the aggregation definition by selecting the relevant check boxes. For more information about configuring the annotations once you select them, see Incremental Analysis . Projection : This section specifies the attributes to be included in the aggregation query. In the Select field, you can select All attributes to perform the aggregation for all the attributes of the stream specified under Input , or select User Defined Attributes to select specific attributes. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Aggregation Criteria : Here, you can specify the time values based on which the aggregates are calculated. Example The details entered in the above form creates an aggregation definition as follows: define aggregation TradeAggregation from TradeStream select symbol, avg (price) as avgPrice, sum (price) as total group by symbol aggregate by timestamp every seconds .. .years; Source N/A Target Join queries","title":"Aggregation"},{"location":"develop/working-with-the-Design-View/#function","text":"Icon Description The function icon represents Script in Siddhi Query Language . It allows you to write functions in other programming languages and execute them within Siddhi queries. A function component in a Siddhi application is not connected to ther Siddhi components in the design UI. However, the configuration of one or more Query components can include a reference to it. Form To configure the function, click the settings icon on the function component you added to the grid, and update the following information. Name : A unique name for the function. Script Type : The language in which the function is written. Return Value : The data format of the value that is generated as the output via the function. Script Body : This is a free text field to write the function in the specified script type. Example The details entered in the above form creates a function definition as follows: define function concatFN[JAVASCRIPT] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3; return responce; };","title":"Function"},{"location":"develop/working-with-the-Design-View/#projection-query","text":"Icon Description !!! tip Before you add a projection query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. This icon represents a query to project the events in an input stream to an output stream. This invoves selectng the attributes to be included in the output, renaming attributes, introducing constant values, and using mathematical and/or logical expressions. For more information, see Siddhi Query Guide - Query Projection . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the projection query, click the settings icon on the projection query component you added to the grid, and update the following information. Query Meta Information : This section specifies the stream to be considered as the input stream with the events to which the query needs to be applied. The input stream connected to the query as the source is automatically displayed. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.| Example The details entered in the above form creates a query as follows: from TradeStream select symbol, avg (price) as averagePrice, sum (volume) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Projection Query"},{"location":"develop/working-with-the-Design-View/#filter-query","text":"Icon Description !!! tip Before you add a filter query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A filter query filters information in an input stream based on a given condition. For more information, see Siddhi Query Guide - Filters . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the filter query, click the settings icon on the filter query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the filter type is available under it to indicate that the query is a filter. Expand this stream handler, and enter the condition based on which the information needs to be filtered. !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above form creates a query with a filter as follows: from TradeStream[ sum (amount) > 10000 ] select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Filter Query"},{"location":"develop/working-with-the-Design-View/#window-query","text":"Icon Description !!! tip Before you add a window query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. Window queries include a window to select a subset of events to be processed based on a specific criterion. For more information, see Siddhi Query Guide - (Defined) Window . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the window query, click the settings icon on the window query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the window type is available under it to indicate that the query is a filter. Expand this stream handler, and enter details to determine the window including the window type and the basis on which the subset of events considered by the window is determined (i.e., based on the window type selected). !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above Query Configuration form creates a query with a window as follows: from TradeStream#window. time ( 1 month ) select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source !!! info A window query can have only one source at a given time. Streams Tables Triggers Windows Target Streams Tables Windows","title":"Window Query"},{"location":"develop/working-with-the-Design-View/#join-query","text":"Icon Description A join query derives a combined result from two streams in real-time based on a specified condition. For more information, see Siddhi Query Guide - Join . Form Once you connect two Siddhi components to the join query as sources and another Siddhi component as the target, you can configure the join query. To configure the join query, click the settings icon on the join query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : Here, you can specify the input sources, the references, the join type, join condition, and stream handlers for the left source and right source of the join. For a detailed explanation of the join concept, see Siddhi Query Guide - Joins . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example A join query is configured as follows: The above configurations result in creating the following join query. from TempStream[temp > 30.0 ]#window. time ( 1 min ) as T join RegulatorStream[isOn == false ]#window. length ( 1 ) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, 'start' as action insert into RegulatorActionStream; Source !!! info A join query must always be connected to two sources, and at least one of them must be a defined stream/trigger/window. Streams Tables Aggregations Windows Target !!! info A join query must always be connected to a single target. Streams Tables Windows","title":"Join Query"},{"location":"develop/working-with-the-Design-View/#pattern-query","text":"Icon Description !!! tip Before you add a pattern query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A pattern query detects patterns in events that arrive overtime. For more information, see Siddhi Query Guide - Patterns . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the pattern query, click the settings icon on the pattern query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which patterns are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Pattern concept, see Siddhi Query Guide - Patterns . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every (e1 = MaterialSupplyStream) -> not MaterialConsumptionStream[name == e1.name and amount == e1.amount] for 15 sec select e1.name, e1.amount insert into ProductionDelayAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Pattern Query"},{"location":"develop/working-with-the-Design-View/#sequence-query","text":"Icon Description !!! tip Before you add a sequence query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A sequence query detects sequences in event occurrences over time. For more information, see Siddhi Query Guide - Sequence . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the sequence query, click the settings icon on the sequence query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which sequences are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Sequence concept, see Siddhi Query Guide - Sequences . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every e1 = SweetProductionStream, e2 = SweetProductionStream[e1.amount > amount and ( timestamp - e1. timestamp ) < 10 * 60000 ] * , e3 = SweetProductionStream[ timestamp - e1. timestamp > 10 * 60000 and e1.amount > amount] select e1.name, e1.amount as initialAmount, e2.amount as finalAmount, e2. timestamp insert into DecreasingTrendAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Sequence Query"},{"location":"develop/working-with-the-Design-View/#partitions","text":"Icon Description !!! tip Before you add a partition: You need to add the stream to be partitioned. Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. For more information, see Siddhi Query Guide - Partition . Form Once the stream to be partitioned is connected as a source to the partition, you can configure the partition. In order to do so, move the cursor over the partition and click the settings icon on the partition component. This opens the Partition Configuration form. In this form, you can enter expressions to convert the attributes of the stream that is selected to be partitioned. Example The above configuration creates the following partition query. partition with ( roomNo >= 1030 as 'serverRoom' or roomNo < 1030 and roomNo >= 330 as 'officeRoom' or roomNo < 330 as 'lobby' of TempStream) begin from TempStream#window. time ( 10 min ) select roomNo, deviceID, avg (temp) as avgTemp insert into AreaTempStream end ; Source Streams Target N/A","title":"Partitions"},{"location":"develop/working-with-the-Design-View/#connecting-siddhi-components","text":"In order to define how the Siddhi components in a Siddhi application interact with each other to process events, you need to define connections between Siddhi components. A connection is defined by drawing an arrow from one component to another by dragging the cursor as demonstrated below.","title":"Connecting Siddhi components"},{"location":"develop/working-with-the-Design-View/#saving-running-and-debugging-siddhi-applications","text":"To save a Siddhi application that you created in the design view, you need to switch to the source view. You also need to switch to the source view to run or debug a Siddhi application. For more information, see the following sections: Streaming Integrator Tiooling Overview Debugging a Siddhi Application","title":"Saving, running and debugging Siddhi applications"},{"location":"examples/analyzing-data-and-deriving-insights/","text":"","title":"Analyzing data and deriving insights"},{"location":"examples/exposing-processed-data-as-api/","text":"Siddhi Query API \u00b6 Introduction \u00b6 Siddhi query API is the REST API exposed by the Streaming Integrator (SI). It gives you a set of APIs to perform all of the essential operations relating to Siddhi applications, including developing, testing, and querying them. Siddhi query API provides APIs related to: - Siddhi application management (such as creating, updating, deleting a Siddhi application; Listing all running Siddhi applications etc.) - Event simulation - Authentication and Permission management - Health check - Siddhi Store operations For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide . This tutorial demonstrates how you can use the Siddhi query API to perform essential operations in SI, using simple examples. Preparing the server \u00b6 Before you begin: You need to have access to a MySQL instance. Save the MySQL JDBC driver in the <SI_HOME>/lib directory as follows: Download the MySQL JDBC driver from the MySQL site . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. Start the SI server. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Creating a Siddhi application \u00b6 Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Store\") @App:description('Receive events via HTTP and persist the received data in the store.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream insertSweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2si\", password=\"wso2\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table SweetProductionTable (name string, amount double); from insertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/production?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as SweetProduction-Store.siddhi in a location of your choice in the local file system. Now you need to execute a CURL command and deploy this Siddhi application. In the command line, navigate to the location where you saved the Siddhi application in the previous step, and execute following command: curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @SweetProduction-Store.siddhi -u admin:admin -k Upon successful deployment, the following response is logged for the CURL command you just executed. {\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"} In addition to that, the following is logged in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App SweetProduction-Store deployed successfully !!!info Next, you are going to send a few events to `insertSweetProductionStream` stream via a `CURL` command. Execute following CURL command in the console: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Info You have written the Siddhi application to insert a new record from the insertSweetProductionStream stream into the SweetProductionTable table, or to update the record if it already exists in the table. As a result, above event is now inserted into the SweetProductionTable . To verify whether above event is inserted into SweetProductionTable , execute following SQL query in the SQL console: SELECT * FROM SweetProductionTable; The following table appears to indicate that the record has been inserted into the table. +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+ Running a Siddhi Store API query \u00b6 You can use Siddhi Store Query API to execute queries on Siddhi Store tables. In this section shows you how to execute a simple store query via the REST API in order to fetch all records from the SweetProductionTable Siddhi Store table. To learn about other types of queries, see Streaming Integrator REST API Guide . Execute following CURL command on the console: curl -k -X POST http://localhost:7070/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select *\" }' The following output is logged in the console: {\"records\":[[\"Almond cookie\",100.0]]} Fetching the status of a Siddhi Application \u00b6 Now let's fetch the status of the Siddhi application you just deployed. Execute following CURL command, in the console: curl -X GET \"http://localhost:9090/siddhi-apps/SweetProduction-Store/status\" -H \"accept: application/json\" -u admin:admin -k The following output appears in the command line: {\"status\":\"active\"} Taking a snapshot of a Siddhi Application \u00b6 In this section, deploy a stateful Siddhi application and use the REST API to take a snapshot of it. To do this, follow the procedure below: To enable the state persistence feature in SI server, open the <SI_HOME>/conf/server/deployment.yaml file in a text editor and locate the state.persistence section, and then update it as follows. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence As shown above, set enabled parameter to true and set the intervalInMin to 5 . Then save the file. Restart the Streaming Integrator server for the above change to be effective. Open a text file and copy-paste following application into it. @App:name(\"CountProductions\") @App:description(\"Siddhi application to count the total number of orders.\") @Source(type = 'http', receiver.url='http://localhost:8007/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions.siddhi in a location of your choice in the local file system. Now execute a CURL command and deploy this Siddhi application. To do this, use the command line to navigate to the location where you saved the Siddhi application in above step, and then execute following command. curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @CountProductions.siddhi -u admin:admin -k Upon successful deployment, the following response is logged for the CURL command you just executed. {\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"} In addition to that, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully. Now let's send two sweet production events using CURL by issuing tghe following two CURL commands via the command line: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following logs appears on the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[100.0], isExpired=false} INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[120.0], isExpired=false} Note that the current productions count is 120 . Now you can invoke the Siddhi Query API to take a snapshot of the Siddhi application. To do this, execute following CURL command on the command line: curl -X POST \"https://localhost:9443/siddhi-apps/CountProductions/backup\" -H \"accept: application/json\" -u admin:admin -k An output similar to the following is logged. {\"revision\":\"1566293390654__CountProductions\"} Info 1566293390654__CountProductions is the revision number of the Siddhi application snapshot that you requested via the REST API. You can store this revision number and later use it in order to restore the Siddhi application to the state at which you took the snapshot. Restoring a Siddhi Application via a snapshot \u00b6 In the previous section, you took a snapshot of the CountProductions Siddhi application when the productions count was 120 . In this section, you can increase the count further by sending a few more production events, and then restore the Siddhi application to the state you backed up. To do this, follow the procedure below Send following two sweet production events: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following two lines of log appear in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[420.0], isExpired=false} INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[920.0], isExpired=false} Note that the current productions count is 920 . Now you can invoke the Siddhi Query API to restore the snapshot that you obtained in step 10 of the Taking a snapshot of a Siddhi Application section of this tutorial. In this example, the revision number obtained is 1566293390654__CountProductions (see step 10 in Taking a snapshot of a Siddhi Application section.). When restoring the state, use the exact revision number that you obtained. Note Replace 1566293390654__CountProductions with the revision number that you obtained when taking the Siddhi application snapshot. The response you receive is as follows: {\"type\":\"success\",\"message\":\"State restored to revision 1566293390654__CountProductions for Siddhi App :CountProductions\"} In addition to that, the following log is printed in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - State loaded for CountProductions revision 1566293390654__CountProductions from the file system. Now send another sweet production event by executing the following CURL command: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Danish pastry\\\",\\\"amount\\\": 100.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566293520176, data=[220.0], isExpired=false} Note that the productions count is 220 . This is because the count was reset to 120 when you restored the snapshot.","title":"Exposing Processed Data as API"},{"location":"examples/exposing-processed-data-as-api/#siddhi-query-api","text":"","title":"Siddhi Query API"},{"location":"examples/exposing-processed-data-as-api/#introduction","text":"Siddhi query API is the REST API exposed by the Streaming Integrator (SI). It gives you a set of APIs to perform all of the essential operations relating to Siddhi applications, including developing, testing, and querying them. Siddhi query API provides APIs related to: - Siddhi application management (such as creating, updating, deleting a Siddhi application; Listing all running Siddhi applications etc.) - Event simulation - Authentication and Permission management - Health check - Siddhi Store operations For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide . This tutorial demonstrates how you can use the Siddhi query API to perform essential operations in SI, using simple examples.","title":"Introduction"},{"location":"examples/exposing-processed-data-as-api/#preparing-the-server","text":"Before you begin: You need to have access to a MySQL instance. Save the MySQL JDBC driver in the <SI_HOME>/lib directory as follows: Download the MySQL JDBC driver from the MySQL site . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. Start the SI server. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));","title":"Preparing the server"},{"location":"examples/exposing-processed-data-as-api/#creating-a-siddhi-application","text":"Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Store\") @App:description('Receive events via HTTP and persist the received data in the store.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream insertSweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2si\", password=\"wso2\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table SweetProductionTable (name string, amount double); from insertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/production?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as SweetProduction-Store.siddhi in a location of your choice in the local file system. Now you need to execute a CURL command and deploy this Siddhi application. In the command line, navigate to the location where you saved the Siddhi application in the previous step, and execute following command: curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @SweetProduction-Store.siddhi -u admin:admin -k Upon successful deployment, the following response is logged for the CURL command you just executed. {\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"} In addition to that, the following is logged in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App SweetProduction-Store deployed successfully !!!info Next, you are going to send a few events to `insertSweetProductionStream` stream via a `CURL` command. Execute following CURL command in the console: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Info You have written the Siddhi application to insert a new record from the insertSweetProductionStream stream into the SweetProductionTable table, or to update the record if it already exists in the table. As a result, above event is now inserted into the SweetProductionTable . To verify whether above event is inserted into SweetProductionTable , execute following SQL query in the SQL console: SELECT * FROM SweetProductionTable; The following table appears to indicate that the record has been inserted into the table. +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+","title":"Creating a Siddhi application"},{"location":"examples/exposing-processed-data-as-api/#running-a-siddhi-store-api-query","text":"You can use Siddhi Store Query API to execute queries on Siddhi Store tables. In this section shows you how to execute a simple store query via the REST API in order to fetch all records from the SweetProductionTable Siddhi Store table. To learn about other types of queries, see Streaming Integrator REST API Guide . Execute following CURL command on the console: curl -k -X POST http://localhost:7070/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select *\" }' The following output is logged in the console: {\"records\":[[\"Almond cookie\",100.0]]}","title":"Running a Siddhi Store API query"},{"location":"examples/exposing-processed-data-as-api/#fetching-the-status-of-a-siddhi-application","text":"Now let's fetch the status of the Siddhi application you just deployed. Execute following CURL command, in the console: curl -X GET \"http://localhost:9090/siddhi-apps/SweetProduction-Store/status\" -H \"accept: application/json\" -u admin:admin -k The following output appears in the command line: {\"status\":\"active\"}","title":"Fetching the status of a Siddhi Application"},{"location":"examples/exposing-processed-data-as-api/#taking-a-snapshot-of-a-siddhi-application","text":"In this section, deploy a stateful Siddhi application and use the REST API to take a snapshot of it. To do this, follow the procedure below: To enable the state persistence feature in SI server, open the <SI_HOME>/conf/server/deployment.yaml file in a text editor and locate the state.persistence section, and then update it as follows. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 5 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence As shown above, set enabled parameter to true and set the intervalInMin to 5 . Then save the file. Restart the Streaming Integrator server for the above change to be effective. Open a text file and copy-paste following application into it. @App:name(\"CountProductions\") @App:description(\"Siddhi application to count the total number of orders.\") @Source(type = 'http', receiver.url='http://localhost:8007/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions.siddhi in a location of your choice in the local file system. Now execute a CURL command and deploy this Siddhi application. To do this, use the command line to navigate to the location where you saved the Siddhi application in above step, and then execute following command. curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @CountProductions.siddhi -u admin:admin -k Upon successful deployment, the following response is logged for the CURL command you just executed. {\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"} In addition to that, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully. Now let's send two sweet production events using CURL by issuing tghe following two CURL commands via the command line: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following logs appears on the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[100.0], isExpired=false} INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[120.0], isExpired=false} Note that the current productions count is 120 . Now you can invoke the Siddhi Query API to take a snapshot of the Siddhi application. To do this, execute following CURL command on the command line: curl -X POST \"https://localhost:9443/siddhi-apps/CountProductions/backup\" -H \"accept: application/json\" -u admin:admin -k An output similar to the following is logged. {\"revision\":\"1566293390654__CountProductions\"} Info 1566293390654__CountProductions is the revision number of the Siddhi application snapshot that you requested via the REST API. You can store this revision number and later use it in order to restore the Siddhi application to the state at which you took the snapshot.","title":"Taking a snapshot of a Siddhi Application"},{"location":"examples/exposing-processed-data-as-api/#restoring-a-siddhi-application-via-a-snapshot","text":"In the previous section, you took a snapshot of the CountProductions Siddhi application when the productions count was 120 . In this section, you can increase the count further by sending a few more production events, and then restore the Siddhi application to the state you backed up. To do this, follow the procedure below Send following two sweet production events: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following two lines of log appear in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[420.0], isExpired=false} INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[920.0], isExpired=false} Note that the current productions count is 920 . Now you can invoke the Siddhi Query API to restore the snapshot that you obtained in step 10 of the Taking a snapshot of a Siddhi Application section of this tutorial. In this example, the revision number obtained is 1566293390654__CountProductions (see step 10 in Taking a snapshot of a Siddhi Application section.). When restoring the state, use the exact revision number that you obtained. Note Replace 1566293390654__CountProductions with the revision number that you obtained when taking the Siddhi application snapshot. The response you receive is as follows: {\"type\":\"success\",\"message\":\"State restored to revision 1566293390654__CountProductions for Siddhi App :CountProductions\"} In addition to that, the following log is printed in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - State loaded for CountProductions revision 1566293390654__CountProductions from the file system. Now send another sweet production event by executing the following CURL command: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Danish pastry\\\",\\\"amount\\\": 100.0}}\" http://localhost:8007/productionStream --header \"Content-Type:application/json\" As a result, the following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566293520176, data=[220.0], isExpired=false} Note that the productions count is 220 . This is because the count was reset to 120 when you restored the snapshot.","title":"Restoring a\u00a0Siddhi Application via a snapshot"},{"location":"examples/integrating-databases-to-data-flows/","text":"","title":"Integrating databases to data flows"},{"location":"examples/manage-stored-data-via-rest-api/","text":"Manage Stored Data via Rest API \u00b6 Introduction \u00b6 The Siddhi Store Query API is the REST API exposed by the Streaming Integrator (SI) in order to perform actions on the stored data. Stored data includes, - Siddhi Stores - Siddhi Aggregations - Siddhi Windows, that you have defined in Siddhi applications. You can perform actions such as, - inserting - searching - updating - deleting on those stored data, using the Rest API. For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide . This tutorial demonstrates how you can use the Siddhi Store Query API to perform a few essential operations in SI, using simple examples. Preparing the server \u00b6 !!!tip\"Before you begin:\" - You need to have access to a MySQL instance. - Save the MySQL JDBC driver in the <SI_HOME>/lib directory as follows: 1. Download the MySQL JDBC driver from the MySQL site . 2. Unzip the archive. 3. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. 4. Start the SI server. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Managing Siddhi Store Data \u00b6 Preparing an RDBMS Store \u00b6 First let's create a Siddhi application with an RDBMS Store so that we can try out certain operations on it. Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Store\") @App:description('Receive events via HTTP and persist the received data in the store.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream insertSweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2si\", password=\"wso2\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table SweetProductionTable (name string, amount double); from insertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/production?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as SweetProduction-Store.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Inserting records \u00b6 To insert a record into the SweetProductionTable , execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond cookie\\\" as name, 100.0 as amount insert into SweetProductionTable;\" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} Let's insert a few more records by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Baked alaska\\\" as name, 20.0 as amount insert into SweetProductionTable;\" }' -k curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Cup cake\\\" as name, 30.0 as amount insert into SweetProductionTable;\" }' -k curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Doughnut\\\" as name, 500.0 as amount insert into SweetProductionTable;\" }' -k !!!info Above `CURL` commands, insert the following records into the `SweetProductionTable` table. In the next section, we will retrieve these record from the table. +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+ | Baked alaska | 20.00 | +---------------+--------+ | Cup cake | 30.00 | +---------------+--------+ | Doughnut | 500.00 | +---------------+--------+ Searching records \u00b6 To obtain all of the records from the SweetProductionTable , execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Now let's obtain all of the records which has amount greater than 25 and order those by amount . curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount > 25 select name, amount order by amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0],[\"Doughnut\",500.0]]} To get the top two records from above ouput, let's use the limit condition on the Store Query: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount > 25 select name, amount order by amount limit 2; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0]]} !!! info For more information on search queries, refer [Store APIs: Streaming Integration REST API Guide](https://ei.docs.wso2.com/en/latest/streaming-integrator/ref/si-rest-api-guide/#store-apis). Updating records \u00b6 Now let's update the following record in the SweetProductionTable . We will set the name to Almond Cookie and amount to 150.0 . +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+ In order to update above record, execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond Cookie\\\" as name, 150.0 as amount update or insert into SweetProductionTable set SweetProductionTable.name = name, SweetProductionTable.amount = amount on SweetProductionTable.name == \\\"Almond cookie\\\";\"}' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} !!! tip To verify whether the update is successful, you can execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Almond Cookie\",150.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Notice that the [\"Almond cookie\",100.0] has being changed to [\"Almond Cookie\",150.0] Deleting records \u00b6 Let's delete the entry [\"Almond Cookie\",150.0] from the SweetProductionTable by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select 150.0 as amount delete SweetProductionTable on SweetProductionTable2.amount == amount;\" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} !!! tip To verify whether the delete is successful, you can execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Notice that the [\"Almond Cookie\",150.0] record has being deleted. Managing Data in a Siddhi Aggregation \u00b6 First let's create a Siddhi application with an Aggregation, so that we can try out search operations on it later. Open a text file and copy-paste following application to it. @App:name(\"AggregateDataIncrementally\") @App:description('Aggregates values every second until year and gets statistics') @Source(type = 'http', receiver.url='http://localhost:8006/rawMaterialStream', basic.auth.enabled='false', @map(type='json')) define stream RawMaterialStream (name string, amount double); @store( type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation RawMaterialAggregation from RawMaterialStream select name, avg(amount) as avgAmount, sum(amount) as totalAmount group by name aggregate every sec...year; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/sweetFactoryDB?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as AggregateDataIncrementally.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Let's insert a few records into the RawMaterialStream so that those data will be summarized and you can query for the summary later. Info Unlike RDBMS Stores, you cannot insert records into a Aggregation table straight away. In order to put records, you need to insert data into the event stream which is associated to the Aggregation. In this example, the RawMaterialAggregation aggregation table is associated to the event stream 'RawMaterialStream . Therefore, in order to insert into the aggregation table, you need to insert into the 'RawMaterialStream . Execute following CURL command on a terminal: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 30.0}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" Now let's use the Store Query API, in order to find out the average and total amount of Almond cookie productions. Info Above, you have executed following Store Query: from RawMaterialAggregation on name==\"Almond cookie\" within \"2019-**-** **:**:** +05:30\" per \"hours\" select AGG_TIMESTAMP, name, avgAmount, totalAmount\" This query retrieves the average and total Almond cookie productions happened within the year 2019 . The average and total is calculated for every hour. You will get following response: {\"records\":[[1571234400000,\"Almond cookie\",50.166666666666664,150.5]]} !!! info The value 1571234400000 indicates the Unix timestamp for which the result set belong. In this example, the result set belongs to October 16, 2019 7:30:00 PM GMT+05:30 . 50.166666666666664 is the average amount of Almond cookie productions happened within the hour, while 150.5 is the total amount of Almond cookie productions happened within the hour. Managing Data in a Siddhi Window \u00b6 Let's create a Siddhi application with a Window and then query the status of the Window, using a Store query. Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Window\") @Source(type = 'http', receiver.url='http://localhost:8008/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); define window LastFourProductions (name string, amount double) lengthBatch(4); @sink(type='log') define stream LogStream (name string, sumAmount double); @info(name = 'query1') from SweetProductionStream select name, amount insert into LastFourProductions; @info(name = 'query2') from LastFourProductions select name,sum(amount) as sumAmount insert into LogStream; Info The above Siddhi application calculates the sum of last four productions (in batches). The last four productions are retained in the LastFourProductions window which is a lengthBatch window of size four. query1 inserts all of the incoming sweet productions into the LastFourProductions window. query2 calculates the sum of the batch of four productions, within the window. Save this file as SweetProduction-Window.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Let's insert four events into SweetProductionStream . Execute following CURL commands on the terminal: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" Once you send the fourth event, a batch of four productions completes; hence the following log appears on the SI console. The log prints the sum of amounts of the four productions. INFO {io.siddhi.core.stream.output.sink.LogSink} - SweetProduction-Window : LogStream : Event{timestamp=1571675148391, data=[Doughnut, 920.0], isExpired=false} Now, using the Store Query API, you will be querying the contents in the LastFourProductions window. Let's select all events in the window by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select *\" }' -k On successful execution of the command, you get following response on the terminal. The output shows the last four sweet productions. {\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",300.0],[\"Doughnut\",500.0]]} Next, using the Store Query API, you will be querying for the maximum production amount, among the four productions that are in the LastFourProductions window. Execute following CURL command on the terminal: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select max(amount) as maxAmount\" }' -k On successful execution of the command, you get following response on the terminal. The output shows the maximum amount of the four sweet productions. {\"records\":[[500.0]]}","title":"Manage Stored Data via Rest API"},{"location":"examples/manage-stored-data-via-rest-api/#manage-stored-data-via-rest-api","text":"","title":"Manage Stored Data via Rest API"},{"location":"examples/manage-stored-data-via-rest-api/#introduction","text":"The Siddhi Store Query API is the REST API exposed by the Streaming Integrator (SI) in order to perform actions on the stored data. Stored data includes, - Siddhi Stores - Siddhi Aggregations - Siddhi Windows, that you have defined in Siddhi applications. You can perform actions such as, - inserting - searching - updating - deleting on those stored data, using the Rest API. For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide . This tutorial demonstrates how you can use the Siddhi Store Query API to perform a few essential operations in SI, using simple examples.","title":"Introduction"},{"location":"examples/manage-stored-data-via-rest-api/#preparing-the-server","text":"!!!tip\"Before you begin:\" - You need to have access to a MySQL instance. - Save the MySQL JDBC driver in the <SI_HOME>/lib directory as follows: 1. Download the MySQL JDBC driver from the MySQL site . 2. Unzip the archive. 3. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. 4. Start the SI server. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));","title":"Preparing the server"},{"location":"examples/manage-stored-data-via-rest-api/#managing-siddhi-store-data","text":"","title":"Managing Siddhi Store Data"},{"location":"examples/manage-stored-data-via-rest-api/#preparing-an-rdbms-store","text":"First let's create a Siddhi application with an RDBMS Store so that we can try out certain operations on it. Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Store\") @App:description('Receive events via HTTP and persist the received data in the store.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream insertSweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2si\", password=\"wso2\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table SweetProductionTable (name string, amount double); from insertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/production?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as SweetProduction-Store.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory.","title":"Preparing an RDBMS Store"},{"location":"examples/manage-stored-data-via-rest-api/#inserting-records","text":"To insert a record into the SweetProductionTable , execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond cookie\\\" as name, 100.0 as amount insert into SweetProductionTable;\" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} Let's insert a few more records by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Baked alaska\\\" as name, 20.0 as amount insert into SweetProductionTable;\" }' -k curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Cup cake\\\" as name, 30.0 as amount insert into SweetProductionTable;\" }' -k curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Doughnut\\\" as name, 500.0 as amount insert into SweetProductionTable;\" }' -k !!!info Above `CURL` commands, insert the following records into the `SweetProductionTable` table. In the next section, we will retrieve these record from the table. +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+ | Baked alaska | 20.00 | +---------------+--------+ | Cup cake | 30.00 | +---------------+--------+ | Doughnut | 500.00 | +---------------+--------+","title":"Inserting records"},{"location":"examples/manage-stored-data-via-rest-api/#searching-records","text":"To obtain all of the records from the SweetProductionTable , execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Now let's obtain all of the records which has amount greater than 25 and order those by amount . curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount > 25 select name, amount order by amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0],[\"Doughnut\",500.0]]} To get the top two records from above ouput, let's use the limit condition on the Store Query: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount > 25 select name, amount order by amount limit 2; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0]]} !!! info For more information on search queries, refer [Store APIs: Streaming Integration REST API Guide](https://ei.docs.wso2.com/en/latest/streaming-integrator/ref/si-rest-api-guide/#store-apis).","title":"Searching records"},{"location":"examples/manage-stored-data-via-rest-api/#updating-records","text":"Now let's update the following record in the SweetProductionTable . We will set the name to Almond Cookie and amount to 150.0 . +---------------+--------+ | name | amount | +---------------+--------+ | Almond cookie | 100.00 | +---------------+--------+ In order to update above record, execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond Cookie\\\" as name, 150.0 as amount update or insert into SweetProductionTable set SweetProductionTable.name = name, SweetProductionTable.amount = amount on SweetProductionTable.name == \\\"Almond cookie\\\";\"}' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} !!! tip To verify whether the update is successful, you can execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Almond Cookie\",150.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Notice that the [\"Almond cookie\",100.0] has being changed to [\"Almond Cookie\",150.0]","title":"Updating records"},{"location":"examples/manage-stored-data-via-rest-api/#deleting-records","text":"Let's delete the entry [\"Almond Cookie\",150.0] from the SweetProductionTable by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select 150.0 as amount delete SweetProductionTable on SweetProductionTable2.amount == amount;\" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[]} !!! tip To verify whether the delete is successful, you can execute following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k On successful execution of the command, you will get following response on the terminal: {\"records\":[[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]} Notice that the [\"Almond Cookie\",150.0] record has being deleted.","title":"Deleting records"},{"location":"examples/manage-stored-data-via-rest-api/#managing-data-in-a-siddhi-aggregation","text":"First let's create a Siddhi application with an Aggregation, so that we can try out search operations on it later. Open a text file and copy-paste following application to it. @App:name(\"AggregateDataIncrementally\") @App:description('Aggregates values every second until year and gets statistics') @Source(type = 'http', receiver.url='http://localhost:8006/rawMaterialStream', basic.auth.enabled='false', @map(type='json')) define stream RawMaterialStream (name string, amount double); @store( type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation RawMaterialAggregation from RawMaterialStream select name, avg(amount) as avgAmount, sum(amount) as totalAmount group by name aggregate every sec...year; Here the jdbc.url parameter has the value jdbc:mysql://localhost:3306/sweetFactoryDB?useSSL=false . Change it to point to your MySQL server. Similarly change username and password parameters as well. Save this file as AggregateDataIncrementally.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Let's insert a few records into the RawMaterialStream so that those data will be summarized and you can query for the summary later. Info Unlike RDBMS Stores, you cannot insert records into a Aggregation table straight away. In order to put records, you need to insert data into the event stream which is associated to the Aggregation. In this example, the RawMaterialAggregation aggregation table is associated to the event stream 'RawMaterialStream . Therefore, in order to insert into the aggregation table, you need to insert into the 'RawMaterialStream . Execute following CURL command on a terminal: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 30.0}}\" http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\" Now let's use the Store Query API, in order to find out the average and total amount of Almond cookie productions. Info Above, you have executed following Store Query: from RawMaterialAggregation on name==\"Almond cookie\" within \"2019-**-** **:**:** +05:30\" per \"hours\" select AGG_TIMESTAMP, name, avgAmount, totalAmount\" This query retrieves the average and total Almond cookie productions happened within the year 2019 . The average and total is calculated for every hour. You will get following response: {\"records\":[[1571234400000,\"Almond cookie\",50.166666666666664,150.5]]} !!! info The value 1571234400000 indicates the Unix timestamp for which the result set belong. In this example, the result set belongs to October 16, 2019 7:30:00 PM GMT+05:30 . 50.166666666666664 is the average amount of Almond cookie productions happened within the hour, while 150.5 is the total amount of Almond cookie productions happened within the hour.","title":"Managing Data in a Siddhi Aggregation"},{"location":"examples/manage-stored-data-via-rest-api/#managing-data-in-a-siddhi-window","text":"Let's create a Siddhi application with a Window and then query the status of the Window, using a Store query. Open a text file and copy-paste following application to it. @App:name(\"SweetProduction-Window\") @Source(type = 'http', receiver.url='http://localhost:8008/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); define window LastFourProductions (name string, amount double) lengthBatch(4); @sink(type='log') define stream LogStream (name string, sumAmount double); @info(name = 'query1') from SweetProductionStream select name, amount insert into LastFourProductions; @info(name = 'query2') from LastFourProductions select name,sum(amount) as sumAmount insert into LogStream; Info The above Siddhi application calculates the sum of last four productions (in batches). The last four productions are retained in the LastFourProductions window which is a lengthBatch window of size four. query1 inserts all of the incoming sweet productions into the LastFourProductions window. query2 calculates the sum of the batch of four productions, within the window. Save this file as SweetProduction-Window.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Let's insert four events into SweetProductionStream . Execute following CURL commands on the terminal: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\" http://localhost:8008/productionStream --header \"Content-Type:application/json\" Once you send the fourth event, a batch of four productions completes; hence the following log appears on the SI console. The log prints the sum of amounts of the four productions. INFO {io.siddhi.core.stream.output.sink.LogSink} - SweetProduction-Window : LogStream : Event{timestamp=1571675148391, data=[Doughnut, 920.0], isExpired=false} Now, using the Store Query API, you will be querying the contents in the LastFourProductions window. Let's select all events in the window by executing following CURL command: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select *\" }' -k On successful execution of the command, you get following response on the terminal. The output shows the last four sweet productions. {\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",300.0],[\"Doughnut\",500.0]]} Next, using the Store Query API, you will be querying for the maximum production amount, among the four productions that are in the LastFourProductions window. Execute following CURL command on the terminal: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select max(amount) as maxAmount\" }' -k On successful execution of the command, you get following response on the terminal. The output shows the maximum amount of the four sweet productions. {\"records\":[[500.0]]}","title":"Managing Data in a Siddhi Window"},{"location":"examples/performing-real-time-etl-with-files/","text":"Performing Real-time ETL with Files \u00b6 Introduction \u00b6 The Streaming Integrator (SI) allows you to perform real-time ETL with data that is stored in files. This tutorial takes you through the different modes and options you could use, in order to perform real-time ETL with files using the SI. Before you begin: Start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: - For Windows: streaming-integrator.bat - For Linux: sh server.sh The following log appears in the Streaming Integrator console once the server is successfully started. INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec Tutorial steps \u00b6 Extracting data from a file \u00b6 In this section of the tutorial, you are exploring the different ways in which you could extract data from a specific file. Tailing a text file line by line \u00b6 In this scenario, you are tailing a text file, line by line, in order to extract data from it. Each line is extracted as an event that undergoes a simple transformation thereafter. Let's write a simple Siddhi application to do this. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('TailFileLineByLine') @App:description('Tails a file line by line and does a simple transformation.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='true', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the file path to which you downloaded productions.csv file in step 1. Save this file as TailFileLineByLine.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the file productions.csv line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs. The transformation involves converting the value for the name attribute to upper case. Finally, the output is logged in the Streaming Integrator console. Upon successful deployment, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileLineByLine deployed successfully Now the Siddhi application starts to process the productions.csv file. The file contains the following entries. Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830652, data=[ALMOND COOKIE, 100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830657, data=[BAKED ALASKA, 20.0], isExpired=false} Now append the following line to productions.csv file and save the file. Cup cake,300.0 The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490869579, data=[CUP CAKE, 300.0], isExpired=false} Tailing a text file using a regular expression \u00b6 In this scenario, you are using a regular expression to extract data from the file. After data is extracted, a simple transformation is performed on them. Finally, the transformed event is logged in the SI console. Let's write a simple Siddhi application to do this. Download noisy_data.txt file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('TailFileRegex') @App:description('Tails a file using a regex and does a simple transformation.') @source(type='file', mode='REGEX', file.uri='file:/Users/foo/noisy_data.txt', begin.regex='\\<', end.regex='\\>', tailing='true', @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream[NOT(symbol is null)] select str:upper(symbol) as symbol, price, volume insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the file path to which you downloaded noisy_data.txt file in step 1. Save this file as TailFileRegex.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the noisy_data.txt file to find matches according to the regular expressions given: begin.regex and end.regex . Each match is converted to an event in the StockStream stream. After that, a simple transformation is carried out on the StockStream stream where the value for the symbol attribute from the event is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileRegex deployed successfully Now the Siddhi application starts to process the noisy_data.txt file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307974, data=[WSO2, 75.0, 100], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307975, data=[ORCL, 95.0, 200], isExpired=false} Now append the following text to noisy_data.txt file and save the file. IBM <ibm 88 volume 150> 1 New Orchard Rd Armonk, NY 10504 Phone Number: (914) 499-1900 Fax Number: (914) 765-6021 The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564588585214, data=[IBM, 88.0, 150], isExpired=false} Reading a remote text file and moving it after processing \u00b6 In the previous scenarios, you tailed a file and each file generated multiple events. In this scenario, you are reading the complete file to build a single event. Furthermore, to try out the capability of processing remote files, you are processing a remote file instead of a file located in the local file system. Download portfolio.txt file from here and upload it into an FTP server. Create a directory on the FTP server. The portfolio.txt file is moved to this folder after the processing is complete. Open a text file and copy-paste following Siddhi application to it. @App:name('TextFullFileProcessing') @App:description('Reads a text file and moves it after processing.') @source(type='file', mode='TEXT.FULL', file.uri=\"ftp://<username>:<password>@<ftp_hostname>:<ftp_port>/Users/foo/portfolio.txt\", action.after.process='MOVE', move.after.process=\"ftp://<username>:<password>@<ftp_hostname>:<ftp_port>/Users/foo/move.after.process\", @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the remote file path to which you uploaded the portfolio.txt file in step 1. In addition to that, change move.after.process so that it points to the remote folder you created in step 2. When configuring both of the above parameters, change the values for <username> , <password> , <ftp_hostname> , and <ftp_port> parameters accordingly. Save this file as TextFullFileProcessing.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application reads the complete portfolio.txt remote file to create a StockStream event. After that, a simple transformation is carried out on the StockStream stream where the value for the symbol attribute in each event is converted ito upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TextFullFileProcessing deployed successfully Now the Siddhi application starts to process the portfolio.txt file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TextFullFileProcessing : LogStream : Event{timestamp=1564660443519, data=[WSO2, 55.6, 100], isExpired=false} Info In this scenario, you moved the file after processing. To delete a file after processing, remove the action.after.process and move.after.process parameters from the Siddhi application. For other configuration options, see Siddhi File Source documentation . Reading a binary file and moving it after processing \u00b6 In the previous scenarios, you processed text files in order to extract data. In this scenario, you are reading a binary file. The content of the file generates a single event. Download wso2.bin file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('BinaryFullFileProcessing') @App:description('Reads a binary file and moves it after processing.') @source(type='file', mode='TEXT.FULL', file.uri='file:/Users/foo/wso2.bin', action.after.process='MOVE', move.after.process='file:/Users/foo/move.after.process', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value for the file.uri parameter to the file path to which you downloaded the wso2.bin file in step 1. Save this file as BinaryFullFileProcessing.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application reads the file wso2.bin fully to create a StockStream event. After that, a simple transformation is carried out for the StockStream stream where the value for the symbol attribute is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App BinaryFullFileProcessing deployed successfully Now the Siddhi application starts to process the wso2.bin file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - BinaryFullFileProcessing : LogStream : Event{timestamp=1564660553623, data=[WSO2, 55.6, 100], isExpired=false} Reading a file line by line and delete it after processing \u00b6 In this scenario, you are reading a text file completely, and then deleting it after processing. In other words, the file is not tailed. You read the file line by line where each line generates an event. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ReadFileLineByLine') @App:description('Reads a file line by line and does a simple transformation.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='false', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into LogStream; In the above Siddhi application, change the value for the file.uri parameter to the file path to which you downloaded the productions.csv file in step 1. Save this file as ReadFileLineByLine.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the productions.csv file line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs where the value for the name attribute from the event is converted into upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileLineByLine deployed successfully Now the Siddhi application starts to process the productions.csv file. The file has below two entries: Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[ALMOND COOKIE, 100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[BAKED ALASKA, 20.0], isExpired=false} Note that productions.csv file is not present in the file.uri location. Next, create a new productions.csv file in the file.uri location that includes the latest set of productions. Download productions.csv file from here and save it in the file.uri location. Now the Siddhi application starts to process the new set of production runs in the productions.csv file. The file has the following two entries. Cup cake,300.0 Doughnut,500.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[CUP CAKE, 300.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[DOUGHNUT, 500.0], isExpired=false} Reading a file using a regular expression and deleting it after processing \u00b6 In this scenario, you are using a regular expression to extract data from the content of the file. Here, you do not tail the file. Instead, you read the full content of the file and generate a single event. After this is done, the file is deleted. To generate an event stream, you can keep re-creating the file with new data. Download noisy_data.txt file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ReadFileRegex') @App:description('Reads a file using a regex and does a simple transformation.') @source(type='file', mode='REGEX', file.uri='file:/Users/foo/noisy_data.txt', begin.regex='\\<', end.regex='\\>', tailing='false', @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream[NOT(symbol is null)] select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value of the file.uri parameter to the file path to which you downloaded the noisy_data.txt file in step 1. Save this file as ReadFileRegex.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the noisy_data.txt file to find matches based on the begin.regex and end.regex regular expressions. Each match is converted to an event in the StockStream stream. After that, a simple transformation is carried out for the StockStream stream where value for the symbol attribute converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileRegex deployed successfully Now the Siddhi application starts to process the noisy_data.txt file. As a result, the following log appears in the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906475623, data=[WSO2, 75.0, 100], isExpired=false} Note that noisy_data.txt file is not present in the file.uri location. Next, let's create a new noisy_data.txt file in the file.uri location that includes the latest set of productions. Download noisy_data.txt file from here and save it in the file.uri location. Now the Siddhi application starts to process the new content in the noisy_data.txt file. The file has the following content. Oracle Corporation <orcl 95 volume 200> 500 Oracle Parkway. Redwood Shores CA, 94065. Corporate Phone: 650.506.7000. HQ-Security: 650.506.5555 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906713176, data=[ORCL, 95.0, 200], isExpired=false} Extracting data from a folder \u00b6 Processing all files in the folder \u00b6 In this scenario, you extract data from a specific folder. All of the files are processed sequentially, where each file generates a single event. Download productions.zip file from here and extract it. Now you have a folder named productions . Place it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ProcessFolder') @App:description('Process all files in the folder and delete files after processing.') @source(type='file', mode='text.full', dir.uri='file:/Users/foo/productions', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value for the dir.uri parameter so that it points to the productions folder you created in step 1. Save this file as ProcessFolder.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application processes each file in productions folder. Each file generates an event in the StockStream stream. After that, a simple transformation is carried out for the StockStream stream where the value for the symbol attribute is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ProcessFolder deployed successfully Now the Siddhi application starts to process each file in the productions directory. As a result, the following logs appear in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[WSO2, 75.0, 100], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[ORCL, 95.0, 200], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[IBM, 88.0, 150], isExpired=false} Info In this scenario, you deleted each file in the folder after processing. You can choose to move the files instead of deleting them. To do this, set the action.after.process parameter to MOVE and specify the directory to which the files should be moved via the move.after.process parameter. For more information about these parameters, see Siddhi File Source documentation . Loading data into a file \u00b6 In this section of the tutorial, you are exploring the different ways in which you could load data into a file. Appending or over-writing events to a file \u00b6 In this scenario, you are appending a stream of events to the end of a file. Open a text file and copy-paste following Siddhi application to it. @App:name('AppendToFile') @App:description('Append incoming events in to a file.') @Source(type = 'http', receiver.url='http://localhost:8006/SweetProductionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='json'), file.uri='/Users/foo/low_productions.txt') define stream LowProductionStream (name string, amount double); -- Query to filter productions which have amount < 500.0 @info(name='query1') from SweetProductionStream[amount < 500.0] select * insert into LowProductionStream; Create an empty file and specify the location of the file as the value for the file.uri parameter. If this file does not exist, it is created at runtime. Save this file as AppendToFile.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application filters incoming SweetProductionStream events, selects the production runs of which the value for the amount attribute is less than 500.0 , and inserts the results into the LowProductionStream . Finally, all the events in the LowProductionStream events are appended to the file specified via the file.uri parameter in the Siddhi application. Once the Siddhi application is successfully deployed, the following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App AppendToFile deployed successfully To insert a few events into SweetProductionStream , let's issue the following CURL commands: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" Now open the file that you specified via the file.uri parameter. Note that the file has following content. {\"event\":{\"name\":\"Almond cookie\",\"amount\":100.0}} {\"event\":{\"name\":\"Baked alaska\",\"amount\":20.0}} {\"event\":{\"name\":\"Cup cake\",\"amount\":300.0}} Info Instead of appending each event to the end of the file, you can configure your Siddhi application to over-write the file. To do this, set the append='false' configuration in the Siddhi application as shown in the sample file sink configuration below. @sink(type='file', append='false', @map(type='json'), file.uri='/Users/foo/low_productions.txt') define stream LowProductionAlertStream (name string, amount double); For other configuration options, see Siddhi File Sink documentation . Preserving the state of the application through a system failure \u00b6 Let's try out a scenario where you deploy a Siddhi application to count the total number of production runs of a sweet factory. The production data is updated in a file and therefore you have to keep tailing this file, in order to get updates on the productions. Info In this scenario, the Streaming Integrator server needs to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. To enable the state persistence debug logs, open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Then add following <Logger> element below that line. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('CountProductions') @App:description('Siddhi application to count the total number of orders.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='true', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); -- Following query counts the number of sweet productions. @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Change the file.uri parameter in the above Siddhi application to the file path to which you downloaded the productions.csv file in step 4. Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the file productions.csv line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs. This transformation involves converting the value for the name attribute to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now the Siddhi application starts to process the productions.csv file. The file two entries as follows. Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet productions is being printed as 120 in the second log. This is because the factory has so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear in the SI console. DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. The Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute. Next, let's append two sweet production entries into the productions.csv file and shutdown the SI server before the state persistence happens (i.e., before the above log appears). Tip It is better to start appending the records immediately after the state persistence log appears so that you have plenty of time to append the records and shutdown the server before next log appears. Now append following content into the productions.csv file: Croissant,100.0 Croutons,100.0 Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore, the Streaming Integrator server cannot persist the latest count (which includes the last two production runs that produced 100 Croissants and 100 Croutons). The good news is, the File source source replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846807, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846812, data=[320.0], isExpired=false} Note that the File source has replayed the last two messages. This indicates that the sweet productions count has been correctly restored.","title":"Performing Real-time ETL with Files"},{"location":"examples/performing-real-time-etl-with-files/#performing-real-time-etl-with-files","text":"","title":"Performing Real-time ETL with Files"},{"location":"examples/performing-real-time-etl-with-files/#introduction","text":"The Streaming Integrator (SI) allows you to perform real-time ETL with data that is stored in files. This tutorial takes you through the different modes and options you could use, in order to perform real-time ETL with files using the SI. Before you begin: Start the SI server by navigating to the <SI_HOME>/bin directory and issuing one of the following commands: - For Windows: streaming-integrator.bat - For Linux: sh server.sh The following log appears in the Streaming Integrator console once the server is successfully started. INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec","title":"Introduction"},{"location":"examples/performing-real-time-etl-with-files/#tutorial-steps","text":"","title":"Tutorial steps"},{"location":"examples/performing-real-time-etl-with-files/#extracting-data-from-a-file","text":"In this section of the tutorial, you are exploring the different ways in which you could extract data from a specific file.","title":"Extracting data from a file"},{"location":"examples/performing-real-time-etl-with-files/#tailing-a-text-file-line-by-line","text":"In this scenario, you are tailing a text file, line by line, in order to extract data from it. Each line is extracted as an event that undergoes a simple transformation thereafter. Let's write a simple Siddhi application to do this. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('TailFileLineByLine') @App:description('Tails a file line by line and does a simple transformation.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='true', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the file path to which you downloaded productions.csv file in step 1. Save this file as TailFileLineByLine.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the file productions.csv line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs. The transformation involves converting the value for the name attribute to upper case. Finally, the output is logged in the Streaming Integrator console. Upon successful deployment, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileLineByLine deployed successfully Now the Siddhi application starts to process the productions.csv file. The file contains the following entries. Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830652, data=[ALMOND COOKIE, 100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830657, data=[BAKED ALASKA, 20.0], isExpired=false} Now append the following line to productions.csv file and save the file. Cup cake,300.0 The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490869579, data=[CUP CAKE, 300.0], isExpired=false}","title":"Tailing a text file line by line"},{"location":"examples/performing-real-time-etl-with-files/#tailing-a-text-file-using-a-regular-expression","text":"In this scenario, you are using a regular expression to extract data from the file. After data is extracted, a simple transformation is performed on them. Finally, the transformed event is logged in the SI console. Let's write a simple Siddhi application to do this. Download noisy_data.txt file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('TailFileRegex') @App:description('Tails a file using a regex and does a simple transformation.') @source(type='file', mode='REGEX', file.uri='file:/Users/foo/noisy_data.txt', begin.regex='\\<', end.regex='\\>', tailing='true', @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream[NOT(symbol is null)] select str:upper(symbol) as symbol, price, volume insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the file path to which you downloaded noisy_data.txt file in step 1. Save this file as TailFileRegex.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the noisy_data.txt file to find matches according to the regular expressions given: begin.regex and end.regex . Each match is converted to an event in the StockStream stream. After that, a simple transformation is carried out on the StockStream stream where the value for the symbol attribute from the event is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileRegex deployed successfully Now the Siddhi application starts to process the noisy_data.txt file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307974, data=[WSO2, 75.0, 100], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307975, data=[ORCL, 95.0, 200], isExpired=false} Now append the following text to noisy_data.txt file and save the file. IBM <ibm 88 volume 150> 1 New Orchard Rd Armonk, NY 10504 Phone Number: (914) 499-1900 Fax Number: (914) 765-6021 The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564588585214, data=[IBM, 88.0, 150], isExpired=false}","title":"Tailing a text file using a regular expression"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-remote-text-file-and-moving-it-after-processing","text":"In the previous scenarios, you tailed a file and each file generated multiple events. In this scenario, you are reading the complete file to build a single event. Furthermore, to try out the capability of processing remote files, you are processing a remote file instead of a file located in the local file system. Download portfolio.txt file from here and upload it into an FTP server. Create a directory on the FTP server. The portfolio.txt file is moved to this folder after the processing is complete. Open a text file and copy-paste following Siddhi application to it. @App:name('TextFullFileProcessing') @App:description('Reads a text file and moves it after processing.') @source(type='file', mode='TEXT.FULL', file.uri=\"ftp://<username>:<password>@<ftp_hostname>:<ftp_port>/Users/foo/portfolio.txt\", action.after.process='MOVE', move.after.process=\"ftp://<username>:<password>@<ftp_hostname>:<ftp_port>/Users/foo/move.after.process\", @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; Change the value of the file.uri parameter in the above Siddhi application to the remote file path to which you uploaded the portfolio.txt file in step 1. In addition to that, change move.after.process so that it points to the remote folder you created in step 2. When configuring both of the above parameters, change the values for <username> , <password> , <ftp_hostname> , and <ftp_port> parameters accordingly. Save this file as TextFullFileProcessing.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application reads the complete portfolio.txt remote file to create a StockStream event. After that, a simple transformation is carried out on the StockStream stream where the value for the symbol attribute in each event is converted ito upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TextFullFileProcessing deployed successfully Now the Siddhi application starts to process the portfolio.txt file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - TextFullFileProcessing : LogStream : Event{timestamp=1564660443519, data=[WSO2, 55.6, 100], isExpired=false} Info In this scenario, you moved the file after processing. To delete a file after processing, remove the action.after.process and move.after.process parameters from the Siddhi application. For other configuration options, see Siddhi File Source documentation .","title":"Reading a remote text file and moving it after processing"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-binary-file-and-moving-it-after-processing","text":"In the previous scenarios, you processed text files in order to extract data. In this scenario, you are reading a binary file. The content of the file generates a single event. Download wso2.bin file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('BinaryFullFileProcessing') @App:description('Reads a binary file and moves it after processing.') @source(type='file', mode='TEXT.FULL', file.uri='file:/Users/foo/wso2.bin', action.after.process='MOVE', move.after.process='file:/Users/foo/move.after.process', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value for the file.uri parameter to the file path to which you downloaded the wso2.bin file in step 1. Save this file as BinaryFullFileProcessing.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application reads the file wso2.bin fully to create a StockStream event. After that, a simple transformation is carried out for the StockStream stream where the value for the symbol attribute is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App BinaryFullFileProcessing deployed successfully Now the Siddhi application starts to process the wso2.bin file. As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - BinaryFullFileProcessing : LogStream : Event{timestamp=1564660553623, data=[WSO2, 55.6, 100], isExpired=false}","title":"Reading a binary file and moving it after processing"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-file-line-by-line-and-delete-it-after-processing","text":"In this scenario, you are reading a text file completely, and then deleting it after processing. In other words, the file is not tailed. You read the file line by line where each line generates an event. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ReadFileLineByLine') @App:description('Reads a file line by line and does a simple transformation.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='false', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into LogStream; In the above Siddhi application, change the value for the file.uri parameter to the file path to which you downloaded the productions.csv file in step 1. Save this file as ReadFileLineByLine.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the productions.csv file line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs where the value for the name attribute from the event is converted into upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileLineByLine deployed successfully Now the Siddhi application starts to process the productions.csv file. The file has below two entries: Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[ALMOND COOKIE, 100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[BAKED ALASKA, 20.0], isExpired=false} Note that productions.csv file is not present in the file.uri location. Next, create a new productions.csv file in the file.uri location that includes the latest set of productions. Download productions.csv file from here and save it in the file.uri location. Now the Siddhi application starts to process the new set of production runs in the productions.csv file. The file has the following two entries. Cup cake,300.0 Doughnut,500.0 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[CUP CAKE, 300.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[DOUGHNUT, 500.0], isExpired=false}","title":"Reading a file line by line and delete it after processing"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-file-using-a-regular-expression-and-deleting-it-after-processing","text":"In this scenario, you are using a regular expression to extract data from the content of the file. Here, you do not tail the file. Instead, you read the full content of the file and generate a single event. After this is done, the file is deleted. To generate an event stream, you can keep re-creating the file with new data. Download noisy_data.txt file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ReadFileRegex') @App:description('Reads a file using a regex and does a simple transformation.') @source(type='file', mode='REGEX', file.uri='file:/Users/foo/noisy_data.txt', begin.regex='\\<', end.regex='\\>', tailing='false', @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B'))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream[NOT(symbol is null)] select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value of the file.uri parameter to the file path to which you downloaded the noisy_data.txt file in step 1. Save this file as ReadFileRegex.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the noisy_data.txt file to find matches based on the begin.regex and end.regex regular expressions. Each match is converted to an event in the StockStream stream. After that, a simple transformation is carried out for the StockStream stream where value for the symbol attribute converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileRegex deployed successfully Now the Siddhi application starts to process the noisy_data.txt file. As a result, the following log appears in the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906475623, data=[WSO2, 75.0, 100], isExpired=false} Note that noisy_data.txt file is not present in the file.uri location. Next, let's create a new noisy_data.txt file in the file.uri location that includes the latest set of productions. Download noisy_data.txt file from here and save it in the file.uri location. Now the Siddhi application starts to process the new content in the noisy_data.txt file. The file has the following content. Oracle Corporation <orcl 95 volume 200> 500 Oracle Parkway. Redwood Shores CA, 94065. Corporate Phone: 650.506.7000. HQ-Security: 650.506.5555 As a result, the following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906713176, data=[ORCL, 95.0, 200], isExpired=false}","title":"Reading a file using a regular expression and deleting it after processing"},{"location":"examples/performing-real-time-etl-with-files/#extracting-data-from-a-folder","text":"","title":"Extracting data from a folder"},{"location":"examples/performing-real-time-etl-with-files/#processing-all-files-in-the-folder","text":"In this scenario, you extract data from a specific folder. All of the files are processed sequentially, where each file generates a single event. Download productions.zip file from here and extract it. Now you have a folder named productions . Place it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('ProcessFolder') @App:description('Process all files in the folder and delete files after processing.') @source(type='file', mode='text.full', dir.uri='file:/Users/foo/productions', @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\"))) define stream StockStream (symbol string, price float, volume long); @sink(type = 'log') define stream LogStream (symbol string, price float, volume long); from StockStream select str:upper(symbol) as symbol, price, volume insert into LogStream; In the above Siddhi application, change the value for the dir.uri parameter so that it points to the productions folder you created in step 1. Save this file as ProcessFolder.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application processes each file in productions folder. Each file generates an event in the StockStream stream. After that, a simple transformation is carried out for the StockStream stream where the value for the symbol attribute is converted to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ProcessFolder deployed successfully Now the Siddhi application starts to process each file in the productions directory. As a result, the following logs appear in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[WSO2, 75.0, 100], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[ORCL, 95.0, 200], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[IBM, 88.0, 150], isExpired=false} Info In this scenario, you deleted each file in the folder after processing. You can choose to move the files instead of deleting them. To do this, set the action.after.process parameter to MOVE and specify the directory to which the files should be moved via the move.after.process parameter. For more information about these parameters, see Siddhi File Source documentation .","title":"Processing all files in the folder"},{"location":"examples/performing-real-time-etl-with-files/#loading-data-into-a-file","text":"In this section of the tutorial, you are exploring the different ways in which you could load data into a file.","title":"Loading data into a file"},{"location":"examples/performing-real-time-etl-with-files/#appending-or-over-writing-events-to-a-file","text":"In this scenario, you are appending a stream of events to the end of a file. Open a text file and copy-paste following Siddhi application to it. @App:name('AppendToFile') @App:description('Append incoming events in to a file.') @Source(type = 'http', receiver.url='http://localhost:8006/SweetProductionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='json'), file.uri='/Users/foo/low_productions.txt') define stream LowProductionStream (name string, amount double); -- Query to filter productions which have amount < 500.0 @info(name='query1') from SweetProductionStream[amount < 500.0] select * insert into LowProductionStream; Create an empty file and specify the location of the file as the value for the file.uri parameter. If this file does not exist, it is created at runtime. Save this file as AppendToFile.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application filters incoming SweetProductionStream events, selects the production runs of which the value for the amount attribute is less than 500.0 , and inserts the results into the LowProductionStream . Finally, all the events in the LowProductionStream events are appended to the file specified via the file.uri parameter in the Siddhi application. Once the Siddhi application is successfully deployed, the following log appears in the SI console: INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App AppendToFile deployed successfully To insert a few events into SweetProductionStream , let's issue the following CURL commands: curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\" http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\" Now open the file that you specified via the file.uri parameter. Note that the file has following content. {\"event\":{\"name\":\"Almond cookie\",\"amount\":100.0}} {\"event\":{\"name\":\"Baked alaska\",\"amount\":20.0}} {\"event\":{\"name\":\"Cup cake\",\"amount\":300.0}} Info Instead of appending each event to the end of the file, you can configure your Siddhi application to over-write the file. To do this, set the append='false' configuration in the Siddhi application as shown in the sample file sink configuration below. @sink(type='file', append='false', @map(type='json'), file.uri='/Users/foo/low_productions.txt') define stream LowProductionAlertStream (name string, amount double); For other configuration options, see Siddhi File Sink documentation .","title":"Appending or over-writing events to a file"},{"location":"examples/performing-real-time-etl-with-files/#preserving-the-state-of-the-application-through-a-system-failure","text":"Let's try out a scenario where you deploy a Siddhi application to count the total number of production runs of a sweet factory. The production data is updated in a file and therefore you have to keep tailing this file, in order to get updates on the productions. Info In this scenario, the Streaming Integrator server needs to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. To enable the state persistence debug logs, open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Then add following <Logger> element below that line. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Download productions.csv file from here and save it in a location of your choice. Open a text file and copy-paste following Siddhi application to it. @App:name('CountProductions') @App:description('Siddhi application to count the total number of orders.') @source(type='file', mode='LINE', file.uri='file:/Users/foo/productions.csv', tailing='true', @map(type='csv')) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); -- Following query counts the number of sweet productions. @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Change the file.uri parameter in the above Siddhi application to the file path to which you downloaded the productions.csv file in step 4. Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application tails the file productions.csv line by line. Each line is converted to an event in the SweetProductionStream stream. After that, a simple transformation is carried out for the sweet production runs. This transformation involves converting the value for the name attribute to upper case. Finally, the output is logged in the SI console. Once the Siddhi application is successfully deployed, the following log appears in the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now the Siddhi application starts to process the productions.csv file. The file two entries as follows. Almond cookie,100.0 Baked alaska,20.0 As a result, the following log appears in the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet productions is being printed as 120 in the second log. This is because the factory has so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear in the SI console. DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. The Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute. Next, let's append two sweet production entries into the productions.csv file and shutdown the SI server before the state persistence happens (i.e., before the above log appears). Tip It is better to start appending the records immediately after the state persistence log appears so that you have plenty of time to append the records and shutdown the server before next log appears. Now append following content into the productions.csv file: Croissant,100.0 Croutons,100.0 Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore, the Streaming Integrator server cannot persist the latest count (which includes the last two production runs that produced 100 Croissants and 100 Croutons). The good news is, the File source source replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846807, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846812, data=[320.0], isExpired=false} Note that the File source has replayed the last two messages. This indicates that the sweet productions count has been correctly restored.","title":"Preserving the state of the application through a system failure"},{"location":"examples/performing-real-time-etl-with-mysql/","text":"Performing Real-time ETL with MySQL \u00b6 Introduction \u00b6 The Streaming Integrator (SI) allows you to capture changes to a database table, in a streaming manner, enabling you to perform ETL operations. This tutorial takes you through the different modes and options you could use to perform Change Data Capturing (CDC) using the SI. In this tutorial, you are using a MySQL datasource. Info To use a different database other than MySQL, see dependencies for CDC and add the corresponding driver jar. In addition to that, modify the JDBC URL accordingly, in url parameter in all Siddhi applications given in this tutorial. Listening mode and Polling mode There are two modes in which you could perform CDC using the SI: Listening mode and Polling mode . Polling mode: In the polling mode, the data source is periodically polled for capturing the changes. The polling period can be configured. Listening mode: In listening mode, the SI keeps listening to the Change Log of the database and notifies if a change takes place. Here, unlike the polling mode, you are notified about the change immediately. Type of events captured You can capture following type of changes done to a database table: Insert operations Update operations Delete operations (available for Listening mode only) Tutorial steps \u00b6 Listening mode \u00b6 Before you begin: You need to have access to a MySQL instance. Enable binary logging in the MySQL server. For detailed instructions, see Enabling the Binlog tutorial by debezium . Add the MySQL JDBC driver into the <SI_HOME>/lib directoryas follows: Download the MySQL JDBC driver from the MySQL site . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. Start the SI server. Once you install MySQL and start the MySQL server, create the database and the database table you require as follows: Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Capturing inserts \u00b6 Now you can write a simple Siddhi application to monitor the SweetProductionTable for insert operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForInserts') @App:description('Capture MySQL Inserts using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream InsertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); @info(name = 'query') from InsertSweetProductionStream select * insert into LogStream; Here the url parameter has the value jdbc:mysql://localhost:3306/production . Change it to point to your MySQL server. Save this file as CDCListenForInserts.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the inserts made to the SweetProductionTable database table and logs them. Now let's perform an insert operation on the MySQL table by executing the following MySQL query on the database: insert into SweetProductionTable values('chocolate',100.0); The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : logStream : Event{timestamp=1563200225948, data=[chocolate, 100.0], isExpired=false} Capturing updates \u00b6 Now you can write a Siddhi application to monitor the SweetProductionTable for update operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForUpdates') @App:description('Capture MySQL Updates using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'update', @map(type = 'keyvalue')) define stream UpdateSweetProductionStream (before_name string, name string, before_amount double, amount double); @sink(type = 'log') define stream LogStream (before_name string, name string, before_amount double, amount double); @info(name = 'query') from UpdateSweetProductionStream select * insert into LogStream; Save this file as CDCListenForUpdates.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the updates to the SweetProductionTable database table and logs them. Now let's perform an update operation on the MySQL table. For this, execute following MySQL query on the database: update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate'; As a result, you can see the following log in the SI console. INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : updateSweetProductionStream : Event{timestamp=1563201040953, data=[chocolate, Almond cookie, 100.0, 100.0], isExpired=false} Info Here, the before_name1 attribute indicates the value of the name attribute before the update was made ( chocolate in this case), and the name attribute has the current name after the update (i.e., almond cookie ). Capturing deletes \u00b6 Now you can write a Siddhi application to monitor the SweetProductionTable for delete operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForDeletes') @App:description('Capture MySQL Deletes using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'delete', @map(type = 'keyvalue')) define stream DeleteSweetProductionStream (before_name string, before_amount double); @sink(type = 'log') define stream LogStream (before_name string, before_amount double); @info(name = 'query') from DeleteSweetProductionStream select * insert into LogStream; Save this file as CDCListenForDeletes.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the delete operations carried out for the SweetProductionTable database table and logs them. Now let's perform a delete operation for the MySQL table. To do this, execute following MySQL query on the database: delete from SweetProductionTable where name = 'Almond cookie'; The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : DeleteSweetProductionStream : Event{timestamp=1563367367098, data=[Almond cookie, 100.0], isExpired=false} Info Here, the before_name attribute indicates the name of the sweet in the deleted record (i.e., Almond cookie in this case). Similarly, the before_amount indicates the amount in the deleted record. Preserving State of the application through a system failure \u00b6 Let's try out a scenario in which you are going to deploy a Siddhi application to count the total number of productions. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate the following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Open a text file and copy-paste following Siddhi application to it. @App:name('CountProductions') @App:description('Siddhi application to count the total number of orders.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream InsertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from InsertSweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database: insert into SweetProductionTable values('Almond cookie',100.0); insert into SweetProductionTable values('Baked alaska',20.0); Now you can see following logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151034866, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151037870, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet productions is being printed as 120 in the second log. This is because the factory has so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear on the SI console DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute. Next, let's insert two sweet productions into the SweetProductionTable and shutdown the SI server before the state persistence happens (in other words, before the above log appears). Tip It is better to start inserting records immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears. Now insert following sweets into the SweetProductionTable by executing following queries on the database : insert into SweetProductionTable values('Croissant',100.0); insert into SweetProductionTable values('Croutons',100.0); Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions 100 Croissants and 100 Croutons). The good news is, CDC source replays the last two messages, allowing the Streaming Integrator to recover successfully from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078607, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078612, data=[320.0], isExpired=false} Note that the CDC source has replayed the last two messages. As a result, the sweet production runs count has being correctly restored. Polling mode \u00b6 Before you begin: You are required to have access to a MySQL instance. Create the required database and the database table in the MySQL instance as follows: 1. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, issue the following command. CREATE SCHEMA production_pol; 2. Switch to the production database and create a new table by executing following queries. use production_pol; CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2)); 3. If you have not already created a user under Listening Mode , create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; 4. If you have not already added the MySQL JDBC driver into <SI_HOME>/lib under Listening Mode , add it as follows: a. Download the MySQL JDBC driver from the MySQL site . b. Unzip the archive. c. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. Capturing inserts \u00b6 Now you can write a simple Siddhi application to monitor the SweetProductionTable table for insert operations. Open a text file and copy-paste the following application into it. @App:name(\"CDCPolling\") @App:description(\"Capture MySQL changes, using CDC source - polling mode.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '10', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); @info(name = 'query') from SweetProductionStream select * insert into LogStream; Here the url parameter currently specifies the URL jdbc:mysql://localhost:3306/production_pol . Change it to point to your MySQL server. Save this file as CDCPolling.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application polls the database periodically, captures the changes made to the SweetProductionTable database table during the polled interval and logs them. The polling interval is specified via the polling.interval parameter in the Siddhi application when defining the CDC source. In this example, the polling interval is 10 seconds. Now let's perform an insert operation on the MySQL table. To do this, execute following MySQL query on the database. insert into SweetProductionTable(name,amount) values('chocolate',100.0); The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : LogStream : Event{timestamp=1563378804914, data=[chocolate, 100.0], isExpired=false} Capturing Updates \u00b6 For capturing updates, you can use the same CDCPolling.siddhi Siddhi application that you deployed in the Capturing inserts section. Let's perform an update operation on the MySQL table. To do this, execute the following MySQL query on the database: update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate'; The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : logStream : Event{timestamp=1563436388530, data=[Almond cookie, 100.0], isExpired=false} Preserving State of the application through a system failure \u00b6 Let's try out a scenario in which you deploy a Siddhi application to count the total number of production runs. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate the following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Open a text file and copy-paste following Siddhi application to it. @App:name(\"CountProductions_pol\") @App:description(\"Siddhi application to count the total number of orders.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '10', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions_pol.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions_pol deployed successfully Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database: insert into SweetProductionTable(name,amount) values('Almond cookie',100.0); insert into SweetProductionTable(name,amount) values('Baked alaska',20.0); Now you can see following logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564385971323, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386011344, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet production runs is being printed as 120 in the second log. This is because we have so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear on the SI console. DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions_pol persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute, therefore you can see this log appearing every minute. Next, you are going to insert two sweet production runs into the SweetProductionTable and shutdown the SI server before state persistence happens (in other words, before above log appears). Tip It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears. Now insert following sweets into the SweetProductionTable by executing following queries on the database : insert into SweetProductionTable(name,amount) values('Croissant',100.0); insert into SweetProductionTable(name,amount) values('Croutons',100.0); Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore, the SI server cannot persist the latest count (which should include the last two production runs 100 Croissants and 100 Croutons). The good news is, the CDC source replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386179998, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386180004, data=[320.0], isExpired=false} Note that the CDC source has replayed the last two messages. This indicates that the sweet production run count is correctly restored.","title":"Performing Real-time ETL with MySQL"},{"location":"examples/performing-real-time-etl-with-mysql/#performing-real-time-etl-with-mysql","text":"","title":"Performing Real-time ETL with MySQL"},{"location":"examples/performing-real-time-etl-with-mysql/#introduction","text":"The Streaming Integrator (SI) allows you to capture changes to a database table, in a streaming manner, enabling you to perform ETL operations. This tutorial takes you through the different modes and options you could use to perform Change Data Capturing (CDC) using the SI. In this tutorial, you are using a MySQL datasource. Info To use a different database other than MySQL, see dependencies for CDC and add the corresponding driver jar. In addition to that, modify the JDBC URL accordingly, in url parameter in all Siddhi applications given in this tutorial. Listening mode and Polling mode There are two modes in which you could perform CDC using the SI: Listening mode and Polling mode . Polling mode: In the polling mode, the data source is periodically polled for capturing the changes. The polling period can be configured. Listening mode: In listening mode, the SI keeps listening to the Change Log of the database and notifies if a change takes place. Here, unlike the polling mode, you are notified about the change immediately. Type of events captured You can capture following type of changes done to a database table: Insert operations Update operations Delete operations (available for Listening mode only)","title":"Introduction"},{"location":"examples/performing-real-time-etl-with-mysql/#tutorial-steps","text":"","title":"Tutorial steps"},{"location":"examples/performing-real-time-etl-with-mysql/#listening-mode","text":"Before you begin: You need to have access to a MySQL instance. Enable binary logging in the MySQL server. For detailed instructions, see Enabling the Binlog tutorial by debezium . Add the MySQL JDBC driver into the <SI_HOME>/lib directoryas follows: Download the MySQL JDBC driver from the MySQL site . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory. Start the SI server. Once you install MySQL and start the MySQL server, create the database and the database table you require as follows: Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. CREATE SCHEMA production; Create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; Switch to the production database and create a new table, by executing the following queries: use production; CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));","title":"Listening mode"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-inserts","text":"Now you can write a simple Siddhi application to monitor the SweetProductionTable for insert operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForInserts') @App:description('Capture MySQL Inserts using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream InsertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); @info(name = 'query') from InsertSweetProductionStream select * insert into LogStream; Here the url parameter has the value jdbc:mysql://localhost:3306/production . Change it to point to your MySQL server. Save this file as CDCListenForInserts.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the inserts made to the SweetProductionTable database table and logs them. Now let's perform an insert operation on the MySQL table by executing the following MySQL query on the database: insert into SweetProductionTable values('chocolate',100.0); The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : logStream : Event{timestamp=1563200225948, data=[chocolate, 100.0], isExpired=false}","title":"Capturing inserts"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-updates","text":"Now you can write a Siddhi application to monitor the SweetProductionTable for update operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForUpdates') @App:description('Capture MySQL Updates using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'update', @map(type = 'keyvalue')) define stream UpdateSweetProductionStream (before_name string, name string, before_amount double, amount double); @sink(type = 'log') define stream LogStream (before_name string, name string, before_amount double, amount double); @info(name = 'query') from UpdateSweetProductionStream select * insert into LogStream; Save this file as CDCListenForUpdates.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the updates to the SweetProductionTable database table and logs them. Now let's perform an update operation on the MySQL table. For this, execute following MySQL query on the database: update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate'; As a result, you can see the following log in the SI console. INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : updateSweetProductionStream : Event{timestamp=1563201040953, data=[chocolate, Almond cookie, 100.0, 100.0], isExpired=false} Info Here, the before_name1 attribute indicates the value of the name attribute before the update was made ( chocolate in this case), and the name attribute has the current name after the update (i.e., almond cookie ).","title":"Capturing updates"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-deletes","text":"Now you can write a Siddhi application to monitor the SweetProductionTable for delete operations. Open a text file and copy-paste following application into it. @App:name('CDCListenForDeletes') @App:description('Capture MySQL Deletes using CDC listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'delete', @map(type = 'keyvalue')) define stream DeleteSweetProductionStream (before_name string, before_amount double); @sink(type = 'log') define stream LogStream (before_name string, before_amount double); @info(name = 'query') from DeleteSweetProductionStream select * insert into LogStream; Save this file as CDCListenForDeletes.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application captures all the delete operations carried out for the SweetProductionTable database table and logs them. Now let's perform a delete operation for the MySQL table. To do this, execute following MySQL query on the database: delete from SweetProductionTable where name = 'Almond cookie'; The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : DeleteSweetProductionStream : Event{timestamp=1563367367098, data=[Almond cookie, 100.0], isExpired=false} Info Here, the before_name attribute indicates the name of the sweet in the deleted record (i.e., Almond cookie in this case). Similarly, the before_amount indicates the amount in the deleted record.","title":"Capturing deletes"},{"location":"examples/performing-real-time-etl-with-mysql/#preserving-state-of-the-application-through-a-system-failure","text":"Let's try out a scenario in which you are going to deploy a Siddhi application to count the total number of productions. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate the following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Open a text file and copy-paste following Siddhi application to it. @App:name('CountProductions') @App:description('Siddhi application to count the total number of orders.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream InsertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from InsertSweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database: insert into SweetProductionTable values('Almond cookie',100.0); insert into SweetProductionTable values('Baked alaska',20.0); Now you can see following logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151034866, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151037870, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet productions is being printed as 120 in the second log. This is because the factory has so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear on the SI console DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute. Next, let's insert two sweet productions into the SweetProductionTable and shutdown the SI server before the state persistence happens (in other words, before the above log appears). Tip It is better to start inserting records immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears. Now insert following sweets into the SweetProductionTable by executing following queries on the database : insert into SweetProductionTable values('Croissant',100.0); insert into SweetProductionTable values('Croutons',100.0); Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions 100 Croissants and 100 Croutons). The good news is, CDC source replays the last two messages, allowing the Streaming Integrator to recover successfully from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078607, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078612, data=[320.0], isExpired=false} Note that the CDC source has replayed the last two messages. As a result, the sweet production runs count has being correctly restored.","title":"Preserving State of the application through a system failure"},{"location":"examples/performing-real-time-etl-with-mysql/#polling-mode","text":"Before you begin: You are required to have access to a MySQL instance. Create the required database and the database table in the MySQL instance as follows: 1. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, issue the following command. CREATE SCHEMA production_pol; 2. Switch to the production database and create a new table by executing following queries. use production_pol; CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2)); 3. If you have not already created a user under Listening Mode , create a new user by executing the following SQL query. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2'; 4. If you have not already added the MySQL JDBC driver into <SI_HOME>/lib under Listening Mode , add it as follows: a. Download the MySQL JDBC driver from the MySQL site . b. Unzip the archive. c. Copy the mysql-connector-java-5.1.45-bin.jar to the <SI_HOME>/lib directory.","title":"Polling mode"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-inserts_1","text":"Now you can write a simple Siddhi application to monitor the SweetProductionTable table for insert operations. Open a text file and copy-paste the following application into it. @App:name(\"CDCPolling\") @App:description(\"Capture MySQL changes, using CDC source - polling mode.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '10', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (name string, amount double); @info(name = 'query') from SweetProductionStream select * insert into LogStream; Here the url parameter currently specifies the URL jdbc:mysql://localhost:3306/production_pol . Change it to point to your MySQL server. Save this file as CDCPolling.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info This Siddhi application polls the database periodically, captures the changes made to the SweetProductionTable database table during the polled interval and logs them. The polling interval is specified via the polling.interval parameter in the Siddhi application when defining the CDC source. In this example, the polling interval is 10 seconds. Now let's perform an insert operation on the MySQL table. To do this, execute following MySQL query on the database. insert into SweetProductionTable(name,amount) values('chocolate',100.0); The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : LogStream : Event{timestamp=1563378804914, data=[chocolate, 100.0], isExpired=false}","title":"Capturing inserts"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-updates_1","text":"For capturing updates, you can use the same CDCPolling.siddhi Siddhi application that you deployed in the Capturing inserts section. Let's perform an update operation on the MySQL table. To do this, execute the following MySQL query on the database: update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate'; The following log appears in the SI console: INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : logStream : Event{timestamp=1563436388530, data=[Almond cookie, 100.0], isExpired=false}","title":"Capturing Updates"},{"location":"examples/performing-real-time-etl-with-mysql/#preserving-state-of-the-application-through-a-system-failure_1","text":"Let's try out a scenario in which you deploy a Siddhi application to count the total number of production runs. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate the following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Open a text file and copy-paste following Siddhi application to it. @App:name(\"CountProductions_pol\") @App:description(\"Siddhi application to count the total number of orders.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '10', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream SweetProductionStream (name string, amount double); @sink(type = 'log') define stream LogStream (totalProductions double); @info(name = 'query') from SweetProductionStream select sum(amount) as totalProductions insert into LogStream; Save this file as CountProductions_pol.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions_pol deployed successfully Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database: insert into SweetProductionTable(name,amount) values('Almond cookie',100.0); insert into SweetProductionTable(name,amount) values('Baked alaska',20.0); Now you can see following logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564385971323, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386011344, data=[120.0], isExpired=false} These logs print the sweet production count. Note that the current count of sweet production runs is being printed as 120 in the second log. This is because we have so far produced 120 sweets: 100 Almond cookies and 20 Baked alaskas. Now wait for following log to appear on the SI console. DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions_pol persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute, therefore you can see this log appearing every minute. Next, you are going to insert two sweet production runs into the SweetProductionTable and shutdown the SI server before state persistence happens (in other words, before above log appears). Tip It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears. Now insert following sweets into the SweetProductionTable by executing following queries on the database : insert into SweetProductionTable(name,amount) values('Croissant',100.0); insert into SweetProductionTable(name,amount) values('Croutons',100.0); Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here, the SI server crashes before the state is persisted. Therefore, the SI server cannot persist the latest count (which should include the last two production runs 100 Croissants and 100 Croutons). The good news is, the CDC source replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute. The following log appears in the SI console: INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386179998, data=[220.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386180004, data=[320.0], isExpired=false} Note that the CDC source has replayed the last two messages. This indicates that the sweet production run count is correctly restored.","title":"Preserving State of the application through a system failure"},{"location":"examples/reliable-data-integration/","text":"","title":"Reliable data integration"},{"location":"examples/running-si-with-docker-and-kubernetes/","text":"Running the Streaming Integrator in Containerized Environments \u00b6 Running the Streaming Integrator with Docker \u00b6 This section shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment. Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here . Save the following Siddhi application as a .siddhi file in a preferred location in your machine. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, amount double); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, amount insert into TransformedProductionStream; Note the following about this Siddhi application. The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is 0.0.0.0 . The 8006 port of the receiver URL is the same HTTP port that you previously exposed via Docker. Starting the Streaming Integrator in Docker \u00b6 In this scenario, you are downloading and installing the Streaming Integrator via Docker. WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 . To run the Streaming Integrator in the open source image that is available for it To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker pull -it wso2/streaming-integrator Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports: The 9443 port where the Streaming Integrator server is run. The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages. To expose these ports, issue the following command. docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator -v <local-absolute-siddhi-file-path>/MySimpleApp.siddhi:/apps/MySimpleApp.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/MySimpleApp.siddhi Info In the above command, you are mounting the location where you have saved the MySimpleApp.siddhi file so that the Streaming Integrator can locate it and run it when it starts in Docker. Therefore, replace <local-absolute-siddhi-file-path> with the path in which you saved the Siddhi application in your machine. If you did not mount the location to the MySimpleApp.siddhi file when issuing the command to start the Streaming Integrator, you can deploy the Siddhi application via the Streaming Integrator tool. Click here for detailed instructions. Start and access the Streaming Integrator Tooling. Open a new file and copy-paste the MySimpleApp.siddhi Siddhi application in the Source View. Then save the Siddhi application. To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Then click Add . Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. The following is logged in the console in which you started the Streaming Integrator in Docker. Now the Streaming Integrator has started in the Docker environment. Creating and deploying the Siddhi application \u00b6 Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console. Trying-out the Siddhi application \u00b6 To try out the MySimpleApp Siddhi application you deployed in Docker, issue the following CURL command. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\" The following output appears in the console in which you started the Streaming Integrator in Docker. Running the Streaming Integrator with Kubernetes \u00b6 In this section, you get to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes. Before you begin: Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows. Install Minikube and start a cluster by following the Minikube Documentation . Enable ingress on Minikube by issuing the following command. minikube addons enable ingress Make sure that you have admin privileges to install the Siddhi operator . Installing the Siddhi Operator for the Streaming Integrator \u00b6 To install the Siddhi Operator, follow the procedure below: To install the Siddhi Kubernetes operator for streaming integrator issue the following commands: kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml To verify whether the Siddhi operator is successfully installed, issue the following command. kubectl get deployment If the installation is successful, the following deployments should be running in the Kubernetes cluster. Deploying Siddhi applications in Kubernetes \u00b6 You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster. First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for power . @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named spec as shown below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" Here, you are specifying that Siddhi applications running within the container should receive events to the http://0.0.0.0:8080/checkPower URL and basic authentication is not enabled for them. Add a runner section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows. runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* To view the complete file, click here. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* Save the file as siddhi-process.yaml in a preferred location To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command. kubectl apply -f <PATH_to_siddhi-process.yaml> Info This file overrules the configurations in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. Invoking the Siddhi application \u00b6 To invoke the PowerSurgeDetection Siddhi application that you deployed in the Kubernetes cluster, follow the steps below. First, get the external IP of minikube by issuing the following command. minikube ip Add the IP it returns to the /etc/hosts file in your machine. Issue the following CURL command to invoke the PowerSurgeDetection Siddhi application. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command. `kubectl get pods' This returns the list of pods as shown in the example below. NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is streaming-integrator-app-0-b4dcf85-npgj7 . streaming-integrator-app-0-b4dcf85-npgj7","title":"Running SI with Docker and Kubernetes"},{"location":"examples/running-si-with-docker-and-kubernetes/#running-the-streaming-integrator-in-containerized-environments","text":"","title":"Running the Streaming Integrator in Containerized Environments"},{"location":"examples/running-si-with-docker-and-kubernetes/#running-the-streaming-integrator-with-docker","text":"This section shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment. Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here . Save the following Siddhi application as a .siddhi file in a preferred location in your machine. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, amount double); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, amount insert into TransformedProductionStream; Note the following about this Siddhi application. The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is 0.0.0.0 . The 8006 port of the receiver URL is the same HTTP port that you previously exposed via Docker.","title":"Running the Streaming Integrator with Docker"},{"location":"examples/running-si-with-docker-and-kubernetes/#starting-the-streaming-integrator-in-docker","text":"In this scenario, you are downloading and installing the Streaming Integrator via Docker. WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 . To run the Streaming Integrator in the open source image that is available for it To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker pull -it wso2/streaming-integrator Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports: The 9443 port where the Streaming Integrator server is run. The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages. To expose these ports, issue the following command. docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator -v <local-absolute-siddhi-file-path>/MySimpleApp.siddhi:/apps/MySimpleApp.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/MySimpleApp.siddhi Info In the above command, you are mounting the location where you have saved the MySimpleApp.siddhi file so that the Streaming Integrator can locate it and run it when it starts in Docker. Therefore, replace <local-absolute-siddhi-file-path> with the path in which you saved the Siddhi application in your machine. If you did not mount the location to the MySimpleApp.siddhi file when issuing the command to start the Streaming Integrator, you can deploy the Siddhi application via the Streaming Integrator tool. Click here for detailed instructions. Start and access the Streaming Integrator Tooling. Open a new file and copy-paste the MySimpleApp.siddhi Siddhi application in the Source View. Then save the Siddhi application. To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Then click Add . Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. The following is logged in the console in which you started the Streaming Integrator in Docker. Now the Streaming Integrator has started in the Docker environment.","title":"Starting the Streaming Integrator in Docker"},{"location":"examples/running-si-with-docker-and-kubernetes/#creating-and-deploying-the-siddhi-application","text":"Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console.","title":"Creating and deploying the Siddhi application"},{"location":"examples/running-si-with-docker-and-kubernetes/#trying-out-the-siddhi-application","text":"To try out the MySimpleApp Siddhi application you deployed in Docker, issue the following CURL command. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\" The following output appears in the console in which you started the Streaming Integrator in Docker.","title":"Trying-out the Siddhi application"},{"location":"examples/running-si-with-docker-and-kubernetes/#running-the-streaming-integrator-with-kubernetes","text":"In this section, you get to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes. Before you begin: Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows. Install Minikube and start a cluster by following the Minikube Documentation . Enable ingress on Minikube by issuing the following command. minikube addons enable ingress Make sure that you have admin privileges to install the Siddhi operator .","title":"Running the Streaming Integrator with Kubernetes"},{"location":"examples/running-si-with-docker-and-kubernetes/#installing-the-siddhi-operator-for-the-streaming-integrator","text":"To install the Siddhi Operator, follow the procedure below: To install the Siddhi Kubernetes operator for streaming integrator issue the following commands: kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml To verify whether the Siddhi operator is successfully installed, issue the following command. kubectl get deployment If the installation is successful, the following deployments should be running in the Kubernetes cluster.","title":"Installing the Siddhi Operator for the Streaming Integrator"},{"location":"examples/running-si-with-docker-and-kubernetes/#deploying-siddhi-applications-in-kubernetes","text":"You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster. First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for power . @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named spec as shown below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" Here, you are specifying that Siddhi applications running within the container should receive events to the http://0.0.0.0:8080/checkPower URL and basic authentication is not enabled for them. Add a runner section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows. runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* To view the complete file, click here. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* Save the file as siddhi-process.yaml in a preferred location To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command. kubectl apply -f <PATH_to_siddhi-process.yaml> Info This file overrules the configurations in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file.","title":"Deploying Siddhi applications in Kubernetes"},{"location":"examples/running-si-with-docker-and-kubernetes/#invoking-the-siddhi-application","text":"To invoke the PowerSurgeDetection Siddhi application that you deployed in the Kubernetes cluster, follow the steps below. First, get the external IP of minikube by issuing the following command. minikube ip Add the IP it returns to the /etc/hosts file in your machine. Issue the following CURL command to invoke the PowerSurgeDetection Siddhi application. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command. `kubectl get pods' This returns the list of pods as shown in the example below. NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is streaming-integrator-app-0-b4dcf85-npgj7 . streaming-integrator-app-0-b4dcf85-npgj7","title":"Invoking the Siddhi application"},{"location":"examples/summarizing-and-aggregating-data/","text":"Summarizing and Aggregating Data \u00b6 Short-term summarization \u00b6 Time-based summarization \u00b6 Length-based summarization \u00b6 Long-term summarization \u00b6","title":"Summarizing and Aggregating Data"},{"location":"examples/summarizing-and-aggregating-data/#summarizing-and-aggregating-data","text":"","title":"Summarizing and Aggregating Data"},{"location":"examples/summarizing-and-aggregating-data/#short-term-summarization","text":"","title":"Short-term summarization"},{"location":"examples/summarizing-and-aggregating-data/#time-based-summarization","text":"","title":"Time-based summarization"},{"location":"examples/summarizing-and-aggregating-data/#length-based-summarization","text":"","title":"Length-based summarization"},{"location":"examples/summarizing-and-aggregating-data/#long-term-summarization","text":"","title":"Long-term summarization"},{"location":"examples/transforming-xml-messages/","text":"","title":"Transforming xml messages"},{"location":"examples/triggering-integrations-via-micro-integrator/","text":"Triggering Integration Flows via the Micro Integrator \u00b6 Introduction \u00b6 In this tutorial, lets look at how the Streaming Integrator generates an alert based on the events received, and how that particular alert can trigger an integration flow in the Micro Integrator, and get a response back to the Streaming Integrator for further processing. To understand this consider a scenario where the Streaming Integrator receives production data from a factory, and triggers an integration flow if it detects a per minute production average that exceeds 100. Configuring the Streaming Integrator \u00b6 Let's design a Siddhi application that triggers an integration flow and deploy it by following the procedure below: Start and access the Streaming Integrator Tooling. Then click New to open a new application. Add a name and a description for your new Siddhi application as follows: @App:name(\"grpc-call-response\") @App:description(\"This application triggers integration process in the micro integrator using gRPC calls\") Let's add an input stream to define the schema of input production events, and connect a source of the http type to to receive those events. @source(type = 'http', receiver.url='http://localhost:8006/InputStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(symbol string, amount double); Here, the Streaming Integrator receives events to the http://localhost:8006/InputStream in the JSON format. Each event reports the product name (via the symbol attribute) and the amount produced. Now, let's add the configurations to publish an alert in the Micro Integrator to trigger an integration flow, and then receive a response back into the Streaming Integrator. @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json', @payload(\"\"\"{\"symbol\":\"{{symbol}}\",\"avgAmount\":{{avgAmount}}}\"\"\")) ) define stream FooStream (symbol string, avgAmount double); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (symbol string, avgAmount double); Note the following in the above configuration: Each output event that represents an alert that is published to the Micro Integrator reports the product name and the average production (as per the schema of the FooStream stream. The grpc-call sink connected to the FooStream stream gets the two attributes from the stream and generates the output events as JSON messages before they are published to the Micro Integrator. The value for the publisher.url parameter in the sink configuration contains process and inSeq which means that the Streaming Integrator calls the process method of the gRPC Listener server in the Micro Integrator, and injects the message to the inSeq which then sends a response back to the client. The grpc-call-response source connected to the BarStream input stream retrieves a response from the Micro Integrator and publishes it as a JSON message in the Streaming Integrator. As specified via the schema of the BarStream input stream, this response comprises of a single JSON message. To publish the messages received from the Micro Integrator as logs in the terminal, let's define an output stream named LogStream , and connect a sink of the log type to it as shown below. @sink(type='log', prefix='response_from_mi: ') define stream LogStream (symbol string, avgAmount double); Let's define Siddhi queries to calculate the average production per minute, filter production runs where the average production per minute is greater than 100, and direct the logs to be published to the output stream. a. To calculate the average per minute, add a Siddi query named CalculateAverageProductionPerMinute as follows: ``` @info(name = 'CalculateAverageProductionPerMinute') from InputStream#window.timeBatch(1 min) select avg(amount) as avgAmount, symbol group by symbol insert into AVGStream; ``` This query applies a time batch window to the InputStream stream so that events within each minute is considered a separate subset to be calculations in the query are applied. The minutes are considered in a tumbling manner because it is a batch window. Then the avg() function is applied to the amount attribute of the input stream to derive the average production amount. The results are then inserted into an inferred stream named AVGStream . b. To filter events from the AVGStream stream where the average production is greater then 100, add a query named FilterExcessProduction as follows. ``` @info(name = 'FilterExcessProduction') from AVGStream[avgAmount > 100] select symbol, avgAmount insert into FooStream; ``` Here, the avgAmount > 100 filter is applied to filter only events that report an average production amount greater than 100. The filtered events are inserted into the FooStream stream. c. To select all the responses from the Micro Integrator to be logged, add a new query named LogResponseEvents ``` @info(name = 'LogResponseEvents') from BarStream select * insert into LogStream; ``` The responses received from the Micro Integrator are directed to the BarStream input stream. This query gets them all these events from the BarStream stream and inserts them into the LogStream stream that is connected to a log stream so that they can be published as logs in the terminal. The Siddhi application is now complete. Click here to view the complete Siddhi application. @App:name(\"grpc-call-response\") @App:description(\"This application triggers integration process in the micro integrator using gRPC calls\") @source(type = 'http', receiver.url='http://localhost:8009/InputStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(symbol string, amount double); @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json', @payload(\"\"\"{\"symbol\":\"{{symbol}}\",\"avgAmount\":{{avgAmount}}}\"\"\")) ) @sink(type='log') define stream FooStream (symbol string, avgAmount double); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (symbol string, avgAmount double); @sink(type='log', prefix='response_from_mi: ') define stream LogStream (symbol string, avgAmount double); @info(name = 'CalculateAverageProductionPerMinute') from InputStream#window.timeBatch(5 sec) select avg(amount) as avgAmount, symbol group by symbol insert into AVGStream; @info(name = 'FilterExcessProduction') from AVGStream[avgAmount > 100] select symbol, avgAmount insert into FooStream; @info(name = 'LogResponseEvents') from BarStream select * insert into LogStream; Save the Siddhi application. As a result, it is saved in the <SI_TOOLING_HOME>/wso2/server/deployment/workspace directory. To deploy your Siddhi application, copy it from the <SI_TOOLING_HOME>/wso2/server/deployment/workspace directory, and then paste it in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory. Configuring Micro integrator \u00b6 After doing the required configurations in the Streaming Integrator, let's configure the Micro Integrator to receive the excess production alert from the Streaming Integrator as a gRPC event and send back a response. Start the gRPC server in the Micro Integrator server to receive the Streaming Integrator event. To do this, save the following inbound endpoint configuration as GrpcInboundEndpoint.xml in the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> This configuration has a configuration parameter to start the gRPC server, and specifies the default sequence to inject messages accordingly. Deploy the following sequence by saving it as inSeq.xml file in the <MI_Home>/repository/deployment/server/synapse-configs/default/sequences directory. Info Note that the name of the sequence is inSeq . This is referred to in the gRPC sink configuration in the grpc-call-response Siddhi application you previously created in the Streaming Integrator. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> This sequence does the following: Calls the REST endpoint that returns a JSON object. Logs the response. Sends the response back to the gRPC client. Executing and getting results \u00b6 To send an event to the defines http source hosted in http://localhost:8006/InputStream , issue the following sample CURL command. curl -X POST -d \"{\\\"event\\\":{\\\"symbol\\\":\\\"soap\\\",\\\"amount\\\":110.23}}\" http://localhost:8006/InputStream --header \"Content-Type:application/json\" In the SI console an output similar to following will be printed after 1 minute (if the average of the amount is larger than 100) INFO {io.siddhi.core.stream.output.sink.LogSink} - response_from_mi: : Event{timestamp=1573711436547, data=[soap, 110.23], isExpired=false}","title":"Triggering Integrations via MI"},{"location":"examples/triggering-integrations-via-micro-integrator/#triggering-integration-flows-via-the-micro-integrator","text":"","title":"Triggering Integration Flows via the Micro Integrator"},{"location":"examples/triggering-integrations-via-micro-integrator/#introduction","text":"In this tutorial, lets look at how the Streaming Integrator generates an alert based on the events received, and how that particular alert can trigger an integration flow in the Micro Integrator, and get a response back to the Streaming Integrator for further processing. To understand this consider a scenario where the Streaming Integrator receives production data from a factory, and triggers an integration flow if it detects a per minute production average that exceeds 100.","title":"Introduction"},{"location":"examples/triggering-integrations-via-micro-integrator/#configuring-the-streaming-integrator","text":"Let's design a Siddhi application that triggers an integration flow and deploy it by following the procedure below: Start and access the Streaming Integrator Tooling. Then click New to open a new application. Add a name and a description for your new Siddhi application as follows: @App:name(\"grpc-call-response\") @App:description(\"This application triggers integration process in the micro integrator using gRPC calls\") Let's add an input stream to define the schema of input production events, and connect a source of the http type to to receive those events. @source(type = 'http', receiver.url='http://localhost:8006/InputStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(symbol string, amount double); Here, the Streaming Integrator receives events to the http://localhost:8006/InputStream in the JSON format. Each event reports the product name (via the symbol attribute) and the amount produced. Now, let's add the configurations to publish an alert in the Micro Integrator to trigger an integration flow, and then receive a response back into the Streaming Integrator. @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json', @payload(\"\"\"{\"symbol\":\"{{symbol}}\",\"avgAmount\":{{avgAmount}}}\"\"\")) ) define stream FooStream (symbol string, avgAmount double); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (symbol string, avgAmount double); Note the following in the above configuration: Each output event that represents an alert that is published to the Micro Integrator reports the product name and the average production (as per the schema of the FooStream stream. The grpc-call sink connected to the FooStream stream gets the two attributes from the stream and generates the output events as JSON messages before they are published to the Micro Integrator. The value for the publisher.url parameter in the sink configuration contains process and inSeq which means that the Streaming Integrator calls the process method of the gRPC Listener server in the Micro Integrator, and injects the message to the inSeq which then sends a response back to the client. The grpc-call-response source connected to the BarStream input stream retrieves a response from the Micro Integrator and publishes it as a JSON message in the Streaming Integrator. As specified via the schema of the BarStream input stream, this response comprises of a single JSON message. To publish the messages received from the Micro Integrator as logs in the terminal, let's define an output stream named LogStream , and connect a sink of the log type to it as shown below. @sink(type='log', prefix='response_from_mi: ') define stream LogStream (symbol string, avgAmount double); Let's define Siddhi queries to calculate the average production per minute, filter production runs where the average production per minute is greater than 100, and direct the logs to be published to the output stream. a. To calculate the average per minute, add a Siddi query named CalculateAverageProductionPerMinute as follows: ``` @info(name = 'CalculateAverageProductionPerMinute') from InputStream#window.timeBatch(1 min) select avg(amount) as avgAmount, symbol group by symbol insert into AVGStream; ``` This query applies a time batch window to the InputStream stream so that events within each minute is considered a separate subset to be calculations in the query are applied. The minutes are considered in a tumbling manner because it is a batch window. Then the avg() function is applied to the amount attribute of the input stream to derive the average production amount. The results are then inserted into an inferred stream named AVGStream . b. To filter events from the AVGStream stream where the average production is greater then 100, add a query named FilterExcessProduction as follows. ``` @info(name = 'FilterExcessProduction') from AVGStream[avgAmount > 100] select symbol, avgAmount insert into FooStream; ``` Here, the avgAmount > 100 filter is applied to filter only events that report an average production amount greater than 100. The filtered events are inserted into the FooStream stream. c. To select all the responses from the Micro Integrator to be logged, add a new query named LogResponseEvents ``` @info(name = 'LogResponseEvents') from BarStream select * insert into LogStream; ``` The responses received from the Micro Integrator are directed to the BarStream input stream. This query gets them all these events from the BarStream stream and inserts them into the LogStream stream that is connected to a log stream so that they can be published as logs in the terminal. The Siddhi application is now complete. Click here to view the complete Siddhi application. @App:name(\"grpc-call-response\") @App:description(\"This application triggers integration process in the micro integrator using gRPC calls\") @source(type = 'http', receiver.url='http://localhost:8009/InputStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(symbol string, amount double); @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json', @payload(\"\"\"{\"symbol\":\"{{symbol}}\",\"avgAmount\":{{avgAmount}}}\"\"\")) ) @sink(type='log') define stream FooStream (symbol string, avgAmount double); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (symbol string, avgAmount double); @sink(type='log', prefix='response_from_mi: ') define stream LogStream (symbol string, avgAmount double); @info(name = 'CalculateAverageProductionPerMinute') from InputStream#window.timeBatch(5 sec) select avg(amount) as avgAmount, symbol group by symbol insert into AVGStream; @info(name = 'FilterExcessProduction') from AVGStream[avgAmount > 100] select symbol, avgAmount insert into FooStream; @info(name = 'LogResponseEvents') from BarStream select * insert into LogStream; Save the Siddhi application. As a result, it is saved in the <SI_TOOLING_HOME>/wso2/server/deployment/workspace directory. To deploy your Siddhi application, copy it from the <SI_TOOLING_HOME>/wso2/server/deployment/workspace directory, and then paste it in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory.","title":"Configuring the Streaming Integrator"},{"location":"examples/triggering-integrations-via-micro-integrator/#configuring-micro-integrator","text":"After doing the required configurations in the Streaming Integrator, let's configure the Micro Integrator to receive the excess production alert from the Streaming Integrator as a gRPC event and send back a response. Start the gRPC server in the Micro Integrator server to receive the Streaming Integrator event. To do this, save the following inbound endpoint configuration as GrpcInboundEndpoint.xml in the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> This configuration has a configuration parameter to start the gRPC server, and specifies the default sequence to inject messages accordingly. Deploy the following sequence by saving it as inSeq.xml file in the <MI_Home>/repository/deployment/server/synapse-configs/default/sequences directory. Info Note that the name of the sequence is inSeq . This is referred to in the gRPC sink configuration in the grpc-call-response Siddhi application you previously created in the Streaming Integrator. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> This sequence does the following: Calls the REST endpoint that returns a JSON object. Logs the response. Sends the response back to the gRPC client.","title":"Configuring Micro integrator"},{"location":"examples/triggering-integrations-via-micro-integrator/#executing-and-getting-results","text":"To send an event to the defines http source hosted in http://localhost:8006/InputStream , issue the following sample CURL command. curl -X POST -d \"{\\\"event\\\":{\\\"symbol\\\":\\\"soap\\\",\\\"amount\\\":110.23}}\" http://localhost:8006/InputStream --header \"Content-Type:application/json\" In the SI console an output similar to following will be printed after 1 minute (if the average of the amount is larger than 100) INFO {io.siddhi.core.stream.output.sink.LogSink} - response_from_mi: : Event{timestamp=1573711436547, data=[soap, 110.23], isExpired=false}","title":"Executing and getting results"},{"location":"examples/tutorials-overview/","text":"Tutorials Overview \u00b6 The following table lists the tutorials available for different user scenarios. User Scenario Tutorials Performing ETL - Performing Real-time ETL with Files - Performing Real-time ETL with MySQL Receiving and Publishing Data Working with Kafka Working with API Exposing Processed Data as API Working with the Micro Integrator Triggering Integration Flows via MI Working in Containerized Environments Running the Streaming Integrator in Containerized Environments","title":"Tutorials Overview"},{"location":"examples/tutorials-overview/#tutorials-overview","text":"The following table lists the tutorials available for different user scenarios. User Scenario Tutorials Performing ETL - Performing Real-time ETL with Files - Performing Real-time ETL with MySQL Receiving and Publishing Data Working with Kafka Working with API Exposing Processed Data as API Working with the Micro Integrator Triggering Integration Flows via MI Working in Containerized Environments Running the Streaming Integrator in Containerized Environments","title":"Tutorials Overview"},{"location":"examples/working-with-kafka/","text":"Working with Kafka \u00b6 Introduction \u00b6 The Streaming Integrator can consume from a Kafka topic as well as to publish to a Kafka topic in a streaming manner. This tutorial takes you through consuming from a Kafka topic, processing the messages, and finally, publishing output to a Kafka topic. Before you begin: Prepare the server to consume from or to publish to Kafka, follow the steps below: 1. Download the Kafka broker from the Apache site and extract it. From here onwards, this directory is referred to as <KAFKA_HOME> . 2. Create a directory named Source in a preferred location in your machine and copy the following JARs to it from the <KAFKA_HOME>/libs directory. - kafka_2.12-2.3.0.jar - kafka-clients-2.3.0.jar - metrics-core-2.2.0.jar - scala-library-2.12.8.jar - zkclient-0.11.jar - zookeeper-3.4.14.jar 3. Create another directory named Destination in a preferred location in your machine. 4. To convert the Kafka JARS you copied to the Source directory, issue the following command: sh <SI_HOME>/bin/jartobundle.sh <{Source}_Directory_Path> <{Destination}_Directory_Path> 5. Copy all the jars from the Destination directory to the <SI_HOME>/lib directory. Tutorial steps \u00b6 Consuming data from Kafka \u00b6 Start Kafka \u00b6 Navigate to the <KAFKA_HOME> directory and start a zookeeper node by issuing the following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to the <KAFKA_HOME> directory and start Kafka server node by issuing the following command. sh bin/kafka-server-start.sh config/server.properties Start the Streaming Integrator \u00b6 Navigate to the <SI_HOME>/bin directory and issue the following command. sh server.sh The following log appears on the SI console when the server is started successfully. INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec Consume from a Kafka topic \u00b6 Let's create a basic Siddhi application to consume messages from a Kafka topic. Open a text file and copy-paste following Siddhi application to it. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); -- Query to transform the name to upper case. from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save this file as HelloKafka.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. The following log appears on the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App HelloKafka deployed successfully Info You just created a Siddhi application that listens to a Kafka topic named productions and logs any incoming messages. When logging, the name attribute of the message is converted to upper case. However, you have still not created this Kafka topic or published any messages to it. To do this, proceed to the next section. Generate Kafka messages \u00b6 Now let's generate some Kafka messages that the Streaming Integrator can receive. First, let's create a topic named productions in the Kafka server. To do this, navigate to <KAFKA_HOME> and run following command: bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic productions Now let's run the Kafka command line client to push a few messages to the Kafka server. bin/kafka-console-producer.sh --broker-list localhost:9092 --topic productions Now you are prompted to type messages in the console. Type the following in the command prompt: {\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}} This pushes a message to the Kafka Server. Then, the Siddhi application you deployed in the Streaming Integrator consumes this message. As a result, the Streaming Integrator log displays the following: ``` INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562069868006, data=[ALMOND COOKIE, 100.0], isExpired=false} ``` You may notice that the output message has an uppercase name: ALMOND COOKIE . This is because of the simple message transformation done in the Siddhi application. Consuming with an offset \u00b6 Previously, you consumed messages from the productions topic without specifying an offset . In other words, the Kafka offset was zero. In this section, instead of consuming with a zero offset, you specify an offset value and consume messages from that offset onwards. For this purpose, you can configure the topic.offsets.map parameter. Let's modify our previous Siddhi application to specify an offset value. Specify an offset value 2 so that the Siddhi application consumes messages with index 2 and above. Open the <SI_HOME>/wso2/server/deployment/siddhi-files/HelloKafka.siddhi file and add the following new configuration parameter. topic.offsets.map='productions=2' Now the complete Siddhi application is as follows. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', topic.offsets.map='productions=2', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save the file. Push the following message to the Kafka server. {\"event\":{ \"name\":\"Baked alaska\", \"amount\":20.0}} Note that this is the second message that you pushed (hence bearing index 1), and therefore it is not consumed by the Streaming Integrator. Let's push another message (bearing index 2) to the Kafka server. {\"event\":{ \"name\":\"Cup cake\", \"amount\":300.0}} Now you can see the following log in the Streaming Integrator Studio console. ``` INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562676477785, data=[CUP CAKE, 300.0], isExpired=false} ``` As you configured your Siddhi application to consume messages with offset 2 , all messages bearing index 2 or above are consumed. Adding more consumers to the consumer group \u00b6 In our HelloKafka Siddhi application, note the group.id parameter. This parameter defines the Kafka consumer group. Let's add another Siddhi application HelloKafka_2 , to add another Kafka consumer to the same consumer group. Open a text file and copy-paste following Siddhi application to it. @App:name(\"HelloKafka_2\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save this file as HelloKafka_2.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App HelloKafka_2 deployed successfully Navigate to the <KAFKA_HOME> directory and run following command. bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 2 --topic productions This adds another partition to the productions Kafka topic. Push following messages to the Kafka server using the Kafka Console Producer. {\"event\":{ \"name\":\"Doughnut\", \"amount\":500.0}} {\"event\":{ \"name\":\"Danish pastry\", \"amount\":200.0}} {\"event\":{ \"name\":\"Eclair\", \"amount\":400.0}} {\"event\":{ \"name\":\"Eclair toffee\", \"amount\":100.0}} Now observe the logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759480019, data=[DOUGHNUT, 500.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759494710, data=[DANISH PASTRY, 200.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759506252, data=[ECLAIR, 400.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759508757, data=[ECLAIR TOFFEE, 100.0], isExpired=false} You can see that the events are being received by the two consumers in a Round Robin manner. Events received by the first consumer are logged by Siddhi application HelloKafka , whilst events received by the second consumer are logged by the HelloKafka_2 Siddhi application. Assigning consumers to partitions \u00b6 In the previous scenario, you had two partitions for the Kafka topic and two consumers. Instead of assigning the consumers to the partitions, you allowed Kafka do the assignments. Optionally, you can assign consumers to partitions. This option is useful if you have multiple consumers with different performance speeds, and you need to balance the load among the consumers. Let's alter your topic to have three partitions. After that, you can assign two partitions to consumer-1 , and the remaining partition to consumer-2 . Navigate to the <KAFKA_HOME> directory and issue following command. bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 3 --topic productions This adds another partition to the productions Kafka topic. Now there are three partitions in total. To assign partitions to the consumers, add the partition.no.list parameter as shown below. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') -- consumer-1 @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='json')) define stream SweetProductionStream1 (name string, amount double); -- consumer-2 @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', partition.no.list='2', @map(type='json')) define stream SweetProductionStream2 (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double, id string); from SweetProductionStream1 select str:upper(name) as name, amount, 'consumer-1' as id insert into OutputStream; from SweetProductionStream2 select str:upper(name) as name, amount, 'consumer-2' as id insert into OutputStream; Note that consumer-1 is assigned partitions 0 and 1 , while consumer-2 is assigned partition 2 . Now let's publish some messages as follows, and see how the load is distributed among the consumers with the new partition assignments. {\"event\":{ \"name\":\"Fortune cookie\", \"amount\":100.0}} {\"event\":{ \"name\":\"Frozen yogurt\", \"amount\":350.0}} {\"event\":{ \"name\":\"Gingerbread\", \"amount\":450.0}} {\"event\":{ \"name\":\"Hot-fudge sundae\", \"amount\":150.0}} {\"event\":{ \"name\":\"Hot-chocolate pudding\", \"amount\":200.0}} {\"event\":{ \"name\":\"Ice cream cake\", \"amount\":250.0}} Now observe the Streaming Integrator logs. The following is displayed. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851086792, data=[FORTUNE COOKIE, 100.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851092100, data=[FROZEN YOGURT, 350.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851094459, data=[GINGERBREAD, 450.0, consumer-2], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851096434, data=[HOT-FUDGE SUNDAE, 150.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851098328, data=[HOT-CHOCOLATE PUDDING, 200.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851100309, data=[ICE CREAM CAKE, 250.0, consumer-2], isExpired=false} You can observe a pattern where the load is distributed among consumer-1 and consumer-2 in the 2:1 ratio. This is because you assigned two partitions to consumer-1 and assigned only one partition to consumer-2 . Publish to a Kafka topic \u00b6 Now let's create a new Siddhi application to consume from the productions topic, filter the incoming messages based on a condition, and then publish those filtered messages to another Kafka topic. First, let's create a new topic named bulk-orders in the Kafka server. To publish the filtered messages to the bulk-orders Kafka topic you created, issue the following command. bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic bulk-orders Next, let's create the Siddhi application. Open a text file, and copy-paste following Siddhi application into it. @App:name(\"PublishToKafka\") @App:description('Consume events from a Kafka Topic, do basic filtering and publish filtered messages to a Kafka topic.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group2\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='kafka', topic='bulk-orders', bootstrap.servers='localhost:9092', partition.no='0', @map(type='json')) define stream BulkOrdersStream (name string, amount double); from SweetProductionStream[amount > 50] select * insert into BulkOrdersStream; Save this file as PublishToKafka.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App PublishToKafka deployed successfully Info The PublishToKafka Siddhi application consumes all the messages from the productions topic and populates the SweetProductionStream stream. All the sweet production runs where the amount is greater than 100 are inserted into the BulkOrdersStream stream. These events are pushed to the bulk-orders Kafka topic. To observe the messages in the bulk-orders topic, run a Kafka Console Consumer. Then navigate to the <KAFKA_HOME> directory and issue the following command. bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic bulk-orders --from-beginning You can see the following message in the Kafka Consumer log. These indicate the production runs of which the amount is greater than 50. {\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}} Preserving the state of the application through a system failure \u00b6 Let's try out a scenario in which you deploy a Siddhi application to count the total number of productions. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Let's create a new topic named sandwich_productions in the Kafka server. To do this, navigate to <KAFKA_HOME> and run following command: bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sandwich_productions Open a text file and copy-paste following Siddhi application to it. @App:name(\"CountProductions\") @App:description('Siddhi application to count the total number of orders.') @source(type='kafka', topic.list='sandwich_productions', threading.option='single.thread', group.id=\"group3\", bootstrap.servers='localhost:9092', partition.no.list='0', @map(type='json')) define stream SandwichProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (totalProductions double); from SandwichProductionStream select sum(amount) as totalProductions insert into OutputStream; Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now let's run the Kafka command line client to push a few messages to the Kafka server. Navigate to <KAFKA_HOME> and run following command: bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sandwich_productions Now you are prompted to type the messages in the console. Type following in the command prompt: {\"event\":{ \"name\":\"Bagel\", \"amount\":100.0}} {\"event\":{ \"name\":\"Buterbrod\", \"amount\":100.0}} Now the following logs appear on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[200.0], isExpired=false} These logs print the sandwich production count. Note that the current count of sandwich productions is being printed as 200 in the second log. This is because the production count up to now is 200 sandwiches: 100 bagels and 100 buterbrods. Now wait for following log to appear on the SI console DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can notice this log appearing every minute. Next, let's push two sandwich production messages to the Kafka server and shutdown the SI server before state persistence happens (i.e., before the above log appears). Tip It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server, until next log appears. Now push following messages to the Kafka server using the Kafka Console Producer: {\"event\":{ \"name\":\"Croissant\", \"amount\":100.0}} {\"event\":{ \"name\":\"Croutons\", \"amount\":100.0}} Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions 100 Croissants and 100 Croutons). The good news is, the Kafka source replays the last two messages, thereby allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute to observe the following logs. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912073, data=[300.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912076, data=[400.0], isExpired=false} Note that the Kafka source has replayed the last two messages. As a result, the sandwich productions count is correctly restored.","title":"Working with Kafka"},{"location":"examples/working-with-kafka/#working-with-kafka","text":"","title":"Working with Kafka"},{"location":"examples/working-with-kafka/#introduction","text":"The Streaming Integrator can consume from a Kafka topic as well as to publish to a Kafka topic in a streaming manner. This tutorial takes you through consuming from a Kafka topic, processing the messages, and finally, publishing output to a Kafka topic. Before you begin: Prepare the server to consume from or to publish to Kafka, follow the steps below: 1. Download the Kafka broker from the Apache site and extract it. From here onwards, this directory is referred to as <KAFKA_HOME> . 2. Create a directory named Source in a preferred location in your machine and copy the following JARs to it from the <KAFKA_HOME>/libs directory. - kafka_2.12-2.3.0.jar - kafka-clients-2.3.0.jar - metrics-core-2.2.0.jar - scala-library-2.12.8.jar - zkclient-0.11.jar - zookeeper-3.4.14.jar 3. Create another directory named Destination in a preferred location in your machine. 4. To convert the Kafka JARS you copied to the Source directory, issue the following command: sh <SI_HOME>/bin/jartobundle.sh <{Source}_Directory_Path> <{Destination}_Directory_Path> 5. Copy all the jars from the Destination directory to the <SI_HOME>/lib directory.","title":"Introduction"},{"location":"examples/working-with-kafka/#tutorial-steps","text":"","title":"Tutorial steps"},{"location":"examples/working-with-kafka/#consuming-data-from-kafka","text":"","title":"Consuming data from Kafka"},{"location":"examples/working-with-kafka/#start-kafka","text":"Navigate to the <KAFKA_HOME> directory and start a zookeeper node by issuing the following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to the <KAFKA_HOME> directory and start Kafka server node by issuing the following command. sh bin/kafka-server-start.sh config/server.properties","title":"Start Kafka"},{"location":"examples/working-with-kafka/#start-the-streaming-integrator","text":"Navigate to the <SI_HOME>/bin directory and issue the following command. sh server.sh The following log appears on the SI console when the server is started successfully. INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec","title":"Start the Streaming Integrator"},{"location":"examples/working-with-kafka/#consume-from-a-kafka-topic","text":"Let's create a basic Siddhi application to consume messages from a Kafka topic. Open a text file and copy-paste following Siddhi application to it. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); -- Query to transform the name to upper case. from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save this file as HelloKafka.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. The following log appears on the SI console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App HelloKafka deployed successfully Info You just created a Siddhi application that listens to a Kafka topic named productions and logs any incoming messages. When logging, the name attribute of the message is converted to upper case. However, you have still not created this Kafka topic or published any messages to it. To do this, proceed to the next section.","title":"Consume from a Kafka topic"},{"location":"examples/working-with-kafka/#generate-kafka-messages","text":"Now let's generate some Kafka messages that the Streaming Integrator can receive. First, let's create a topic named productions in the Kafka server. To do this, navigate to <KAFKA_HOME> and run following command: bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic productions Now let's run the Kafka command line client to push a few messages to the Kafka server. bin/kafka-console-producer.sh --broker-list localhost:9092 --topic productions Now you are prompted to type messages in the console. Type the following in the command prompt: {\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}} This pushes a message to the Kafka Server. Then, the Siddhi application you deployed in the Streaming Integrator consumes this message. As a result, the Streaming Integrator log displays the following: ``` INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562069868006, data=[ALMOND COOKIE, 100.0], isExpired=false} ``` You may notice that the output message has an uppercase name: ALMOND COOKIE . This is because of the simple message transformation done in the Siddhi application.","title":"Generate Kafka messages"},{"location":"examples/working-with-kafka/#consuming-with-an-offset","text":"Previously, you consumed messages from the productions topic without specifying an offset . In other words, the Kafka offset was zero. In this section, instead of consuming with a zero offset, you specify an offset value and consume messages from that offset onwards. For this purpose, you can configure the topic.offsets.map parameter. Let's modify our previous Siddhi application to specify an offset value. Specify an offset value 2 so that the Siddhi application consumes messages with index 2 and above. Open the <SI_HOME>/wso2/server/deployment/siddhi-files/HelloKafka.siddhi file and add the following new configuration parameter. topic.offsets.map='productions=2' Now the complete Siddhi application is as follows. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', topic.offsets.map='productions=2', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save the file. Push the following message to the Kafka server. {\"event\":{ \"name\":\"Baked alaska\", \"amount\":20.0}} Note that this is the second message that you pushed (hence bearing index 1), and therefore it is not consumed by the Streaming Integrator. Let's push another message (bearing index 2) to the Kafka server. {\"event\":{ \"name\":\"Cup cake\", \"amount\":300.0}} Now you can see the following log in the Streaming Integrator Studio console. ``` INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562676477785, data=[CUP CAKE, 300.0], isExpired=false} ``` As you configured your Siddhi application to consume messages with offset 2 , all messages bearing index 2 or above are consumed.","title":"Consuming with an offset"},{"location":"examples/working-with-kafka/#adding-more-consumers-to-the-consumer-group","text":"In our HelloKafka Siddhi application, note the group.id parameter. This parameter defines the Kafka consumer group. Let's add another Siddhi application HelloKafka_2 , to add another Kafka consumer to the same consumer group. Open a text file and copy-paste following Siddhi application to it. @App:name(\"HelloKafka_2\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double); from SweetProductionStream select str:upper(name) as name, amount insert into OutputStream; Save this file as HelloKafka_2.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App HelloKafka_2 deployed successfully Navigate to the <KAFKA_HOME> directory and run following command. bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 2 --topic productions This adds another partition to the productions Kafka topic. Push following messages to the Kafka server using the Kafka Console Producer. {\"event\":{ \"name\":\"Doughnut\", \"amount\":500.0}} {\"event\":{ \"name\":\"Danish pastry\", \"amount\":200.0}} {\"event\":{ \"name\":\"Eclair\", \"amount\":400.0}} {\"event\":{ \"name\":\"Eclair toffee\", \"amount\":100.0}} Now observe the logs on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759480019, data=[DOUGHNUT, 500.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759494710, data=[DANISH PASTRY, 200.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759506252, data=[ECLAIR, 400.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759508757, data=[ECLAIR TOFFEE, 100.0], isExpired=false} You can see that the events are being received by the two consumers in a Round Robin manner. Events received by the first consumer are logged by Siddhi application HelloKafka , whilst events received by the second consumer are logged by the HelloKafka_2 Siddhi application.","title":"Adding more consumers to the consumer group"},{"location":"examples/working-with-kafka/#assigning-consumers-to-partitions","text":"In the previous scenario, you had two partitions for the Kafka topic and two consumers. Instead of assigning the consumers to the partitions, you allowed Kafka do the assignments. Optionally, you can assign consumers to partitions. This option is useful if you have multiple consumers with different performance speeds, and you need to balance the load among the consumers. Let's alter your topic to have three partitions. After that, you can assign two partitions to consumer-1 , and the remaining partition to consumer-2 . Navigate to the <KAFKA_HOME> directory and issue following command. bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 3 --topic productions This adds another partition to the productions Kafka topic. Now there are three partitions in total. To assign partitions to the consumers, add the partition.no.list parameter as shown below. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and log the messages on the console.') -- consumer-1 @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', partition.no.list='0,1', @map(type='json')) define stream SweetProductionStream1 (name string, amount double); -- consumer-2 @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group1\", bootstrap.servers='localhost:9092', partition.no.list='2', @map(type='json')) define stream SweetProductionStream2 (name string, amount double); @sink(type='log') define stream OutputStream (name string, amount double, id string); from SweetProductionStream1 select str:upper(name) as name, amount, 'consumer-1' as id insert into OutputStream; from SweetProductionStream2 select str:upper(name) as name, amount, 'consumer-2' as id insert into OutputStream; Note that consumer-1 is assigned partitions 0 and 1 , while consumer-2 is assigned partition 2 . Now let's publish some messages as follows, and see how the load is distributed among the consumers with the new partition assignments. {\"event\":{ \"name\":\"Fortune cookie\", \"amount\":100.0}} {\"event\":{ \"name\":\"Frozen yogurt\", \"amount\":350.0}} {\"event\":{ \"name\":\"Gingerbread\", \"amount\":450.0}} {\"event\":{ \"name\":\"Hot-fudge sundae\", \"amount\":150.0}} {\"event\":{ \"name\":\"Hot-chocolate pudding\", \"amount\":200.0}} {\"event\":{ \"name\":\"Ice cream cake\", \"amount\":250.0}} Now observe the Streaming Integrator logs. The following is displayed. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851086792, data=[FORTUNE COOKIE, 100.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851092100, data=[FROZEN YOGURT, 350.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851094459, data=[GINGERBREAD, 450.0, consumer-2], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851096434, data=[HOT-FUDGE SUNDAE, 150.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851098328, data=[HOT-CHOCOLATE PUDDING, 200.0, consumer-1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851100309, data=[ICE CREAM CAKE, 250.0, consumer-2], isExpired=false} You can observe a pattern where the load is distributed among consumer-1 and consumer-2 in the 2:1 ratio. This is because you assigned two partitions to consumer-1 and assigned only one partition to consumer-2 .","title":"Assigning consumers to partitions"},{"location":"examples/working-with-kafka/#publish-to-a-kafka-topic","text":"Now let's create a new Siddhi application to consume from the productions topic, filter the incoming messages based on a condition, and then publish those filtered messages to another Kafka topic. First, let's create a new topic named bulk-orders in the Kafka server. To publish the filtered messages to the bulk-orders Kafka topic you created, issue the following command. bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic bulk-orders Next, let's create the Siddhi application. Open a text file, and copy-paste following Siddhi application into it. @App:name(\"PublishToKafka\") @App:description('Consume events from a Kafka Topic, do basic filtering and publish filtered messages to a Kafka topic.') @source(type='kafka', topic.list='productions', threading.option='single.thread', group.id=\"group2\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='kafka', topic='bulk-orders', bootstrap.servers='localhost:9092', partition.no='0', @map(type='json')) define stream BulkOrdersStream (name string, amount double); from SweetProductionStream[amount > 50] select * insert into BulkOrdersStream; Save this file as PublishToKafka.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App PublishToKafka deployed successfully Info The PublishToKafka Siddhi application consumes all the messages from the productions topic and populates the SweetProductionStream stream. All the sweet production runs where the amount is greater than 100 are inserted into the BulkOrdersStream stream. These events are pushed to the bulk-orders Kafka topic. To observe the messages in the bulk-orders topic, run a Kafka Console Consumer. Then navigate to the <KAFKA_HOME> directory and issue the following command. bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic bulk-orders --from-beginning You can see the following message in the Kafka Consumer log. These indicate the production runs of which the amount is greater than 50. {\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}}","title":"Publish to a Kafka topic"},{"location":"examples/working-with-kafka/#preserving-the-state-of-the-application-through-a-system-failure","text":"Let's try out a scenario in which you deploy a Siddhi application to count the total number of productions. Info In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator. Enable state persistence feature in SI server as follows. Open the <SI_HOME>/conf/server/deployment.yaml file on a text editor and locate the state.persistence section. # Periodic Persistence Configuration state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence Set enabled parameter to true and save the file. Enable state persistence debug logs as follows. Open the <SI_HOME>/conf/server/log4j2.xml file on a text editor and locate following line in it. <Logger name=\"com.zaxxer.hikari\" level=\"error\"/> Add following <Logger> element below that. <Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/> Save the file. Restart the Streaming Integrator server for above change to be effective. Let's create a new topic named sandwich_productions in the Kafka server. To do this, navigate to <KAFKA_HOME> and run following command: bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sandwich_productions Open a text file and copy-paste following Siddhi application to it. @App:name(\"CountProductions\") @App:description('Siddhi application to count the total number of orders.') @source(type='kafka', topic.list='sandwich_productions', threading.option='single.thread', group.id=\"group3\", bootstrap.servers='localhost:9092', partition.no.list='0', @map(type='json')) define stream SandwichProductionStream (name string, amount double); @sink(type='log') define stream OutputStream (totalProductions double); from SandwichProductionStream select sum(amount) as totalProductions insert into OutputStream; Save this file as CountProductions.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. When the Siddhi application is successfully deployed, the following INFO log appears in the Streaming Integrator console. INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully Now let's run the Kafka command line client to push a few messages to the Kafka server. Navigate to <KAFKA_HOME> and run following command: bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sandwich_productions Now you are prompted to type the messages in the console. Type following in the command prompt: {\"event\":{ \"name\":\"Bagel\", \"amount\":100.0}} {\"event\":{ \"name\":\"Buterbrod\", \"amount\":100.0}} Now the following logs appear on the SI console. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[100.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[200.0], isExpired=false} These logs print the sandwich production count. Note that the current count of sandwich productions is being printed as 200 in the second log. This is because the production count up to now is 200 sandwiches: 100 bagels and 100 buterbrods. Now wait for following log to appear on the SI console DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can notice this log appearing every minute. Next, let's push two sandwich production messages to the Kafka server and shutdown the SI server before state persistence happens (i.e., before the above log appears). Tip It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server, until next log appears. Now push following messages to the Kafka server using the Kafka Console Producer: {\"event\":{ \"name\":\"Croissant\", \"amount\":100.0}} {\"event\":{ \"name\":\"Croutons\", \"amount\":100.0}} Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count. Info Here the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions 100 Croissants and 100 Croutons). The good news is, the Kafka source replays the last two messages, thereby allowing the Streaming Integrator to successfully recover from the server crash. Restart the SI server and wait for about one minute to observe the following logs. INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912073, data=[300.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912076, data=[400.0], isExpired=false} Note that the Kafka source has replayed the last two messages. As a result, the sandwich productions count is correctly restored.","title":"Preserving the state of the application through a system failure"},{"location":"guides/cleansing-data/","text":"Cleansing Data \u00b6 Introduction \u00b6 When you receive input data via the Streaming Integrator, it may consist of data that is not required to generate the required output, null values for certain attributes, etc. Cleansing data refers to refining the input data received by applying null values, Filtering data based on conditions \u00b6 To understand the different ways you can filter the specific data you need to transform and enrich in order to generate the required output, follow the procedures below: Filtering based on exact match of attribute: Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application via the @App:name annotation. In this example, let's name it TemperatureApp . Define an input stream to specify the schema based on which events are selected to the Streaming Integration flow. define stream TempStream (deviceID long, roomNo string, temp double); Info For more information about defining input streams to receive events, see the Consuming Data guide . Add a query to generate filtered temperature readings as follows. For this example, let's assume that you want to filter only temperature readings for a specific room number (e.g., room no 2233 ). Add the from clause and enter TempStream as the input stream from which the input data. However, because you only need to extract readings for room no 2233 , include a filter in the from clause as shown below: from TempStream [roomNo=='2233'] Add the select clause with * to indicate that all the attributes should be selected without any changes. from TempStream [roomNo=='2233'] select * Add the insert to clause and direct the output to a stream named Room2233AnalysisStream . Info Note that the Room2233AnalysisStream is not defined in the Siddhi application. It is inferred by specifying it as an output stream. from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream; Tip As a best practice, name your queries using the @info annotation. In this example, you can name the query Filtering as follows. @info(name = 'Filtering2233') from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream; The saved Siddhi application is as follows: @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'Filtering2233') from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream Filtering based on regex pattern You can filter events by providing a condition where only events that match a specific Regex pattern are taken for further processing. For this purpose, you can use the TemperatureApp Siddhi application that you created in the previous example. However, instead of filtering the readings for a specific room no, you can filter the readings for many rooms of which the room number matches a specific regex pattern. Assume that you want to filter the temperature readings for a specific rage of rooms located in the Southern wing and used for purpose B. Also assume that this can be derived from the room number because the first three characters of the room no represent the wing, and the eighth character represents the purpose. e.g., in room no SOU5438B765 , the first three characters SOU represent the Southern wing, and the eighth character B represents purpose B. To filter events as described, follow the procedure below. Open the TemperatureApp Siddi application. Create a new query named FilteredRoomRange as follows: Add a from clause as follows to get the required events from the TempStream stream. from TempStream Add select statement with the regex pattern as follows: select deviceID, regex.find(SOU*B*) as roomNo, temp Add the insert to clause as follows to insert the results into a stream named FilteredResultsStream . insert into FilteredResultsStream; The completed query is as follows. @info(name = 'FilteredRoomRange') from TempStream select deviceID, regex.find(SOU*B*) as roomNo, temp insert into FilteredResultsStream; Save the Siddhi application. The completed Siddhi application looks as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'FilteredRoomRange') from TempStream select deviceID, regex.find(SOU*B*) as roomNo, temp insert into FilteredResultsStream; Filtering based on multiple criteria For this purpose, you can use the TemperatureApp Siddhi application that you created in the example under Filtering based on exact match of attribute section. However, instead of filtering only readings for room No 2233 , assume that you need to filter the readings for a range of rooms (e.g., rooms 100-210) where the temperature is greater than 40. For this, you can update the filter as follows. [(roomNo >= 100 and roomNo < 210) and temp > 40] Here, the and logical expression is used to indicate that both the filter conditions provided need to be considered. Modifying, removing and replacing attributes \u00b6 The input data may include attributes that are not required in order to generate the required output, attributes with values that need to be updated or replaced before further processing. Assume that in the previous example, you do not need the device ID for further processing, and you need to remove some unnecessary white spaces from the roomNo before sending the input data for further processing. To do this, follow the procedure below: Open the TemperatureApp Siddhi application that you previously created in the Filtering data based on conditions section and start adding a new query. You can name it as CleaningData as shown below. @info(name = 'CleaningData') Add the from clause and enter FilteredResultsStream as the input stream from which the input data is taken. from FilteredResultsStream Let's create the select statement as follows. To select only the roomNo and temp attributes for further processing and remove the deviceID attribute, add them as follows. select roomNo, temp To remove the unnecessary white spaces from the room number, add the trim() function as shown below. trim(roomNo) Now the completed select statement is as follows. select trim(roomNo), temp Insert the results into an output stream as follows. insert into CleansedDataStream; The completed query is as follows: @info(name = 'CleaningData') from FilteredResultsStream select trim(roomNo), temp insert into CleansedDataStream; Modifying and replacing is also demonstrated in the Enriching Data and Transforming Data guides. Handling attributes with null values \u00b6 To understand this section, you can reuse the TemperatureApp Siddhi application that you created in the Filtering data based on conditions . Assume that some events arrive with null values for the deviceID attribute, and you want to assign the value unknown in such scenarios. To do this, follow the procedure below: Start adding a new query to the TemperatureApp Siddhi application. You can name it AddingMissingValues as follows. @info(name = 'AddingMissingValues') Add the from clause and enter FilteredResultsStream as the input stream from which the input data is taken. from FilteredResultsStream Note Here, we are using the inferred output stream of the previous query as the input stream for this query. As a result, the changes made via this query are applied to the filtered data. Add the select clause. To assign unknown as the value for the deviceID attribute when it has a null value, you need to use the ifThenElse function as shown below. ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID Select the roomNo and temp attributes can be selected without any changes. The query updated with the select clause now looks as follows. select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp Insert the results into an output stream as follows. insert into CleansedDataStream The completed query now looks as follows. @info(name = 'AddingMissingValues') from FilteredResultsStream select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp insert into CleansedDataStream Save the Siddhi application. The completed version looks as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'Filtering') from TempStream [roomNo=='2233'] select * insert into FilteredResultsStream; @info(name = 'AddingMissingValues') from FilteredResultsStream select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp insert into CleansedDataStream;","title":"Cleansing Data"},{"location":"guides/cleansing-data/#cleansing-data","text":"","title":"Cleansing Data"},{"location":"guides/cleansing-data/#introduction","text":"When you receive input data via the Streaming Integrator, it may consist of data that is not required to generate the required output, null values for certain attributes, etc. Cleansing data refers to refining the input data received by applying null values,","title":"Introduction"},{"location":"guides/cleansing-data/#filtering-data-based-on-conditions","text":"To understand the different ways you can filter the specific data you need to transform and enrich in order to generate the required output, follow the procedures below: Filtering based on exact match of attribute: Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application via the @App:name annotation. In this example, let's name it TemperatureApp . Define an input stream to specify the schema based on which events are selected to the Streaming Integration flow. define stream TempStream (deviceID long, roomNo string, temp double); Info For more information about defining input streams to receive events, see the Consuming Data guide . Add a query to generate filtered temperature readings as follows. For this example, let's assume that you want to filter only temperature readings for a specific room number (e.g., room no 2233 ). Add the from clause and enter TempStream as the input stream from which the input data. However, because you only need to extract readings for room no 2233 , include a filter in the from clause as shown below: from TempStream [roomNo=='2233'] Add the select clause with * to indicate that all the attributes should be selected without any changes. from TempStream [roomNo=='2233'] select * Add the insert to clause and direct the output to a stream named Room2233AnalysisStream . Info Note that the Room2233AnalysisStream is not defined in the Siddhi application. It is inferred by specifying it as an output stream. from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream; Tip As a best practice, name your queries using the @info annotation. In this example, you can name the query Filtering as follows. @info(name = 'Filtering2233') from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream; The saved Siddhi application is as follows: @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'Filtering2233') from TempStream [roomNo=='2233'] select * insert into Room2233AnalysisStream Filtering based on regex pattern You can filter events by providing a condition where only events that match a specific Regex pattern are taken for further processing. For this purpose, you can use the TemperatureApp Siddhi application that you created in the previous example. However, instead of filtering the readings for a specific room no, you can filter the readings for many rooms of which the room number matches a specific regex pattern. Assume that you want to filter the temperature readings for a specific rage of rooms located in the Southern wing and used for purpose B. Also assume that this can be derived from the room number because the first three characters of the room no represent the wing, and the eighth character represents the purpose. e.g., in room no SOU5438B765 , the first three characters SOU represent the Southern wing, and the eighth character B represents purpose B. To filter events as described, follow the procedure below. Open the TemperatureApp Siddi application. Create a new query named FilteredRoomRange as follows: Add a from clause as follows to get the required events from the TempStream stream. from TempStream Add select statement with the regex pattern as follows: select deviceID, regex.find(SOU*B*) as roomNo, temp Add the insert to clause as follows to insert the results into a stream named FilteredResultsStream . insert into FilteredResultsStream; The completed query is as follows. @info(name = 'FilteredRoomRange') from TempStream select deviceID, regex.find(SOU*B*) as roomNo, temp insert into FilteredResultsStream; Save the Siddhi application. The completed Siddhi application looks as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'FilteredRoomRange') from TempStream select deviceID, regex.find(SOU*B*) as roomNo, temp insert into FilteredResultsStream; Filtering based on multiple criteria For this purpose, you can use the TemperatureApp Siddhi application that you created in the example under Filtering based on exact match of attribute section. However, instead of filtering only readings for room No 2233 , assume that you need to filter the readings for a range of rooms (e.g., rooms 100-210) where the temperature is greater than 40. For this, you can update the filter as follows. [(roomNo >= 100 and roomNo < 210) and temp > 40] Here, the and logical expression is used to indicate that both the filter conditions provided need to be considered.","title":"Filtering data based on conditions"},{"location":"guides/cleansing-data/#modifying-removing-and-replacing-attributes","text":"The input data may include attributes that are not required in order to generate the required output, attributes with values that need to be updated or replaced before further processing. Assume that in the previous example, you do not need the device ID for further processing, and you need to remove some unnecessary white spaces from the roomNo before sending the input data for further processing. To do this, follow the procedure below: Open the TemperatureApp Siddhi application that you previously created in the Filtering data based on conditions section and start adding a new query. You can name it as CleaningData as shown below. @info(name = 'CleaningData') Add the from clause and enter FilteredResultsStream as the input stream from which the input data is taken. from FilteredResultsStream Let's create the select statement as follows. To select only the roomNo and temp attributes for further processing and remove the deviceID attribute, add them as follows. select roomNo, temp To remove the unnecessary white spaces from the room number, add the trim() function as shown below. trim(roomNo) Now the completed select statement is as follows. select trim(roomNo), temp Insert the results into an output stream as follows. insert into CleansedDataStream; The completed query is as follows: @info(name = 'CleaningData') from FilteredResultsStream select trim(roomNo), temp insert into CleansedDataStream; Modifying and replacing is also demonstrated in the Enriching Data and Transforming Data guides.","title":"Modifying, removing and replacing attributes"},{"location":"guides/cleansing-data/#handling-attributes-with-null-values","text":"To understand this section, you can reuse the TemperatureApp Siddhi application that you created in the Filtering data based on conditions . Assume that some events arrive with null values for the deviceID attribute, and you want to assign the value unknown in such scenarios. To do this, follow the procedure below: Start adding a new query to the TemperatureApp Siddhi application. You can name it AddingMissingValues as follows. @info(name = 'AddingMissingValues') Add the from clause and enter FilteredResultsStream as the input stream from which the input data is taken. from FilteredResultsStream Note Here, we are using the inferred output stream of the previous query as the input stream for this query. As a result, the changes made via this query are applied to the filtered data. Add the select clause. To assign unknown as the value for the deviceID attribute when it has a null value, you need to use the ifThenElse function as shown below. ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID Select the roomNo and temp attributes can be selected without any changes. The query updated with the select clause now looks as follows. select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp Insert the results into an output stream as follows. insert into CleansedDataStream The completed query now looks as follows. @info(name = 'AddingMissingValues') from FilteredResultsStream select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp insert into CleansedDataStream Save the Siddhi application. The completed version looks as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo string, temp double); @info(name = 'Filtering') from TempStream [roomNo=='2233'] select * insert into FilteredResultsStream; @info(name = 'AddingMissingValues') from FilteredResultsStream select ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp insert into CleansedDataStream;","title":"Handling attributes with null values"},{"location":"guides/consuming-messages/","text":"Consuming Data \u00b6 Introduction \u00b6 The first step in a streaming integration flow is to consume the data to be cleaned, enriched, transformed or summarized to produce the required output. For the Streaming Integrator to consume events, the following is required. A message schema: The Streaming Integrator identifies the messages that it selects into a streaming integration flows by their schemas. The schema based on which the messages are selected are defined via a stream . A source: The messages are consumed from different sources including streaming applications, cloud-based applications, databases, and files. The source is defined via a source configuration . A source configuration consists of the following: @source : This annotation defines the source type via which the messages are consumed, and allows you to configure the source parameters (which change depending on the source type). For the complete list of supported source types, see Siddhi Query Guide - Source @map : This annotation specifies the format in which messages are consumed, and allows you to configure the mapping parameters (which change based of the mapping type/format selected). For the complete list of supported mapping types, see Siddhi Query Guide - Source Mapper @attributes : This annotation specifies a custom mapping based on which events to be selected into the streaming integration flow are identified. This is useful when the attributes of the incoming messages you want the Streaming Integrator to consume are different to the corresponding attribute name in the stream definition. e.g., In a scenario where the Streaming Integrator is reading employee records, the employee name might be defined as emp No in the database from which you are extracting the records. However, the corresponding attribute name in the stream definition is employeeNo because that is how yoy want to refer to the attribute in the streaming integration flow. In this instance, you need a custom mapping to indicate that emp No is the same as employeeNo . In a Siddhi application, you can define a source configuration inline or refer to a source configuration defined externally in a configuration file. Defining event source inline in the Siddhi application \u00b6 To create a Siddhi application with the source configuration defined inline, follow the steps below. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) e.g., @App:name(\"SalesTotalsApp\") Define an input stream to define the schema based on which input events are selected to the streaming integrator flow as follows. define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...) e.g., define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long) Connect a source to the input stream you added as follows. @source(type='<SOURCE_TYPE>') define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., You can add a source of the http type to the ConsumeSalesTotalsStream input stream in the example of the previous step. @source(type='<http>') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Configure parameters for the source you added. e.g., You can specify an HTTP URL for the http source in the example used. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add an @map annotation to the source configuration as shown below. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., In the example used, you can specify JSON as the map type as follows: @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Info Mapping is explained in detail in the Consuming a message in default format and Consuming a message in custom format sections. However, note that you need to add a mapping type to complete a source configuration. If no mapping type is specified, an error is indicated. Add a Siddhi query to specify how the output is derived and the name of an output stream to which this output is directed. from <INPUT_STREAM_NAME> select <ATTRIBUTE1_Name>, <ATTRIBUTE2_NAME>, ... group by <ATTRIBUTE_NAME> insert into <OUTPUT_STREAM_NAME>; e.g., Assuming that you are publishing the events with the existing values as logs in the output console without any further processing, you can define the query as follows. from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; Complete the Siddhi application by defining an output stream with a connected sink configuration. Tip In the example used, you can define the PublishSalesTotals stream that you already specified as the output stream in the query, and connect a log sink to it as follows. Publishing the output is explained in detail in the Publishing Data guide . @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi Application. The completed application is as follows: @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; Defining event source externally in the configuration file \u00b6 If you want to use the same source configuration in multiple Siddhi applications, you can define it externally in the <SI_HOME>/conf/server/deployment.yaml file and then refer to it from Siddhi applications. To understand how to do this, follow the procedure below. Open the <SI_HOME>/conf/server/deployment.yaml file. Add a section named siddi , and then add a subsection named refs: as shown below. siddhi: refs: - In the refs subsection, enter a parameter named name and enter a name for the source. siddhi: refs: - name:`<SOURCE_NAME>` To specify the source type, add another parameter named type and enter the relevant source type. siddhi: refs: - name:'<SOURCE_NAME>' type: '<SOURCE_TYPE>' To configure other parameters for the source (based on the source type), add a subsection named properties as shown below. siddhi: refs: - name:'SOURCE_NAME' type: '<SOURCE_TYPE>' properties <PROPERTY1_NAME>:'<PROPERTY1_VALUE>' <PROPERTY2_NAME>:'<PROPERTY2_VALUE>' ... Save the configuration file. e.g., The HTTP source used as the example in the previous section can be defined externally as follows: siddhi: refs: - name:'HTTPSource' type: 'http' properties receiver.url:'http://localhost:5005/SalesTotalsEP' The source configuration you added can be referred to in a Siddhi application as follows: @source(ref='SOURCE_NAME') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); e.g., The HTTP source that you previously created can be referred to as follows. @source(ref='HTTP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Supported event source types \u00b6 Supported message formats \u00b6 Consuming a message in default format \u00b6 SI consumes a message in the default format when it makes no changes to the names of the attributes of the message schema before it processes the message. To understand how messages are consumed in default format, follow the procedure below. Create a Siddhi application with a source configuration following the instructions in the Defining event source inline in Siddhi Application subsection. In the source configuration, make sure that an @map annotation is included with the mapping type as shown below. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); The map type specifies the format in which the messages are received. e.g., In the example used, you can specify JSON as the map type as follows: @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi application. If you save the Siddhi application that was created using the example configurations, the completed Siddhi application is as follows. @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; To check whether the above Siddhi application works as expected, generate some messages. e.g., In the example used, the source type is HTTP. Therefore, you can issue a few curl commands similar to the following: curl -X POST \\ http://localhost:5005/SalesTotalsEP \\ -H 'content-type: application/json' \\ -d '{ \"event\": { \"transNo\": \"001\", \"product\": \"DDT\", \"price\": \"100\", \"quantity\": \"100\", \"salesValue\" \"10000\" } }' Consuming a message in custom format \u00b6 SI consumes a message in the custom format when it makes changes to the names of the attributes of the message schema before it processes the message. To understand how messages are consumed in custom format, follow the procedure below. Info For this section, you can edit the same Siddhi application that you saved in the Consuming a message in default format subsection. Open your Siddhi application with a source configuration. In the @map annotation within the source configuration, add the @attributes annotation with mappings for different attributes. This can be done in two ways as shown below. Defining attributes as keys and mapping content as values in the following format. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>', @attributes( attributeN='mapping_N', attribute1='mapping_1'))) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., In the Siddhi application used as an example in the previous section, assume that when receiving events, the transNo attribute is received as transaction and the salesValue attribute is received as sales . The mapping type is JSON. therefore, you can add the mappings as JSONPath expressions. Stream Attribute Name JSON Event Attribute Name JSONPath Expression transNo transaction $.transaction salesValue sales $.sales The mapping can be defined as follows. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json', @attributes(transNo = '$.transaction', salesValue = '$.sales'))) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Defining the mapping content of all attributes in the same order as how the attributes are defined in stream definition. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>', @attributes( 'mapping_1', 'mapping_N'))) define stream <Stream_Name>(attribute1_name attribute1_type, attributeN_name attributeN_type, ...); e.g., If you consider the same example, mapping can be defined as follows. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json', @attributes(transNo = '$.transaction', product = product, quantity = quantity, salesValue = '$.sales'))) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long);","title":"Consuming Data"},{"location":"guides/consuming-messages/#consuming-data","text":"","title":"Consuming Data"},{"location":"guides/consuming-messages/#introduction","text":"The first step in a streaming integration flow is to consume the data to be cleaned, enriched, transformed or summarized to produce the required output. For the Streaming Integrator to consume events, the following is required. A message schema: The Streaming Integrator identifies the messages that it selects into a streaming integration flows by their schemas. The schema based on which the messages are selected are defined via a stream . A source: The messages are consumed from different sources including streaming applications, cloud-based applications, databases, and files. The source is defined via a source configuration . A source configuration consists of the following: @source : This annotation defines the source type via which the messages are consumed, and allows you to configure the source parameters (which change depending on the source type). For the complete list of supported source types, see Siddhi Query Guide - Source @map : This annotation specifies the format in which messages are consumed, and allows you to configure the mapping parameters (which change based of the mapping type/format selected). For the complete list of supported mapping types, see Siddhi Query Guide - Source Mapper @attributes : This annotation specifies a custom mapping based on which events to be selected into the streaming integration flow are identified. This is useful when the attributes of the incoming messages you want the Streaming Integrator to consume are different to the corresponding attribute name in the stream definition. e.g., In a scenario where the Streaming Integrator is reading employee records, the employee name might be defined as emp No in the database from which you are extracting the records. However, the corresponding attribute name in the stream definition is employeeNo because that is how yoy want to refer to the attribute in the streaming integration flow. In this instance, you need a custom mapping to indicate that emp No is the same as employeeNo . In a Siddhi application, you can define a source configuration inline or refer to a source configuration defined externally in a configuration file.","title":"Introduction"},{"location":"guides/consuming-messages/#defining-event-source-inline-in-the-siddhi-application","text":"To create a Siddhi application with the source configuration defined inline, follow the steps below. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) e.g., @App:name(\"SalesTotalsApp\") Define an input stream to define the schema based on which input events are selected to the streaming integrator flow as follows. define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...) e.g., define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long) Connect a source to the input stream you added as follows. @source(type='<SOURCE_TYPE>') define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., You can add a source of the http type to the ConsumeSalesTotalsStream input stream in the example of the previous step. @source(type='<http>') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Configure parameters for the source you added. e.g., You can specify an HTTP URL for the http source in the example used. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add an @map annotation to the source configuration as shown below. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., In the example used, you can specify JSON as the map type as follows: @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Info Mapping is explained in detail in the Consuming a message in default format and Consuming a message in custom format sections. However, note that you need to add a mapping type to complete a source configuration. If no mapping type is specified, an error is indicated. Add a Siddhi query to specify how the output is derived and the name of an output stream to which this output is directed. from <INPUT_STREAM_NAME> select <ATTRIBUTE1_Name>, <ATTRIBUTE2_NAME>, ... group by <ATTRIBUTE_NAME> insert into <OUTPUT_STREAM_NAME>; e.g., Assuming that you are publishing the events with the existing values as logs in the output console without any further processing, you can define the query as follows. from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; Complete the Siddhi application by defining an output stream with a connected sink configuration. Tip In the example used, you can define the PublishSalesTotals stream that you already specified as the output stream in the query, and connect a log sink to it as follows. Publishing the output is explained in detail in the Publishing Data guide . @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi Application. The completed application is as follows: @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream;","title":"Defining event source inline in the Siddhi application"},{"location":"guides/consuming-messages/#defining-event-source-externally-in-the-configuration-file","text":"If you want to use the same source configuration in multiple Siddhi applications, you can define it externally in the <SI_HOME>/conf/server/deployment.yaml file and then refer to it from Siddhi applications. To understand how to do this, follow the procedure below. Open the <SI_HOME>/conf/server/deployment.yaml file. Add a section named siddi , and then add a subsection named refs: as shown below. siddhi: refs: - In the refs subsection, enter a parameter named name and enter a name for the source. siddhi: refs: - name:`<SOURCE_NAME>` To specify the source type, add another parameter named type and enter the relevant source type. siddhi: refs: - name:'<SOURCE_NAME>' type: '<SOURCE_TYPE>' To configure other parameters for the source (based on the source type), add a subsection named properties as shown below. siddhi: refs: - name:'SOURCE_NAME' type: '<SOURCE_TYPE>' properties <PROPERTY1_NAME>:'<PROPERTY1_VALUE>' <PROPERTY2_NAME>:'<PROPERTY2_VALUE>' ... Save the configuration file. e.g., The HTTP source used as the example in the previous section can be defined externally as follows: siddhi: refs: - name:'HTTPSource' type: 'http' properties receiver.url:'http://localhost:5005/SalesTotalsEP' The source configuration you added can be referred to in a Siddhi application as follows: @source(ref='SOURCE_NAME') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); e.g., The HTTP source that you previously created can be referred to as follows. @source(ref='HTTP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long);","title":"Defining event source externally in the configuration file"},{"location":"guides/consuming-messages/#supported-event-source-types","text":"","title":"Supported event source types"},{"location":"guides/consuming-messages/#supported-message-formats","text":"","title":"Supported message formats"},{"location":"guides/consuming-messages/#consuming-a-message-in-default-format","text":"SI consumes a message in the default format when it makes no changes to the names of the attributes of the message schema before it processes the message. To understand how messages are consumed in default format, follow the procedure below. Create a Siddhi application with a source configuration following the instructions in the Defining event source inline in Siddhi Application subsection. In the source configuration, make sure that an @map annotation is included with the mapping type as shown below. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); The map type specifies the format in which the messages are received. e.g., In the example used, you can specify JSON as the map type as follows: @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi application. If you save the Siddhi application that was created using the example configurations, the completed Siddhi application is as follows. @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; To check whether the above Siddhi application works as expected, generate some messages. e.g., In the example used, the source type is HTTP. Therefore, you can issue a few curl commands similar to the following: curl -X POST \\ http://localhost:5005/SalesTotalsEP \\ -H 'content-type: application/json' \\ -d '{ \"event\": { \"transNo\": \"001\", \"product\": \"DDT\", \"price\": \"100\", \"quantity\": \"100\", \"salesValue\" \"10000\" } }'","title":"Consuming a message in default format"},{"location":"guides/consuming-messages/#consuming-a-message-in-custom-format","text":"SI consumes a message in the custom format when it makes changes to the names of the attributes of the message schema before it processes the message. To understand how messages are consumed in custom format, follow the procedure below. Info For this section, you can edit the same Siddhi application that you saved in the Consuming a message in default format subsection. Open your Siddhi application with a source configuration. In the @map annotation within the source configuration, add the @attributes annotation with mappings for different attributes. This can be done in two ways as shown below. Defining attributes as keys and mapping content as values in the following format. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>', @attributes( attributeN='mapping_N', attribute1='mapping_1'))) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., In the Siddhi application used as an example in the previous section, assume that when receiving events, the transNo attribute is received as transaction and the salesValue attribute is received as sales . The mapping type is JSON. therefore, you can add the mappings as JSONPath expressions. Stream Attribute Name JSON Event Attribute Name JSONPath Expression transNo transaction $.transaction salesValue sales $.sales The mapping can be defined as follows. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json', @attributes(transNo = '$.transaction', salesValue = '$.sales'))) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); Defining the mapping content of all attributes in the same order as how the attributes are defined in stream definition. @source(type='<SOURCE_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', @map(type='<MAP_TYPE>', @attributes( 'mapping_1', 'mapping_N'))) define stream <Stream_Name>(attribute1_name attribute1_type, attributeN_name attributeN_type, ...); e.g., If you consider the same example, mapping can be defined as follows. @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json', @attributes(transNo = '$.transaction', product = product, quantity = quantity, salesValue = '$.sales'))) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long);","title":"Consuming a message in custom format"},{"location":"guides/correlating-events/","text":"Correlating Data \u00b6 The streaming integrator can correlate data in order to detect patterns and trends in streaming data. Correlating can be done via patterns as well as sequences. The difference between patterns and sequence is that sequences require all the matching events to arrive consecutively to match the sequence condition, whereas patterns identify events that match the pattern condition irrespective of the order in which they arrive. Correlating events to find a pattern \u00b6 This section explains how you can use Siddhi patterns to detect trends and patterns. There are two types of Siddhi patterns as follows: Counting Patterns: These count the number of intances that match the given pattern condition. Logical Patterns: These identify logical relationships between events. Count and match multiple events for a given pattern condition \u00b6 To understand how to count and match multiple events that match a specific condition, consider the example where a store wants to check the frequency with which a specific product needs to be repaired within two months after it is purchased. If a specific product is brought back for repairs within two months more than five times, the manager of purchases needs to be notified via a mail. To do this, create a Siddhi application as follows. Start creating a new Siddhi application. You can name it DefectDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"DefectDetectionApp\") Define the input streams into which the events compared are received. To capture information about purchases, define a stream as follows. define stream PurchasesStream (productName string, custID string); To capture information about repairs, define a stream as follows. define stream RepairsStream (productName string, custID string); To notify the purchase manager that the threshold is reached define an output sink with an email sink attached as follows: @sink(type='email', address='storemanager@abc.com', username='storemanager', password='secret_password', subject='Defective Product Alert', to='purchasemanager@headoffice.com', @map(type = 'text', @payload(\"Hello,The product {{productName}} is identified as defective.\\n\\nThis message was generated automatically.\"))) define stream DefectiveProductsStream (productName string); To count occurrences where a product is brought back for repairs withing two months following its purchase, and identify products where the treshold for such occurrences is reached, create a query as follows. To specify the input streams from which the input events to be analyzed for pattern detection are taken, add a from clause as follows. from every (e1=PurchasesStream) -> e2=RepairsStream[e1.productName==e2.productName and e1.custID==e2.custID]<5:> within 2 months Info Note the following about the above from clause: - The input is derived from two streams. Therefore, first, both streams considered are specified and a unique reference is assigned to each stream. The PurchasesStream is referred to as e1 and the RepairsStream is referred to as e2 . - The matching condition to be met is that both streams should have an event where the values for both productName and custID attributes are the same. - The event in the PurchasesStream stream need to arrive before the matching event in the RepairsStream stream. - The matching event in the RepairsStream stream should arrive within two months after the arrival of the event in the PurchasesStream stream. - <5:> indicates that an output is generated only when the matching condition is met five times. - A time window of 2 months is added to consider only a period of two months in a sliding manner when counting the number of times the matching condition for the pattern is met. For more information about time windows, see Summarizing Data - Calculate and store clock-time based aggregate values To specify how the value for each attribute in the DefectiveProductsStream output stream is defined, add the select clause as follows. select e1.productName The output should only consist of the product identified to be possibly defective. Therefore, only the productName attribute is selected. To specify that the output has to directed to the DefectiveProductsStream , add the insert into clause as follows. insert into DefectiveProductsStream The completed Siddhi application is as follows. @App:name(\"DefectDetectionApp\") define stream PurchasesStream (productName string, custID string); define stream RepairsStream (productName string, custID string); @sink(type='email', address='storemanager@abc.com', username='storemanager', password='secret_password', subject='Defective Product Alert', to='purchasemanager@headoffice.com', @map(type = 'text', @payload(\"Hello,The product {{productName}} is identified as defective.\\n\\nThis message was generated automatically.\"))) define stream DefectiveProductsStream (productName string); from every (e1=PurchasesStream) -> e2=RepairsStream[e1.productName==e2.productName and e1.custID==e2.custID]<5:> within 2 months select e1.productName insert into DefectiveProductsStream Info For more information, see Siddhi Query Guide - Counting Patterns . Combine several patterns logically and match events \u00b6 To understand how to combine several patterns logically and match events, consider an example of a factory foreman who needs to observe the factory output, identify any production decreases and check whether those decreases have reached maximum threshold which requires him to take action. To do this, you can create a Siddhi application as follows: Start creating a new Siddhi application. You can name it ProductionDecreaseDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"ProductionDecreaseDetectionApp\") Define an input stream as follows to capture the factory output. define stream ProductionStream(productName string, factoryBranch string, productionAmount long); Now define an output stream as follows to present the observed production trend after applying the logical pattern. @sink(type='log', prefix='Decrease in production detected:') define stream ProductionDecreaseAlertStream (productName string, originalAmount long, laterAmount long, factoryBranch string); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . To apply the pattern so that the production trend can be observed, add the from clause as follows. from every (e1=ProductionStream) -> e2=ProductionStream[e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10] within 10 min Info Note the following about the from clause: Here, two events from the same stream are compared to identify whether the production has decreased. The unique reference for the first event is e1 , and the unique reference for the second event is e2 . e2 arrives after e1 , but it is not necessarily the event that arrives immediately after e1 . The condition that should be met for e1 and e2 to be compared is e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10 . This means, both the events should report the production of the same product, and there should be a decrease in production that is greater than 10 between the e1 and e2 events. A 10 min time window is included to indicate that an output event is generated only if the decrease in production by 10 or more units takes place every ten minutes in a sliding manner. For more information about time windows, see Calculate and store clock time-based aggregate values . To present the required output by deriving values for the attributes of the ProductionDecreaseAlertStream output stream you created, add the select clause as follows. select e1.productName, e1.productionAmount as originalAmount, e2.productionAmount as laterAmount, e1.factoryBranch Here, the production amount of the first event is presented as originalAmount , and the amount of the second event is presented as laterAmount . To insert the output into the ProductionDecreaseAlertStream output stream, add the insert into clause as follows. insert into ProductionDecreaseAlertStream; The completed Siddhi application is as follows. @App:name(\"ProductionDecreaseDetectionApp\") define stream ProductionStream(productName string, factoryBranch string, productionAmount long); @sink(type='log', prefix='Decrease in production detected:') define stream ProductionDecreaseAlertStream (productName string, originalAmount long, laterAmount long, factoryBranch string); from every (e1=ProductionStream) -> e2=ProductionStream[e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10] within 10 min select e1.productName, e1.productionAmount as originalAmount, e2.productionAmount as laterAmount, e1.factoryBranch insert into ProductionDecreaseAlertStream; Find non-occurance of events \u00b6 This section explains how to analyze data by observing scenarios where events do not occur. To understand how this is done, consider a taxi service company that tracks the movements of the taxis it runs and wants to be notified of unexpected delays. Consider a specific scenario where the manager needs to contact the driver if the taxi has not reached either of two specified locations withi 15 minutes. For this, you can create a Siddhi application as follows: Start creating a new Siddhi application. You can name it DelayDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"DelayDetectionApp\") To receive information about the location of taxis, define an input stream as follows. define stream LocationStream (taxiID string, driverID string, latitude double, longitude double); To publish delay notification as a message, define an output stream as follows. @sink(type='http', publisher.url='http://headoffice:8080/endpoint', @map(type = 'json')) define stream AlertStream (taxiID string, driverID string, message string); The output directed to this stream is published via a sink of the http type. For more information about publishing data via sinks, see the Publishing Data guide . To specify the pattern to be used to detect the delays, add the from clause as follows. from not LocationStream[latitude == 44.0096 and longitude == 81.2735] for 15 minutes or not LocationStream[latitude == 43.0096 and longitude == 81.2737] for 15 minutes Info Note the following about this from clause: The not keyword is added to indicate that the SI should look for instances where an event has not occurred when the given conditions are met. Two conditions are given. The alert is generated when either of the two conditions has not occured. To indicate this, the or keyword is used between the two conditions. The given conditions indicate that the taxi should have reached either the latitude == 44.0096 and longitude == 81.2735 location or the latitude == 43.0096 and longitude == 81.2737 location. Either of the locations should be reached within 15 minutes. Therefore, each location is specified as a separate condition and a time window of 15 minutes is applied to each condition in a sliding manner. For more information about time windows, see the Siddhi Query Guide - Calculate and store clock time-based aggregate values . To derive the information relating to the delay to be published as the output, add the select clause as follows. select LocationStream.taxiID, LocationStream.driverID, 'Unexpected Delay' as message The alert message is a standard message that is assigned as a static value to the message attribute. To insert the results into the AlertStream so that the message about the delay can be published, add the insert into clause as follows. insert into AlertStream; The completed Siddhi application is as follows. @App:name(\"DelayDetectionApp\") define stream LocationStream (taxiID string, driverID string, latitude double, longitude double); @sink(type='http', publisher.url='http://headoffice:8080/endpoint', @map(type = 'json')) define stream AlertStream (taxiID string, driverID string, message string); from not LocationStream[latitude == 44.0096 and longitude == 81.2735] for 15 minutes or not LocationStream[latitude == 43.0096 and longitude == 81.2737] for 15 minutes select LocationStream.taxiID, LocationStream.driverID, 'Unexpected Delay' as message insert into AlertStream; For the complete list of methods in which you can apply Siddhi patterns to detect non occuring events, see Siddhi Query Guide - Detecting Non-Occurring Events . Correlating events to find a trend(sequence) \u00b6 This section explains how you can use Siddhi sequences to detect trends in events that arrive in a specific order. There are two types of Siddhi sequences as follows: Counting Sequences: These count the number of instances that match the given sequence condition. Logical Sequences: These identify logical relationships between events. Count and match multiple events for a given trend \u00b6 Counting and matching multiple events over a given period is done via sequences when you need to identify trends in events that occur in a specific order. To understand how this is done, consider a scenario where the temperature is read from a sensor and you need to identify the peaks in temperature. If an event (i.e., a single reading) is a peak, it should report a temperaature greater than that reported by the event that occured immediately before it as well as the event that occurred immediately after it. Therefore, to identify the peaks, follow the procedure below: Start creating a new Siddhi application. You can name it TemperaturePeaksApp For instructions, see Creating a Siddhi Application . @App:name(\"TemperaturePeaksApp\") To capture the temperature readings, define an input stream as follows. define stream TempStream(deviceID long, roomNo int, temp double); To report the peaks once they are identified, define an output stream as follows. @sink(type='log', prefix='TemperaturePeak]:') define stream PeakTempStream(initialTemp double, peakTemp double); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . To specify how to identify the peaks, add a from clause as follows. from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] Info Note the following about the from clause: every indicates that all the events in the TempStream must be checked for the given conditions. Here, e2 is the reference for the event identified as the peak temperature. The e2=TempStream[e1.temp <= temp]+ condition specifies that to be identified as an event reporting a peak temperature, an event should have one or more preceding events that reports a lower or an equal temperature. The e3=TempStream[e2[last].temp > temp] condition specifies a condition for e3 which is the event that follows e2 . It indicates that e2 , the peak temperature event should be the last event before e3 , and that the temperature reported by e2 must be greater than the temperature reported by e3 . To specify how to derive the values for the attributes in the PeakTempStream output stream are derived, add a select clause as follows. select e1.temp as initialTemp, e2[last].temp as peakTemp Here, the temperature reported by e2 event is selected to be output as peakTemp because it is greater than the temperatures reported by events occuring before and after e2 . The temperature reported by the event immediately before e2 is selected as initialTemp . To insert the output generated into the PeakTempStream output stream, add an insert into clause as follows. insert into PeakTempStream; The completed Siddhi application is as follows. @App:name(\"TemperaturePeaksApp\") define stream TempStream(deviceID long, roomNo int, temp double); @sink(type='log', prefix='TemperaturePeak]:') define stream PeakTempStream(initialTemp double, peakTemp double); from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeakTempStream; Combine several trends logically and match events \u00b6 Logical sequences are used to identify logical relationships between events that occur in a specific order. To understand this consider a scenario where an application is able to notify the state only when the event that notifies that the regulator is switched on is immediately followed by two other events to report the temperature and humidity. To create such a Siddhi application, follow the procedure below. Start creating a new Siddhi application. You can name it RoomStateApp For instructions, see Creating a Siddhi Application . @App:name(\"RoomStateApp\") You need three input streams to capture information about the state of the regulator, the temperature, and humidity. Define the inpt stream that captures the state of the regulator as follows. define stream RegulatorStream(deviceID long, isOn bool); Define the input stream that captures the temperature as follows. define stream TempStream(deviceID long, temp double); Define the input stream that captures the humidity as follows. @sink(type='log', prefix='RoomState]:') define stream StateNotificationStream(temp double, humid double); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . Now let's define an output stream to publish the temperature and humidity. define stream StateNotificationStream(temp double, humid double) To apply the logical sequence to derive the output, add the from clause as follows. from every e1=RegulatorStream, e2=TempStream and e3=HumidStream Here, the unique references e1 , e2 , and e3 are assigned to the first, second, and thid events respectively. e1 must arrive at the RegulatorStream stream, e2 must arrive at the TempStream stream, and e3 must arrive at the HumidStream stream in that order. The output event is generated only after all three of these input events have arrived. To derive values for the attributes of the StateNotificationStream output stream, add a select clause as follows. select e2.temp, e3.humid To generate the output event, the value for the temp attribute must be taken from the e2 (second) event, and the value for the humid attribute must be taken from the e3 (third) event. To direct the output to the StateNotificationStream output stream so that it can be logged, add an insert into clause as follows. insert into StateNotificationStream; The completed Siddhi application is as follows. @App:name(\"RoomStateApp\") define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); @sink(type='log', prefix='RoomState]:') define stream StateNotificationStream(temp double, humid double); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream; Correlating two streams of data and unify \u00b6 For a detailed explanation, see Enriching Data - Enrich data by connecting with another stream of data Correlate a stream and a static datasource to enrich \u00b6 For a detailed explanation, see Enriching Data - Enrich data by connecting with a data store","title":"Correlating Data"},{"location":"guides/correlating-events/#correlating-data","text":"The streaming integrator can correlate data in order to detect patterns and trends in streaming data. Correlating can be done via patterns as well as sequences. The difference between patterns and sequence is that sequences require all the matching events to arrive consecutively to match the sequence condition, whereas patterns identify events that match the pattern condition irrespective of the order in which they arrive.","title":"Correlating Data"},{"location":"guides/correlating-events/#correlating-events-to-find-a-pattern","text":"This section explains how you can use Siddhi patterns to detect trends and patterns. There are two types of Siddhi patterns as follows: Counting Patterns: These count the number of intances that match the given pattern condition. Logical Patterns: These identify logical relationships between events.","title":"Correlating events to find a pattern"},{"location":"guides/correlating-events/#count-and-match-multiple-events-for-a-given-pattern-condition","text":"To understand how to count and match multiple events that match a specific condition, consider the example where a store wants to check the frequency with which a specific product needs to be repaired within two months after it is purchased. If a specific product is brought back for repairs within two months more than five times, the manager of purchases needs to be notified via a mail. To do this, create a Siddhi application as follows. Start creating a new Siddhi application. You can name it DefectDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"DefectDetectionApp\") Define the input streams into which the events compared are received. To capture information about purchases, define a stream as follows. define stream PurchasesStream (productName string, custID string); To capture information about repairs, define a stream as follows. define stream RepairsStream (productName string, custID string); To notify the purchase manager that the threshold is reached define an output sink with an email sink attached as follows: @sink(type='email', address='storemanager@abc.com', username='storemanager', password='secret_password', subject='Defective Product Alert', to='purchasemanager@headoffice.com', @map(type = 'text', @payload(\"Hello,The product {{productName}} is identified as defective.\\n\\nThis message was generated automatically.\"))) define stream DefectiveProductsStream (productName string); To count occurrences where a product is brought back for repairs withing two months following its purchase, and identify products where the treshold for such occurrences is reached, create a query as follows. To specify the input streams from which the input events to be analyzed for pattern detection are taken, add a from clause as follows. from every (e1=PurchasesStream) -> e2=RepairsStream[e1.productName==e2.productName and e1.custID==e2.custID]<5:> within 2 months Info Note the following about the above from clause: - The input is derived from two streams. Therefore, first, both streams considered are specified and a unique reference is assigned to each stream. The PurchasesStream is referred to as e1 and the RepairsStream is referred to as e2 . - The matching condition to be met is that both streams should have an event where the values for both productName and custID attributes are the same. - The event in the PurchasesStream stream need to arrive before the matching event in the RepairsStream stream. - The matching event in the RepairsStream stream should arrive within two months after the arrival of the event in the PurchasesStream stream. - <5:> indicates that an output is generated only when the matching condition is met five times. - A time window of 2 months is added to consider only a period of two months in a sliding manner when counting the number of times the matching condition for the pattern is met. For more information about time windows, see Summarizing Data - Calculate and store clock-time based aggregate values To specify how the value for each attribute in the DefectiveProductsStream output stream is defined, add the select clause as follows. select e1.productName The output should only consist of the product identified to be possibly defective. Therefore, only the productName attribute is selected. To specify that the output has to directed to the DefectiveProductsStream , add the insert into clause as follows. insert into DefectiveProductsStream The completed Siddhi application is as follows. @App:name(\"DefectDetectionApp\") define stream PurchasesStream (productName string, custID string); define stream RepairsStream (productName string, custID string); @sink(type='email', address='storemanager@abc.com', username='storemanager', password='secret_password', subject='Defective Product Alert', to='purchasemanager@headoffice.com', @map(type = 'text', @payload(\"Hello,The product {{productName}} is identified as defective.\\n\\nThis message was generated automatically.\"))) define stream DefectiveProductsStream (productName string); from every (e1=PurchasesStream) -> e2=RepairsStream[e1.productName==e2.productName and e1.custID==e2.custID]<5:> within 2 months select e1.productName insert into DefectiveProductsStream Info For more information, see Siddhi Query Guide - Counting Patterns .","title":"Count and match multiple events for a given pattern condition"},{"location":"guides/correlating-events/#combine-several-patterns-logically-and-match-events","text":"To understand how to combine several patterns logically and match events, consider an example of a factory foreman who needs to observe the factory output, identify any production decreases and check whether those decreases have reached maximum threshold which requires him to take action. To do this, you can create a Siddhi application as follows: Start creating a new Siddhi application. You can name it ProductionDecreaseDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"ProductionDecreaseDetectionApp\") Define an input stream as follows to capture the factory output. define stream ProductionStream(productName string, factoryBranch string, productionAmount long); Now define an output stream as follows to present the observed production trend after applying the logical pattern. @sink(type='log', prefix='Decrease in production detected:') define stream ProductionDecreaseAlertStream (productName string, originalAmount long, laterAmount long, factoryBranch string); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . To apply the pattern so that the production trend can be observed, add the from clause as follows. from every (e1=ProductionStream) -> e2=ProductionStream[e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10] within 10 min Info Note the following about the from clause: Here, two events from the same stream are compared to identify whether the production has decreased. The unique reference for the first event is e1 , and the unique reference for the second event is e2 . e2 arrives after e1 , but it is not necessarily the event that arrives immediately after e1 . The condition that should be met for e1 and e2 to be compared is e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10 . This means, both the events should report the production of the same product, and there should be a decrease in production that is greater than 10 between the e1 and e2 events. A 10 min time window is included to indicate that an output event is generated only if the decrease in production by 10 or more units takes place every ten minutes in a sliding manner. For more information about time windows, see Calculate and store clock time-based aggregate values . To present the required output by deriving values for the attributes of the ProductionDecreaseAlertStream output stream you created, add the select clause as follows. select e1.productName, e1.productionAmount as originalAmount, e2.productionAmount as laterAmount, e1.factoryBranch Here, the production amount of the first event is presented as originalAmount , and the amount of the second event is presented as laterAmount . To insert the output into the ProductionDecreaseAlertStream output stream, add the insert into clause as follows. insert into ProductionDecreaseAlertStream; The completed Siddhi application is as follows. @App:name(\"ProductionDecreaseDetectionApp\") define stream ProductionStream(productName string, factoryBranch string, productionAmount long); @sink(type='log', prefix='Decrease in production detected:') define stream ProductionDecreaseAlertStream (productName string, originalAmount long, laterAmount long, factoryBranch string); from every (e1=ProductionStream) -> e2=ProductionStream[e1.productName == e2.productName and e1.productionAmount - e2.productionAmount > 10] within 10 min select e1.productName, e1.productionAmount as originalAmount, e2.productionAmount as laterAmount, e1.factoryBranch insert into ProductionDecreaseAlertStream;","title":"Combine several patterns logically and match events"},{"location":"guides/correlating-events/#find-non-occurance-of-events","text":"This section explains how to analyze data by observing scenarios where events do not occur. To understand how this is done, consider a taxi service company that tracks the movements of the taxis it runs and wants to be notified of unexpected delays. Consider a specific scenario where the manager needs to contact the driver if the taxi has not reached either of two specified locations withi 15 minutes. For this, you can create a Siddhi application as follows: Start creating a new Siddhi application. You can name it DelayDetectionApp For instructions, see Creating a Siddhi Application . @App:name(\"DelayDetectionApp\") To receive information about the location of taxis, define an input stream as follows. define stream LocationStream (taxiID string, driverID string, latitude double, longitude double); To publish delay notification as a message, define an output stream as follows. @sink(type='http', publisher.url='http://headoffice:8080/endpoint', @map(type = 'json')) define stream AlertStream (taxiID string, driverID string, message string); The output directed to this stream is published via a sink of the http type. For more information about publishing data via sinks, see the Publishing Data guide . To specify the pattern to be used to detect the delays, add the from clause as follows. from not LocationStream[latitude == 44.0096 and longitude == 81.2735] for 15 minutes or not LocationStream[latitude == 43.0096 and longitude == 81.2737] for 15 minutes Info Note the following about this from clause: The not keyword is added to indicate that the SI should look for instances where an event has not occurred when the given conditions are met. Two conditions are given. The alert is generated when either of the two conditions has not occured. To indicate this, the or keyword is used between the two conditions. The given conditions indicate that the taxi should have reached either the latitude == 44.0096 and longitude == 81.2735 location or the latitude == 43.0096 and longitude == 81.2737 location. Either of the locations should be reached within 15 minutes. Therefore, each location is specified as a separate condition and a time window of 15 minutes is applied to each condition in a sliding manner. For more information about time windows, see the Siddhi Query Guide - Calculate and store clock time-based aggregate values . To derive the information relating to the delay to be published as the output, add the select clause as follows. select LocationStream.taxiID, LocationStream.driverID, 'Unexpected Delay' as message The alert message is a standard message that is assigned as a static value to the message attribute. To insert the results into the AlertStream so that the message about the delay can be published, add the insert into clause as follows. insert into AlertStream; The completed Siddhi application is as follows. @App:name(\"DelayDetectionApp\") define stream LocationStream (taxiID string, driverID string, latitude double, longitude double); @sink(type='http', publisher.url='http://headoffice:8080/endpoint', @map(type = 'json')) define stream AlertStream (taxiID string, driverID string, message string); from not LocationStream[latitude == 44.0096 and longitude == 81.2735] for 15 minutes or not LocationStream[latitude == 43.0096 and longitude == 81.2737] for 15 minutes select LocationStream.taxiID, LocationStream.driverID, 'Unexpected Delay' as message insert into AlertStream; For the complete list of methods in which you can apply Siddhi patterns to detect non occuring events, see Siddhi Query Guide - Detecting Non-Occurring Events .","title":"Find non-occurance of events"},{"location":"guides/correlating-events/#correlating-events-to-find-a-trendsequence","text":"This section explains how you can use Siddhi sequences to detect trends in events that arrive in a specific order. There are two types of Siddhi sequences as follows: Counting Sequences: These count the number of instances that match the given sequence condition. Logical Sequences: These identify logical relationships between events.","title":"Correlating events to find a trend(sequence)"},{"location":"guides/correlating-events/#count-and-match-multiple-events-for-a-given-trend","text":"Counting and matching multiple events over a given period is done via sequences when you need to identify trends in events that occur in a specific order. To understand how this is done, consider a scenario where the temperature is read from a sensor and you need to identify the peaks in temperature. If an event (i.e., a single reading) is a peak, it should report a temperaature greater than that reported by the event that occured immediately before it as well as the event that occurred immediately after it. Therefore, to identify the peaks, follow the procedure below: Start creating a new Siddhi application. You can name it TemperaturePeaksApp For instructions, see Creating a Siddhi Application . @App:name(\"TemperaturePeaksApp\") To capture the temperature readings, define an input stream as follows. define stream TempStream(deviceID long, roomNo int, temp double); To report the peaks once they are identified, define an output stream as follows. @sink(type='log', prefix='TemperaturePeak]:') define stream PeakTempStream(initialTemp double, peakTemp double); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . To specify how to identify the peaks, add a from clause as follows. from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] Info Note the following about the from clause: every indicates that all the events in the TempStream must be checked for the given conditions. Here, e2 is the reference for the event identified as the peak temperature. The e2=TempStream[e1.temp <= temp]+ condition specifies that to be identified as an event reporting a peak temperature, an event should have one or more preceding events that reports a lower or an equal temperature. The e3=TempStream[e2[last].temp > temp] condition specifies a condition for e3 which is the event that follows e2 . It indicates that e2 , the peak temperature event should be the last event before e3 , and that the temperature reported by e2 must be greater than the temperature reported by e3 . To specify how to derive the values for the attributes in the PeakTempStream output stream are derived, add a select clause as follows. select e1.temp as initialTemp, e2[last].temp as peakTemp Here, the temperature reported by e2 event is selected to be output as peakTemp because it is greater than the temperatures reported by events occuring before and after e2 . The temperature reported by the event immediately before e2 is selected as initialTemp . To insert the output generated into the PeakTempStream output stream, add an insert into clause as follows. insert into PeakTempStream; The completed Siddhi application is as follows. @App:name(\"TemperaturePeaksApp\") define stream TempStream(deviceID long, roomNo int, temp double); @sink(type='log', prefix='TemperaturePeak]:') define stream PeakTempStream(initialTemp double, peakTemp double); from every e1=TempStream, e2=TempStream[e1.temp <= temp]+, e3=TempStream[e2[last].temp > temp] select e1.temp as initialTemp, e2[last].temp as peakTemp insert into PeakTempStream;","title":"Count and match multiple events for a given trend"},{"location":"guides/correlating-events/#combine-several-trends-logically-and-match-events","text":"Logical sequences are used to identify logical relationships between events that occur in a specific order. To understand this consider a scenario where an application is able to notify the state only when the event that notifies that the regulator is switched on is immediately followed by two other events to report the temperature and humidity. To create such a Siddhi application, follow the procedure below. Start creating a new Siddhi application. You can name it RoomStateApp For instructions, see Creating a Siddhi Application . @App:name(\"RoomStateApp\") You need three input streams to capture information about the state of the regulator, the temperature, and humidity. Define the inpt stream that captures the state of the regulator as follows. define stream RegulatorStream(deviceID long, isOn bool); Define the input stream that captures the temperature as follows. define stream TempStream(deviceID long, temp double); Define the input stream that captures the humidity as follows. @sink(type='log', prefix='RoomState]:') define stream StateNotificationStream(temp double, humid double); The output directed to this stream is published via a sink of the log type. For more information about publishing data via sinks, see the Publishing Data guide . Now let's define an output stream to publish the temperature and humidity. define stream StateNotificationStream(temp double, humid double) To apply the logical sequence to derive the output, add the from clause as follows. from every e1=RegulatorStream, e2=TempStream and e3=HumidStream Here, the unique references e1 , e2 , and e3 are assigned to the first, second, and thid events respectively. e1 must arrive at the RegulatorStream stream, e2 must arrive at the TempStream stream, and e3 must arrive at the HumidStream stream in that order. The output event is generated only after all three of these input events have arrived. To derive values for the attributes of the StateNotificationStream output stream, add a select clause as follows. select e2.temp, e3.humid To generate the output event, the value for the temp attribute must be taken from the e2 (second) event, and the value for the humid attribute must be taken from the e3 (third) event. To direct the output to the StateNotificationStream output stream so that it can be logged, add an insert into clause as follows. insert into StateNotificationStream; The completed Siddhi application is as follows. @App:name(\"RoomStateApp\") define stream TempStream(deviceID long, temp double); define stream HumidStream(deviceID long, humid double); define stream RegulatorStream(deviceID long, isOn bool); @sink(type='log', prefix='RoomState]:') define stream StateNotificationStream(temp double, humid double); from every e1=RegulatorStream, e2=TempStream and e3=HumidStream select e2.temp, e3.humid insert into StateNotificationStream;","title":"Combine several trends logically and match events"},{"location":"guides/correlating-events/#correlating-two-streams-of-data-and-unify","text":"For a detailed explanation, see Enriching Data - Enrich data by connecting with another stream of data","title":"Correlating two streams of data and unify"},{"location":"guides/correlating-events/#correlate-a-stream-and-a-static-datasource-to-enrich","text":"For a detailed explanation, see Enriching Data - Enrich data by connecting with a data store","title":"Correlate a stream and a static datasource to enrich"},{"location":"guides/enriching-data/","text":"Enriching Data \u00b6 Introduction \u00b6 Enriching data involves integrated the data received into a streaming integration flow with data from other medium such as a data store, another data stream, or an external service to derive an expected result. To understand the different ways in which this is done, follow the sections below. Enrich data by connecting with a data store \u00b6 This section explains how to enrich the data in a specific stream by joining it with a data store. For this purpose, consider a scenario where you receive sales records generated from multiple locations as events via a system. Before you begin: In this scenario, you need to enrich the sales information you receive based on the records in a database table. You can download and install MySQL, and create this table before you carry out the procedure in this subsection. For detailed instructions to create a database table, expand the following section. Create Table Download and install the MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR ( mysql-connector-java-x.x.xx-bin.jar ) into the <SI_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Add the following configurations for three database tables under the Data Sources Configuration section of the <SI_HOME>/conf/server/deployment.yaml file. - name: UserDataDB description: Datasource used for User Data jndiConfig: name: jdbc/test useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/UserDataDB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To create a database named UserDataDB with a table named UserTable issue the following commands from the terminal: To create the UserDataDB table: mysql> create database UserDataDB; mysql> use UserDataDB; mysql> source <SP_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on UserDataDB.* TO username@localhost identified by \"password\"; To create the UserTable table: create table UserTable ( userId LONG, firstname VARCHAR, lastname VARCHAR, ) Start creating a new Siddhi application. You can name it EnrichingTransactionsApp For instructions, see Creating a Siddhi Application . Define the input stream and the database table that need to be joined as follows. Define the stream as follows. define stream TrasanctionStream (userId long, transactionAmount double, location string); Define the table as follows. define table UserTable (userId long, firstName string, lastName string); Then define the Siddhi query to join the stream and the table, and handle the result as required. Add the from clause as follows with the join key word to join the table and the stream. from TransactionStream as t join UserTable as u on t.userId == u.userId Info Note the following about the from clause: - In this example, the input data is taken from both a stream and a table. You need to assign a unique reference for each of them to allow the query to differentiate between the common attributes. In this example, TransactionStream stream is referred to as t , and the UserTable table is referred to as u . - The join keyword joins the stream and the table together while specifying the unique references. - The condition for the stream and the table to be joined is t.userId == u.userId , which means that for an event to be taken from the TransactionStream for the join, one or more events that have the same value for the userId must exist in the UserTable table and vice versa. To specify how the value for each attribute in the output stream is derived, add a select clause as follows. select t.userId, str:concat( u.firstName, \" \", u.lastName) as userName, transactionAmount, location Info Note the following in the select statement: - The userId attribute name is common to both the stream and the table. Therefore, you need to specify from where this attribute needs gto be taken. Here, you can also specify u.userId instead of t.userId . - You are specifying the output generated to include an attribute named userName . The value for that is derived by concatenating the values of two attributes in the UserTable table (i.e., firstName and lastName attributes) by applying the str:concat() function. Similarly, you can apply any of the range of Siddhi functions available to further enrich the joined output. For more information, see Siddhi Extensions . To infer an output stream into which the enriched data must be directed, add the insert into clause as follows. insert into EnrichedTrasanctionStream; The completed Siddhi application is as follows. @App:name(\"EnrichingTransactionsApp\") define stream TrasanctionStream (userId long, transactionAmount double, location string); define table UserTable (userId long, firstName string, lastName string); from TrasanctionStream as t join UserTable as u on t.userId == u.userId select t.userId, str:concat( u.firstName, \" \", u.lastName) as userName, transactionAmount, location insert into EnrichedTrasanctionStream; Enrich data by connecting with another stream of data \u00b6 This section explains how to enrich the data in a specific stream by joining it with another stream. To understand how this is done, consider a scenario where you receive information about cash withdrawals and cash deposits at different bank branches from two separate applications. Therefore, this two types of information are captured via two separate streams. To compare the withdrawals with deposits and observe whether enough deposits are being made to manage the withdrawals, you need to join both these streams. To do this, follow the procedure below. Start creating a new Siddhi application. You can name it BankTransactionsApp For instructions, see Creating a Siddhi Application . First, define the two input streams via which you are receiving informations about withdrawals and deposits. Define a stream named CashWithdrawalStream to capture information about withdrawals as follows. define stream CashWithdrawalStream(branchID int, amount long); Define a stream named CashDepositsStream to capture information about deposits as follows. define stream CashDepositsStream(branchID string, amount long); Now let's define an output stream to which the combined information from both the input streams need to be directed after the join. @sink(type='log', prefix='Cash withdrawals that go beyond sustainability threshold:') define stream CashFlowStream(branchID string, withdrawalAmount long, depositAmount long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To specify how the join is performed, and how to use the combined information, write a Siddhi query as follows. To perform the join, add the from clause as follows. from CashWithdrawalStream as w join CashDepositStream as d on w.branchID == d.branchID Info Observe the following about the above from clause: - Both the input streams have attributes of the same name. To identify each name, you must specify a reference for each stream. In this example, the reference for the CashWithdrawalStream is w , and the reference for the CashDepositsStream stream is d . - You need to use join as the keyword to join two streams. The join condition is w.branchID == d.branchID where branch IDs are matched. An event in the CashWithdrawalStream stream is directed to the CashFlowStream if there are events with the same branch ID in the CashDepositStream and vice versa. To specify how the value for each attribute is derived, add a select statement as follows. select w.branchID as branchID, w.amount as withdrawals, d.amount as deposits Info The branchID attribute name is common to both input streams. Therefore, you can also specify d.branchID as branchID instead of w.branchId as branchId . To filter only events where total cash withdrawals are greater than 95% of the cash deposits, add a having clause as follows. having w.amount > d.amount * 0.95 To insert the results into the CashFlowStream output stream, add the insert into clause as follows. insert into CashFlowStream; The completed Siddhi application is as follows: @App:name(\"BankTransactionsApp\"); define stream CashWithdrawalStream(branchID int, amount long); define stream CashDepositsStream(branchID string, amount long); @sink(type='log', prefix='Cash withdrawals that go beyond sustainability threshold:') define stream CashFlowStream(branchID string, withdrawalAmount long, depositAmount long); from CashWithdrawalStream as w join CashDepositStream as d on w.branchID == d.branchID select w.branchID as branchID, w.amount as withdrawals, d.amount as deposits having w.amount > d.amount * 0.95 For the different types of joins you can perform via Siddhi logic, see Siddhi Query Guide - Join Enrich data by connecting with external services \u00b6 This section explains how to enrich the data in a specific stream by connecting with an external service and adding information received from that service to the existing data. To understand how this is done, consider an example where you have some credit card numbers, but need to connect with an external service to identify the credit card companies that issued them, and then save that information in a database. To do this, follow the procedure below. Before you begin: To save the enriched information in a database table, install and set up MySQL, and create a database. Then create a table in that database named CCInfoTable . For detailed instructions, see the Enrich data by connecting with a data store section . If you want to try out this guide, you can access the external service used in the example here . This service returns the credit card type when a credit card number is submitted. Click here for an example Request Details POST /ArgoFire/validate.asmx/GetCardType HTTP/1.1 Host: secure.ftipgw.com Content-Type: application/x-www-form-urlencoded Content-Length: length CardNumber=4111111111111111 - **Response Details** HTTP/1.1 200 OK Content-Type: text/xml; charset=utf-8 Content-Length: length <?xml version=\"1.0\" encoding=\"utf-8\"?> <string xmlns=\"http://localhost/SmartPayments/\">VISA> Start creating a new Siddhi application. You can name it CCTypeIdentificationApp For instructions, see Creating a Siddhi Application . Define the input stream from which the input data (i.e., the credit card no in this example) must be taken. define stream CreditCardStream (creditCardNo string); To publish the input data to the external application, connect a sink to the stream you created as shown below. For more information about publishing information, see the Publishing Data guide . Info Note the following about the above sink definition: - It is assumed that the external application receives requests in HTTP. Therefore, the sink type is http-request . - The publisher.url parameter specifies the URL to which the outgoing events need to be published via HTTP. - For more information about the HTTP transport, see Siddhi Extensions - Siddhi IO HTTP . To capture the response of the external application once it returns the credit card type, define a stream as follows. For more information about consuming data, see the Consuming Data guide . define stream EnrichedCreditCardStream (creditCardNo string,creditCardType string); Assuming that the external application sends its output via the HTTP transport, connect a source of the http type to the EnrichedCreditCardStream stream as follows. For more information about consuming events published to SI, see the Consuming Data guide . @source(type='http-response' ,sink.id='cardTypeSink', @map(type='xml', namespaces = \"xmlns=http://localhost/SmartPayments/\", @attributes(creditCardNo = 'trp:creditCardNo',creditCardType = \".\"))) define stream EnrichedCreditCardInfoStream (creditCardNo string,creditCardType string); Info It is assumed that the external application sends requests in HTTP. Therefore, the source type is http-request . For more information about the HTTP transport, see Siddhi Extensions - Siddhi IO HTTP . To save the response of the external application, define a table named CCInfoTable . define table CCInfoTable (cardNo long, cardType string); To save the data enriched by integrating the information received from the external service, add a Siddhi query as follows. from EnrichedCreditCardInfoStream select * update or insert into CCInfoTable; The above query selects all the attributes in the EnrichedCreditCardInfoStream and inserts them into the CCInfoTable table. If a specific record already exists,the query updates it by replacing the attribute values with the latest values taken from the EnrichedCreditCardInfoStream . The completed Siddhi application is as follows: @App:name(\"CCTypeIdentificationApp\") @sink(type='http-request',publisher.url='https://secure.ftipgw.com/ArgoFire/validate.asmx/GetCardType',method='POST', headers=\"'Content-Type:application/x-www-form-urlencoded'\", sink.id=\"cardTypeSink\", @map(type='keyvalue', @payload(CardNumber='{{creditCardNo}}'))) define stream CreditCardStream (creditCardNo string); @source(type='http-response' ,sink.id='cardTypeSink', @map(type='xml', namespaces = \"xmlns=http://localhost/SmartPayments/\", @attributes(creditCardNo = 'trp:creditCardNo',creditCardType = \".\"))) define stream EnrichedCreditCardInfoStream (creditCardNo string,creditCardType string); from EnrichedCreditCardInfoStream select * update or insert into CCInfoTable; Enrich data using built-in extensions \u00b6 The following is a list of Siddhi extensions with which you can enrich data. Siddhi-execution-streamingml Siddhi-execution-env Siddhi-execution-math Siddhi-execution-string Siddhi-execution-time Siddhi-execution-json","title":"Enriching Data"},{"location":"guides/enriching-data/#enriching-data","text":"","title":"Enriching Data"},{"location":"guides/enriching-data/#introduction","text":"Enriching data involves integrated the data received into a streaming integration flow with data from other medium such as a data store, another data stream, or an external service to derive an expected result. To understand the different ways in which this is done, follow the sections below.","title":"Introduction"},{"location":"guides/enriching-data/#enrich-data-by-connecting-with-a-data-store","text":"This section explains how to enrich the data in a specific stream by joining it with a data store. For this purpose, consider a scenario where you receive sales records generated from multiple locations as events via a system. Before you begin: In this scenario, you need to enrich the sales information you receive based on the records in a database table. You can download and install MySQL, and create this table before you carry out the procedure in this subsection. For detailed instructions to create a database table, expand the following section. Create Table Download and install the MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR ( mysql-connector-java-x.x.xx-bin.jar ) into the <SI_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Add the following configurations for three database tables under the Data Sources Configuration section of the <SI_HOME>/conf/server/deployment.yaml file. - name: UserDataDB description: Datasource used for User Data jndiConfig: name: jdbc/test useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/UserDataDB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To create a database named UserDataDB with a table named UserTable issue the following commands from the terminal: To create the UserDataDB table: mysql> create database UserDataDB; mysql> use UserDataDB; mysql> source <SP_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on UserDataDB.* TO username@localhost identified by \"password\"; To create the UserTable table: create table UserTable ( userId LONG, firstname VARCHAR, lastname VARCHAR, ) Start creating a new Siddhi application. You can name it EnrichingTransactionsApp For instructions, see Creating a Siddhi Application . Define the input stream and the database table that need to be joined as follows. Define the stream as follows. define stream TrasanctionStream (userId long, transactionAmount double, location string); Define the table as follows. define table UserTable (userId long, firstName string, lastName string); Then define the Siddhi query to join the stream and the table, and handle the result as required. Add the from clause as follows with the join key word to join the table and the stream. from TransactionStream as t join UserTable as u on t.userId == u.userId Info Note the following about the from clause: - In this example, the input data is taken from both a stream and a table. You need to assign a unique reference for each of them to allow the query to differentiate between the common attributes. In this example, TransactionStream stream is referred to as t , and the UserTable table is referred to as u . - The join keyword joins the stream and the table together while specifying the unique references. - The condition for the stream and the table to be joined is t.userId == u.userId , which means that for an event to be taken from the TransactionStream for the join, one or more events that have the same value for the userId must exist in the UserTable table and vice versa. To specify how the value for each attribute in the output stream is derived, add a select clause as follows. select t.userId, str:concat( u.firstName, \" \", u.lastName) as userName, transactionAmount, location Info Note the following in the select statement: - The userId attribute name is common to both the stream and the table. Therefore, you need to specify from where this attribute needs gto be taken. Here, you can also specify u.userId instead of t.userId . - You are specifying the output generated to include an attribute named userName . The value for that is derived by concatenating the values of two attributes in the UserTable table (i.e., firstName and lastName attributes) by applying the str:concat() function. Similarly, you can apply any of the range of Siddhi functions available to further enrich the joined output. For more information, see Siddhi Extensions . To infer an output stream into which the enriched data must be directed, add the insert into clause as follows. insert into EnrichedTrasanctionStream; The completed Siddhi application is as follows. @App:name(\"EnrichingTransactionsApp\") define stream TrasanctionStream (userId long, transactionAmount double, location string); define table UserTable (userId long, firstName string, lastName string); from TrasanctionStream as t join UserTable as u on t.userId == u.userId select t.userId, str:concat( u.firstName, \" \", u.lastName) as userName, transactionAmount, location insert into EnrichedTrasanctionStream;","title":"Enrich data by connecting with a data store"},{"location":"guides/enriching-data/#enrich-data-by-connecting-with-another-stream-of-data","text":"This section explains how to enrich the data in a specific stream by joining it with another stream. To understand how this is done, consider a scenario where you receive information about cash withdrawals and cash deposits at different bank branches from two separate applications. Therefore, this two types of information are captured via two separate streams. To compare the withdrawals with deposits and observe whether enough deposits are being made to manage the withdrawals, you need to join both these streams. To do this, follow the procedure below. Start creating a new Siddhi application. You can name it BankTransactionsApp For instructions, see Creating a Siddhi Application . First, define the two input streams via which you are receiving informations about withdrawals and deposits. Define a stream named CashWithdrawalStream to capture information about withdrawals as follows. define stream CashWithdrawalStream(branchID int, amount long); Define a stream named CashDepositsStream to capture information about deposits as follows. define stream CashDepositsStream(branchID string, amount long); Now let's define an output stream to which the combined information from both the input streams need to be directed after the join. @sink(type='log', prefix='Cash withdrawals that go beyond sustainability threshold:') define stream CashFlowStream(branchID string, withdrawalAmount long, depositAmount long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To specify how the join is performed, and how to use the combined information, write a Siddhi query as follows. To perform the join, add the from clause as follows. from CashWithdrawalStream as w join CashDepositStream as d on w.branchID == d.branchID Info Observe the following about the above from clause: - Both the input streams have attributes of the same name. To identify each name, you must specify a reference for each stream. In this example, the reference for the CashWithdrawalStream is w , and the reference for the CashDepositsStream stream is d . - You need to use join as the keyword to join two streams. The join condition is w.branchID == d.branchID where branch IDs are matched. An event in the CashWithdrawalStream stream is directed to the CashFlowStream if there are events with the same branch ID in the CashDepositStream and vice versa. To specify how the value for each attribute is derived, add a select statement as follows. select w.branchID as branchID, w.amount as withdrawals, d.amount as deposits Info The branchID attribute name is common to both input streams. Therefore, you can also specify d.branchID as branchID instead of w.branchId as branchId . To filter only events where total cash withdrawals are greater than 95% of the cash deposits, add a having clause as follows. having w.amount > d.amount * 0.95 To insert the results into the CashFlowStream output stream, add the insert into clause as follows. insert into CashFlowStream; The completed Siddhi application is as follows: @App:name(\"BankTransactionsApp\"); define stream CashWithdrawalStream(branchID int, amount long); define stream CashDepositsStream(branchID string, amount long); @sink(type='log', prefix='Cash withdrawals that go beyond sustainability threshold:') define stream CashFlowStream(branchID string, withdrawalAmount long, depositAmount long); from CashWithdrawalStream as w join CashDepositStream as d on w.branchID == d.branchID select w.branchID as branchID, w.amount as withdrawals, d.amount as deposits having w.amount > d.amount * 0.95 For the different types of joins you can perform via Siddhi logic, see Siddhi Query Guide - Join","title":"Enrich data by connecting with another stream of data"},{"location":"guides/enriching-data/#enrich-data-by-connecting-with-external-services","text":"This section explains how to enrich the data in a specific stream by connecting with an external service and adding information received from that service to the existing data. To understand how this is done, consider an example where you have some credit card numbers, but need to connect with an external service to identify the credit card companies that issued them, and then save that information in a database. To do this, follow the procedure below. Before you begin: To save the enriched information in a database table, install and set up MySQL, and create a database. Then create a table in that database named CCInfoTable . For detailed instructions, see the Enrich data by connecting with a data store section . If you want to try out this guide, you can access the external service used in the example here . This service returns the credit card type when a credit card number is submitted. Click here for an example Request Details POST /ArgoFire/validate.asmx/GetCardType HTTP/1.1 Host: secure.ftipgw.com Content-Type: application/x-www-form-urlencoded Content-Length: length CardNumber=4111111111111111 - **Response Details** HTTP/1.1 200 OK Content-Type: text/xml; charset=utf-8 Content-Length: length <?xml version=\"1.0\" encoding=\"utf-8\"?> <string xmlns=\"http://localhost/SmartPayments/\">VISA> Start creating a new Siddhi application. You can name it CCTypeIdentificationApp For instructions, see Creating a Siddhi Application . Define the input stream from which the input data (i.e., the credit card no in this example) must be taken. define stream CreditCardStream (creditCardNo string); To publish the input data to the external application, connect a sink to the stream you created as shown below. For more information about publishing information, see the Publishing Data guide . Info Note the following about the above sink definition: - It is assumed that the external application receives requests in HTTP. Therefore, the sink type is http-request . - The publisher.url parameter specifies the URL to which the outgoing events need to be published via HTTP. - For more information about the HTTP transport, see Siddhi Extensions - Siddhi IO HTTP . To capture the response of the external application once it returns the credit card type, define a stream as follows. For more information about consuming data, see the Consuming Data guide . define stream EnrichedCreditCardStream (creditCardNo string,creditCardType string); Assuming that the external application sends its output via the HTTP transport, connect a source of the http type to the EnrichedCreditCardStream stream as follows. For more information about consuming events published to SI, see the Consuming Data guide . @source(type='http-response' ,sink.id='cardTypeSink', @map(type='xml', namespaces = \"xmlns=http://localhost/SmartPayments/\", @attributes(creditCardNo = 'trp:creditCardNo',creditCardType = \".\"))) define stream EnrichedCreditCardInfoStream (creditCardNo string,creditCardType string); Info It is assumed that the external application sends requests in HTTP. Therefore, the source type is http-request . For more information about the HTTP transport, see Siddhi Extensions - Siddhi IO HTTP . To save the response of the external application, define a table named CCInfoTable . define table CCInfoTable (cardNo long, cardType string); To save the data enriched by integrating the information received from the external service, add a Siddhi query as follows. from EnrichedCreditCardInfoStream select * update or insert into CCInfoTable; The above query selects all the attributes in the EnrichedCreditCardInfoStream and inserts them into the CCInfoTable table. If a specific record already exists,the query updates it by replacing the attribute values with the latest values taken from the EnrichedCreditCardInfoStream . The completed Siddhi application is as follows: @App:name(\"CCTypeIdentificationApp\") @sink(type='http-request',publisher.url='https://secure.ftipgw.com/ArgoFire/validate.asmx/GetCardType',method='POST', headers=\"'Content-Type:application/x-www-form-urlencoded'\", sink.id=\"cardTypeSink\", @map(type='keyvalue', @payload(CardNumber='{{creditCardNo}}'))) define stream CreditCardStream (creditCardNo string); @source(type='http-response' ,sink.id='cardTypeSink', @map(type='xml', namespaces = \"xmlns=http://localhost/SmartPayments/\", @attributes(creditCardNo = 'trp:creditCardNo',creditCardType = \".\"))) define stream EnrichedCreditCardInfoStream (creditCardNo string,creditCardType string); from EnrichedCreditCardInfoStream select * update or insert into CCInfoTable;","title":"Enrich data by connecting with external services"},{"location":"guides/enriching-data/#enrich-data-using-built-in-extensions","text":"The following is a list of Siddhi extensions with which you can enrich data. Siddhi-execution-streamingml Siddhi-execution-env Siddhi-execution-math Siddhi-execution-string Siddhi-execution-time Siddhi-execution-json","title":"Enrich data using built-in extensions"},{"location":"guides/fault-Handling/","text":"Fault Handling \u00b6 Siddhi allows you to manage any faults that may occur when handling streaming data in a graceful manner. This section explains the different ways in which the faults can be handled gracefully. Handling runtime errors \u00b6 To specify how errors that occur at runtime, you need to add an @OnError annotation to a stream definition as shown below. @OnError(action='on_error_action') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The on_error_action parameter specifies the action to be executed during failure scenarios. The possible action types are as follows: LOG : This logs the event with an error, and then drops the event. If you do not specify the fault handling actionvia the @OnError annotation, LOG is considered the default action. STREAM : This automatically creates a fault stream for the base stream. The definition of the fault stream includes all the attributes of the base stream as well as an additional attribute named _error . The events are inserted into the fault stream during a failure. The error identified is captured as the value for the _error attribute. e.g., The following is a Siddhi application that includes the @OnError annotation to handle failures during runtime. @OnError(name='STREAM') define stream StreamA (symbol string, volume long); from StreamA[custom:fault() > volume] insert into StreamB; from !StreamA#log(\"Error Occured\") select symbol, volume long, _error insert into tempStream; Here, if an error occurs for the base stream named StreamA , a stream named !StreamA is automatically created. The base stream has two attributes named symbol and volume. Therefore, !StreamA has the same two attributes, and in addition, another attribute named _error. The Siddhi query uses the custom:fault() extension generates the error detected bsed on the specified condition (i.e., if the volume is less than a specified amount). If no error is detected, the output is inserted into StreamB stream. However, if an error is detected, it is logged with the Error Occured text. The output is inserted into a stream named tempStream , and the error details are presented via the _error stream attribute (which is automatically included in the !StreamA i fault stream and then inserted into the TempStream which is the inferred output stream).. Handling errors that occur when publishing the output \u00b6 To specify the error handling methods for errors that occur at the time the output is published, you can include the on.error parameter in the sink configuration as shown below. @sink(type='sink_type', on.error='on.error.action' ) define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The action types that can be specified via the on.error parameter when configuring a sink are as follows. If this parameter is not included in the sink configuration, LOG is the action type by default. LOG : Logs the event with the error, and then drops the event. WAIT : The thread waits in the back-off and re-trying state, and reconnects once the connection is re-established. STREAM : The corresponding fault stream is populated with the failed event and the error that occured while publishing.","title":"Fault Handling"},{"location":"guides/fault-Handling/#fault-handling","text":"Siddhi allows you to manage any faults that may occur when handling streaming data in a graceful manner. This section explains the different ways in which the faults can be handled gracefully.","title":"Fault Handling"},{"location":"guides/fault-Handling/#handling-runtime-errors","text":"To specify how errors that occur at runtime, you need to add an @OnError annotation to a stream definition as shown below. @OnError(action='on_error_action') define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The on_error_action parameter specifies the action to be executed during failure scenarios. The possible action types are as follows: LOG : This logs the event with an error, and then drops the event. If you do not specify the fault handling actionvia the @OnError annotation, LOG is considered the default action. STREAM : This automatically creates a fault stream for the base stream. The definition of the fault stream includes all the attributes of the base stream as well as an additional attribute named _error . The events are inserted into the fault stream during a failure. The error identified is captured as the value for the _error attribute. e.g., The following is a Siddhi application that includes the @OnError annotation to handle failures during runtime. @OnError(name='STREAM') define stream StreamA (symbol string, volume long); from StreamA[custom:fault() > volume] insert into StreamB; from !StreamA#log(\"Error Occured\") select symbol, volume long, _error insert into tempStream; Here, if an error occurs for the base stream named StreamA , a stream named !StreamA is automatically created. The base stream has two attributes named symbol and volume. Therefore, !StreamA has the same two attributes, and in addition, another attribute named _error. The Siddhi query uses the custom:fault() extension generates the error detected bsed on the specified condition (i.e., if the volume is less than a specified amount). If no error is detected, the output is inserted into StreamB stream. However, if an error is detected, it is logged with the Error Occured text. The output is inserted into a stream named tempStream , and the error details are presented via the _error stream attribute (which is automatically included in the !StreamA i fault stream and then inserted into the TempStream which is the inferred output stream)..","title":"Handling runtime errors"},{"location":"guides/fault-Handling/#handling-errors-that-occur-when-publishing-the-output","text":"To specify the error handling methods for errors that occur at the time the output is published, you can include the on.error parameter in the sink configuration as shown below. @sink(type='sink_type', on.error='on.error.action' ) define stream <stream name> (<attribute name> <attribute type>, <attribute name> <attribute type>, ... ); The action types that can be specified via the on.error parameter when configuring a sink are as follows. If this parameter is not included in the sink configuration, LOG is the action type by default. LOG : Logs the event with the error, and then drops the event. WAIT : The thread waits in the back-off and re-trying state, and reconnects once the connection is re-established. STREAM : The corresponding fault stream is populated with the failed event and the error that occured while publishing.","title":"Handling errors that occur when publishing the output"},{"location":"guides/publishing-data/","text":"Publishing Data \u00b6 Introduction \u00b6 Once information is processed by the Streaming Integrator, the output is presented as events in a streaming manner. This output can be published to databases, files, cloud-based applications or other streaming applications. For the Streaming Integrator to publish events, the following is required. A message schema: The messages selected to be published by a streaming integration flow are identified by their schemas. This schema is defined via an output stream definition. A sink: The output can be published to different interfaces including streaming applications, cloud-based applications, databases, and files. There are different sink types to support the different interfaces. The output can also be published in a range of formats. In order to select the required interface and format for a specific streaming integration flow, you need to configure a sink in the relevant Siddhi application via the @sink annotation. As shown in the image above, a sink configuration consists of three parts. @sink : This annotation defines the sink type via which the data is published, and allows you to configure the sink parameters (which change depending on the sink type). For the complete list of supported sink types, see Siddhi Query Guide - Sink . @map : This annotation specifies the format in which the data is published, and allows you to configure the mapping parameters (which change based of the mapping type/format selected). For the complete list of supported mapping types, see Siddhi Query Guide - Sink Mapper . @attributes : This annotation specifies a custom mapping based on which events in the streaming integration flow that need to be published are identified. This is useful when the attributes of the output messages you want the Streaming Integrator to publish are different to the corresponding attribute name in the stream definition. e.g., In a scenario where the Streaming Integrator is publishing the average temperature per second, the temperature can be referred to as avgTemp in the output stream definition in your Siddhi application. However, you want to publish it with the Temperature to the streaming application to which you are publishing. In this instance, you need a custom mapping to indicate that Temperature is the same as avgTemp . Publishing data using an event sink \u00b6 This section explains how to configure a basic sink without mapping. A Siddhi application can contain a sink configuration inline, or refer to a sink configuration that is defined externally in a configuration file. Defining event sink inline in the Siddhi application \u00b6 To create a Siddhi application with the sink configuration defined inline, follow the steps below. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) e.g., @App:name(\"SalesTotalsApp\") Define the output stream based on the schema in which you want to publish data. The format is as follows. define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Connect a sink to the stream definition you added as follows. @sink(type='<SINK_TYPE>') define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); Here, the sink type needs to be selected based on the interface to which you want to publish the output. For more information, see Supported sink types . e.g., If you want to publish the output as logs in the console, you can add a sink with log as the type. @sink(type='log') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add and configure parameters related to the sink type you selected as shown below. @sink(type='<SINK_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', ...) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., By adding a parameter named prefix to the log sink used as an example in the previous step, you can specify a prefix with which you want to see the output logs printed. @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Now let's complete adding the required Siddhi constructs to receive and process the input data. Add an input stream with a connected source configuration as shown below. For more information, see the Consuming Data guide . e.g., Assuming that the schema of the input events are same as that of the output events, and that they are received via HTTP, you can add the input stream definition @source(type='http', receiver.url='http://localhost:5005/SweetProductionEP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add a query to get the received events from the input stream and direct them to the output stream as follows. from <INPUT_STREAM_NAME> select <ATTRIBUTE1_Name>, <ATTRIBUTE2_NAME>, ... group by <ATTRIBUTE_NAME> insert into <OUTPUT_STREAM_NAME>; e.g., Assuming that you are publishing the events with the existing values as logs in the output console without any further processing, you can define the query as follows. from ConsumeSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; Save the Siddhi Application. Defining event sink externally in the configuration file \u00b6 If you want to use the same sin k configuration in multiple Siddhi applications, you can define it externally in the <SI_HOME>/conf/server/deployment.yaml file and then refer to it from Siddhi applications. To understand how to do this, follow the procedure below. Open the <SI_HOME>/conf/server/deployment.yaml file. Add a section named siddi , and then add a subsection named refs: as shown below. siddhi: refs: - In the refs subsection, enter a parameter named name and enter a name for the sink. siddhi: refs: - name:`<SINK_NAME>` To specify the sink type, add another parameter named type and enter the relevant sink type. siddhi: refs: - name:'<SINK_NAME>' type: '<SINK_TYPE>' To configure other parameters for the sink (based on the sink type), add a subsection named properties as shown below. siddhi: refs: - name:'SINK_NAME' type: '<SINK_TYPE>' properties <PROPERTY1_NAME>:'<PROPERTY1_VALUE>' <PROPERTY2_NAME>:'<PROPERTY2_VALUE>' ... Save the configuration file. e.g., The log sink used as the example in the previous section can be defined externally as follows: siddhi: refs: - name:'LogSink' type: 'log' properties prefix:'Sales Totals' Supported event sink types \u00b6 Supported event formats \u00b6 Publishing data in default format \u00b6 SI publishes events in default format when it does not make any changes to the attribute names in the output stream before publishing. To understand how this is done, follow the procedure below: Create a Siddhi application with a sink configuration following the instructions in the Defining event sink inline in the Siddhi application section. Add an @map annotation with the mapping type to the sink configuration as shown below. @sink(type='<SINK_TYPE>', @map(type='MAP_TYPE')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); The map type specifies the format in which the events are published. e.g., In the example that you used, you can specify the output logs to be printed in the text format by specifying text as the mapping type. @sink(type='log', prefix='Sales Totals:', @map(type=text)) define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi application. If you save the Siddhi application that was created using the example configurations, the completed Siddhi application is as follows. @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:', @map(type=text)) define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select transNo, product, price, quantity, salesValue group by product insert into PublishSalesTotalsStream; Publishing data in custom format \u00b6 SI publishes data in the custom format when it makes changes to the attribute names in the output stream before publishing. To understand how this is done, follow the procedure below: Info In this section, you can update the same Siddhi application that you saved in the Publishing data in default format section. Open your Siddhi application with a sink configuration. Within the @map annotation of the sink configuration, add an @payload annotation. There are two ways to configure this as follows: Some mappers such as xml , json , and text accept only one output payload using the following format: @payload( '<PAYLOAD>' ) e.g., In the example, the mapping type is text . Therefore, you can add a message to be printed with the output by configuring the @payload annotation as follows. @payload( 'This is a test message from {{user}}.' ) Some mappers such as key-value accept series of mapping values defined as follows: @payload( key1='mapping_1', 'key2'='user : {{user}}') Save the Siddhi application.","title":"Publishing Data"},{"location":"guides/publishing-data/#publishing-data","text":"","title":"Publishing Data"},{"location":"guides/publishing-data/#introduction","text":"Once information is processed by the Streaming Integrator, the output is presented as events in a streaming manner. This output can be published to databases, files, cloud-based applications or other streaming applications. For the Streaming Integrator to publish events, the following is required. A message schema: The messages selected to be published by a streaming integration flow are identified by their schemas. This schema is defined via an output stream definition. A sink: The output can be published to different interfaces including streaming applications, cloud-based applications, databases, and files. There are different sink types to support the different interfaces. The output can also be published in a range of formats. In order to select the required interface and format for a specific streaming integration flow, you need to configure a sink in the relevant Siddhi application via the @sink annotation. As shown in the image above, a sink configuration consists of three parts. @sink : This annotation defines the sink type via which the data is published, and allows you to configure the sink parameters (which change depending on the sink type). For the complete list of supported sink types, see Siddhi Query Guide - Sink . @map : This annotation specifies the format in which the data is published, and allows you to configure the mapping parameters (which change based of the mapping type/format selected). For the complete list of supported mapping types, see Siddhi Query Guide - Sink Mapper . @attributes : This annotation specifies a custom mapping based on which events in the streaming integration flow that need to be published are identified. This is useful when the attributes of the output messages you want the Streaming Integrator to publish are different to the corresponding attribute name in the stream definition. e.g., In a scenario where the Streaming Integrator is publishing the average temperature per second, the temperature can be referred to as avgTemp in the output stream definition in your Siddhi application. However, you want to publish it with the Temperature to the streaming application to which you are publishing. In this instance, you need a custom mapping to indicate that Temperature is the same as avgTemp .","title":"Introduction"},{"location":"guides/publishing-data/#publishing-data-using-an-event-sink","text":"This section explains how to configure a basic sink without mapping. A Siddhi application can contain a sink configuration inline, or refer to a sink configuration that is defined externally in a configuration file.","title":"Publishing data using an event sink"},{"location":"guides/publishing-data/#defining-event-sink-inline-in-the-siddhi-application","text":"To create a Siddhi application with the sink configuration defined inline, follow the steps below. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) e.g., @App:name(\"SalesTotalsApp\") Define the output stream based on the schema in which you want to publish data. The format is as follows. define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Connect a sink to the stream definition you added as follows. @sink(type='<SINK_TYPE>') define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); Here, the sink type needs to be selected based on the interface to which you want to publish the output. For more information, see Supported sink types . e.g., If you want to publish the output as logs in the console, you can add a sink with log as the type. @sink(type='log') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add and configure parameters related to the sink type you selected as shown below. @sink(type='<SINK_TYPE>', <PARAMETER1_NAME>='<PARAMETER1_VALUE>', ...) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); e.g., By adding a parameter named prefix to the log sink used as an example in the previous step, you can specify a prefix with which you want to see the output logs printed. @sink(type='log', prefix='Sales Totals:') define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Now let's complete adding the required Siddhi constructs to receive and process the input data. Add an input stream with a connected source configuration as shown below. For more information, see the Consuming Data guide . e.g., Assuming that the schema of the input events are same as that of the output events, and that they are received via HTTP, you can add the input stream definition @source(type='http', receiver.url='http://localhost:5005/SweetProductionEP') define stream ConsumeSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Add a query to get the received events from the input stream and direct them to the output stream as follows. from <INPUT_STREAM_NAME> select <ATTRIBUTE1_Name>, <ATTRIBUTE2_NAME>, ... group by <ATTRIBUTE_NAME> insert into <OUTPUT_STREAM_NAME>; e.g., Assuming that you are publishing the events with the existing values as logs in the output console without any further processing, you can define the query as follows. from ConsumeSalesTotalsStream select * group by product insert into PublishSalesTotalsStream; Save the Siddhi Application.","title":"Defining event sink inline in the Siddhi application"},{"location":"guides/publishing-data/#defining-event-sink-externally-in-the-configuration-file","text":"If you want to use the same sin k configuration in multiple Siddhi applications, you can define it externally in the <SI_HOME>/conf/server/deployment.yaml file and then refer to it from Siddhi applications. To understand how to do this, follow the procedure below. Open the <SI_HOME>/conf/server/deployment.yaml file. Add a section named siddi , and then add a subsection named refs: as shown below. siddhi: refs: - In the refs subsection, enter a parameter named name and enter a name for the sink. siddhi: refs: - name:`<SINK_NAME>` To specify the sink type, add another parameter named type and enter the relevant sink type. siddhi: refs: - name:'<SINK_NAME>' type: '<SINK_TYPE>' To configure other parameters for the sink (based on the sink type), add a subsection named properties as shown below. siddhi: refs: - name:'SINK_NAME' type: '<SINK_TYPE>' properties <PROPERTY1_NAME>:'<PROPERTY1_VALUE>' <PROPERTY2_NAME>:'<PROPERTY2_VALUE>' ... Save the configuration file. e.g., The log sink used as the example in the previous section can be defined externally as follows: siddhi: refs: - name:'LogSink' type: 'log' properties prefix:'Sales Totals'","title":"Defining event sink externally in the configuration file"},{"location":"guides/publishing-data/#supported-event-sink-types","text":"","title":"Supported event sink types"},{"location":"guides/publishing-data/#supported-event-formats","text":"","title":"Supported event formats"},{"location":"guides/publishing-data/#publishing-data-in-default-format","text":"SI publishes events in default format when it does not make any changes to the attribute names in the output stream before publishing. To understand how this is done, follow the procedure below: Create a Siddhi application with a sink configuration following the instructions in the Defining event sink inline in the Siddhi application section. Add an @map annotation with the mapping type to the sink configuration as shown below. @sink(type='<SINK_TYPE>', @map(type='MAP_TYPE')) define stream <Stream_Name>(attribute1_name attribute1_type, attribute2_name attribute2_type, ...); The map type specifies the format in which the events are published. e.g., In the example that you used, you can specify the output logs to be printed in the text format by specifying text as the mapping type. @sink(type='log', prefix='Sales Totals:', @map(type=text)) define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); Save the Siddhi application. If you save the Siddhi application that was created using the example configurations, the completed Siddhi application is as follows. @App:name(\"SalesTotalsApp\") @App:description(\"Description of the plan\") @source(type='http', receiver.url='http://localhost:5005/SalesTotalsEP', @map(type='json')) define stream ConsumerSalesTotalsStream(transNo int, product string, price int, quantity int, salesValue long); @sink(type='log', prefix='Sales Totals:', @map(type=text)) define stream PublishSalesTotalsStream (transNo int, product string, price int, quantity int, salesValue long); from ConsumerSalesTotalsStream select transNo, product, price, quantity, salesValue group by product insert into PublishSalesTotalsStream;","title":"Publishing data in default format"},{"location":"guides/publishing-data/#publishing-data-in-custom-format","text":"SI publishes data in the custom format when it makes changes to the attribute names in the output stream before publishing. To understand how this is done, follow the procedure below: Info In this section, you can update the same Siddhi application that you saved in the Publishing data in default format section. Open your Siddhi application with a sink configuration. Within the @map annotation of the sink configuration, add an @payload annotation. There are two ways to configure this as follows: Some mappers such as xml , json , and text accept only one output payload using the following format: @payload( '<PAYLOAD>' ) e.g., In the example, the mapping type is text . Therefore, you can add a message to be printed with the output by configuring the @payload annotation as follows. @payload( 'This is a test message from {{user}}.' ) Some mappers such as key-value accept series of mapping values defined as follows: @payload( key1='mapping_1', 'key2'='user : {{user}}') Save the Siddhi application.","title":"Publishing data in custom format"},{"location":"guides/storage-Integration/","text":"Storage Integration \u00b6 Introduction \u00b6 This section explains how to integrate data stores into Streaming Integration flows. The Streaming integrator can consume data from stores in a streaming manner as well as publish data into them. You can perform CRUD operations and Change Data Capture (CDC). Configuring stores \u00b6 You can store data in-memory or in a physical data store. In a production environment, it is recommended to store data in a physical store because data stored in-memory can be lost if a system failure occurs. If you want to share a database across multiple Siddhi applications, you must define a data store in the <SI_HOME>/conf/server/deployment.yaml file. If you want a single connection pool, you can define a store as a ref in the ref section of the <SI_HOME>/conf/server/deployment.yaml file. If you want to define a unique data store for your Siddhi application, you can define it inline. To understand all three methods consider an example where a factory manager wants to store the purchase records of raw material supplies to be stored in order to able to check the availability of materials at any given time. Storing data in existing data sources \u00b6 To understand how to store data in existing data sources, follow the procedure below: Info You need to create a database and a table, and then connect it to the Streaming Integrator via a data source. For this example, you can create a database named FactoryMaterialDB as follows: Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Add the following configuration under the Data Sources Configuration section of the <SI_HOME>/conf/server/deployment.yaml file. Info You need to change the values for the username and password parameters to the username and password that you are using to access the MySQL database. - name: FactoryMaterialDB description: Datasource used for Factory Supply Records jndiConfig: name: jdbc/FactoryMaterialDB useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/FactoryMaterialDB' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect the table you defined to the datasource that you previously created. @Store(type='rdbms', datasource='FactoryMaterialDB')@PrimaryKey(\"transRef\") define table ShipmentDetails(transRef long, material string, supplier string, amount double); To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record. Referring to externally defined stores \u00b6 To understand how to refer to am externally defined store, follow the procedure below: Before you begin: You need to create a database and a table, and then define an external store referring to it as follows. Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the <SI_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. In the <SI_HOME>/conf/server/deployment.yaml file, add a subsection for refs and connect to the database you created as shown below. siddhi: refs: - ref: name: 'FactoryMaterial' type: 'rdbms' properties: jdbc.url: ''jdbc:mysql://localhost:3306/FactoryMaterialDB'' username: '<YOUR_USERNAME>' password: '<YOUR_PASSWORD>' field.length='currentTime:100' jdbc.driver.name: 'com.mysql.jdbc.Driver' To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect the table you defined to the ref that you previously created. @Store(ref='FactoryMaterial') @PrimaryKey('transRef') define table ShipmentDetails(transRef long, material string, supplier string, amount double); Here, you are referring to the FactoryMaterial external store you configured and specifying the transRef attribute as the primary key. To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record. Configuring data stores inline \u00b6 To understand how to define a store inline, follow the procedure below: Before you begin You need to create a database and a table, and then define an external store referring to it as follows. Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect an inline store definition to the table definition via the @store annotation as follows @primaryKey('transRef') @index('supplier') @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/FactoryMaterialDB, username=\"<YOUR_USERNAME>\", password=\"<YOUR_PASSWORD>\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table ShipmentDetails(transRef long, material string, supplier string, amount double); Here, the jdbc.url parameter connects the ShipmentDetails table to the FactoryMaterialDB database that you previously created. The value for the transRef parameter is unique for each purchase event. Therefore, it is specified as the primary key for the table. The records are indexed by the supplier. To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record. Performing CRUD operations \u00b6 Performing CRUD operations via streams \u00b6 If you tried out the example in the previous sections of this guide, you have already performed an update or insert operation via streams when you updated or inserted a record relating to the purchase records into the ShipmentDetails table. In this section, you can try more CRUD operations via streams as follows: If you want to insert all the records from the RawMaterialStream stream to the ShipmentDetails table, you can write the Siddhi query as follows. from RawMaterialStream select * insert into ShipmentDetails If you want to search for all the records by a specific supplier, you can write the Siddhi query as follows. from RawMaterialStream on supplier == 'Lakspur landing' in ShipmentDetails select * insert into SearchResultsStream Here, you search for records by entering the search condition in the from clause and directing the filtered results into an output stream named SearchResultsStream . If you want to delete all the records for a specific material, you can write the Siddhi query as follows: from DeleteMaterialStream on material == 'honey' in ShipmentDetails select * delete ShipmentDeails on ShipmentDetails.material == matarial; Performing CRUD operations via REST API \u00b6 This section explains how to use REST API to perform the same CRUD operations that you previously performed via streams. Before you begin: The Siddhi store query endpoint can be configured as follows: In the siddhi.stores.query.api: section of the <SI_HOME>/conf/server/deployment.yaml file, configure the following properties. The following is a sample configuration with default values. siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7070 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 7443 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon transport properties server.bootstrap.socket.timeout : The number of seconds after which the connection socket of the bootstrap server times out. client.bootstrap.socket.timeout : The number of seconds after which the connection socket of the bootstrap server times out. latency.metrics.enabled : If this is set to true , the latency metrics are enabled and logged for the HTTP transport. listenerConfigurations : Multiple listeners can be configured as shown in the above sample. id : A unique ID for the listener. host : The host of the listener. port : The port of the listener. scheme : This specifies whether the transport scheme is HTTP or HTTPS. keyStoreFile : If the transport scheme is HTTPS, this parameter specifies the path to the key store file. keyStorePassword : If the transport scheme is HTTPS, this parameter specifies the key store password. In the siddhi.stores.query.api: section of the <SI_HOME>/conf/server/deployment.yaml file, the following properties are configured by default: siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7370 The same parameter descriptions provided for the <SI_HOME>/conf/server/deployment.yaml file apply to this configuration. Inserting records \u00b6 This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select <attribute name>, <attribute name>, ... insert into <table>; Sample CURL command The following CURL command submits a query that inserts a new record with the specified attribute values to a table named RoomOccupancyTable . For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k Retrieving records \u00b6 This is the store query to retrieve records from a table or a window. Retrieving records from tables and windows \u00b6 Syntax from <table/window> <on condition>? select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Sample CURL command The following CURL command submits a query that retrieves room numbers and types of the rooms starting from room no 10 , from a table named roomTypeTable . The roomTypeTable table must be defined in the RoomService Siddhi application. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo >= 10 select roomNo, type; \" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo >= 10 select roomNo, type; \" }' -k Sample response The following is a sample response to the sample CURL command given above. {\"records\":[ [10,\"single\"], [11, \"triple\"], [12, \"double\"] ]} Retrieving records from aggregations \u00b6 This is the store query to retrieve records from an aggregation. Syntax from <aggregation> <on condition>? within <time range> per <time granularity> select <attribute name>, <attribute name>, ... <groupby>? <having>? <order by>? <limit>? Sample CURL command The following CURL command submits a query that retrieves average price of a stock. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k Sample response The following is a sample response to the sample CURL command given above. {\"records\":[ [1531180800, 'FB', 10000.0, 250.0], [1531184400, 'FB', 11000.0, 260.0], [1531188000, 'FB', 9000.0, 240.0] ]} Updating records \u00b6 This store query updates selected attributes stored in a specific table based on a given condition. Syntax select <attribute name>, <attribute name>, ...? update <table> set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> Sample CURL command The following CURL command updates the room occupancy for selected records in the table named RoomOccupancyTable . The records that are updated are ones of which the roon number is greater than 10 . The room occupancy is updated by adding 1 to the existing value of the people attribute. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k Deleting records \u00b6 This store query deletes selected records from a specified table. Syntax <select>? delete <table> on <conditional expresssion> Sample CURL command The following CURL command submits a query that deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection that has 10 as the actual value. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k Inserting/updating records \u00b6 Syntax select <attribute name>, <attribute name>, ... update or insert into <table> set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> Sample CURL command The following CURL command submits a query that attempts to update selected records in the RoomAssigneeTable table. The records that are selected to be updated are ones with room numbers that match the numbers specified in the select clause. If matching records are not found, it inserts a new record with the values provided in the select clause. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For more information, see Siddhi Query Guide - Store Query . Manupulating data in multiple tables \u00b6 The Streaming Integrator allows you to perform CRUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension . Performing CRUD operations for multiple tables In order to perform CRUD operations, the system parameter named perform.CRUD.operations needs to be set to true in the deployment.yaml file. The syntax for a Siddhi query to perform a CRUD operation in multiple tables is as follows. from TriggerStream#rdbms:cud(<STRING> datasource.name, <STRING> query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a data source named, SAMPLE_DB a Siddhi query can be written as follows. from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream Retrieving data from multiple tables In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query(<STRING> dataource.name, <STRING> query, <STRING> stream definition) select <attributes> insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows. from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderData insert into OutputStream;","title":"Integrating Stores"},{"location":"guides/storage-Integration/#storage-integration","text":"","title":"Storage Integration"},{"location":"guides/storage-Integration/#introduction","text":"This section explains how to integrate data stores into Streaming Integration flows. The Streaming integrator can consume data from stores in a streaming manner as well as publish data into them. You can perform CRUD operations and Change Data Capture (CDC).","title":"Introduction"},{"location":"guides/storage-Integration/#configuring-stores","text":"You can store data in-memory or in a physical data store. In a production environment, it is recommended to store data in a physical store because data stored in-memory can be lost if a system failure occurs. If you want to share a database across multiple Siddhi applications, you must define a data store in the <SI_HOME>/conf/server/deployment.yaml file. If you want a single connection pool, you can define a store as a ref in the ref section of the <SI_HOME>/conf/server/deployment.yaml file. If you want to define a unique data store for your Siddhi application, you can define it inline. To understand all three methods consider an example where a factory manager wants to store the purchase records of raw material supplies to be stored in order to able to check the availability of materials at any given time.","title":"Configuring stores"},{"location":"guides/storage-Integration/#storing-data-in-existing-data-sources","text":"To understand how to store data in existing data sources, follow the procedure below: Info You need to create a database and a table, and then connect it to the Streaming Integrator via a data source. For this example, you can create a database named FactoryMaterialDB as follows: Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Add the following configuration under the Data Sources Configuration section of the <SI_HOME>/conf/server/deployment.yaml file. Info You need to change the values for the username and password parameters to the username and password that you are using to access the MySQL database. - name: FactoryMaterialDB description: Datasource used for Factory Supply Records jndiConfig: name: jdbc/FactoryMaterialDB useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/FactoryMaterialDB' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect the table you defined to the datasource that you previously created. @Store(type='rdbms', datasource='FactoryMaterialDB')@PrimaryKey(\"transRef\") define table ShipmentDetails(transRef long, material string, supplier string, amount double); To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record.","title":"Storing data in existing data sources"},{"location":"guides/storage-Integration/#referring-to-externally-defined-stores","text":"To understand how to refer to am externally defined store, follow the procedure below: Before you begin: You need to create a database and a table, and then define an external store referring to it as follows. Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the <SI_HOME>/lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. In the <SI_HOME>/conf/server/deployment.yaml file, add a subsection for refs and connect to the database you created as shown below. siddhi: refs: - ref: name: 'FactoryMaterial' type: 'rdbms' properties: jdbc.url: ''jdbc:mysql://localhost:3306/FactoryMaterialDB'' username: '<YOUR_USERNAME>' password: '<YOUR_PASSWORD>' field.length='currentTime:100' jdbc.driver.name: 'com.mysql.jdbc.Driver' To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect the table you defined to the ref that you previously created. @Store(ref='FactoryMaterial') @PrimaryKey('transRef') define table ShipmentDetails(transRef long, material string, supplier string, amount double); Here, you are referring to the FactoryMaterial external store you configured and specifying the transRef attribute as the primary key. To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record.","title":"Referring to externally defined stores"},{"location":"guides/storage-Integration/#configuring-data-stores-inline","text":"To understand how to define a store inline, follow the procedure below: Before you begin You need to create a database and a table, and then define an external store referring to it as follows. Download and install MySQL Server . Download the MySQL JDBC driver . Unzip the downloaded MySQL driver zipped archive, and copy the MySQL JDBC driver JAR (mysql-connector-java-x.x.xx-bin.jar) into the /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. To create a database table named FactoryMaterialDB , issue the following commands from the terminal. mysql> create database FactoryMaterialDB; mysql> use FactoryMaterialDB; mysql> source <SI_HOME>/wso2/editor/dbscripts/metrics/mysql.sql; mysql> grant all on FactoryMaterialDB.* TO username@localhost identified by \"password\"; Start creating a new Siddhi application. You can name it ShipmentHistoryApp For instructions, see Creating a Siddhi Application . @App:name('ShipmentHistory'); First, define an input stream to capture the data that you need to store as follows. @source(type = 'http', @map(type = 'json')) define stream RawMaterialStream(transRef long, material string, supplier string, amount double); Here, each event representing a material shipment transaction is received via the http transport and in JSON format. This is specified via the source annotation. For more information about consuming data via sources, see the Consuming Data guide . Now let's define the table in which you are storing the data as follows. define table ShipmentDetails(transRef long, material string, supplier string, amount double); To save the data in the required database, connect an inline store definition to the table definition via the @store annotation as follows @primaryKey('transRef') @index('supplier') @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/FactoryMaterialDB, username=\"<YOUR_USERNAME>\", password=\"<YOUR_PASSWORD>\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define table ShipmentDetails(transRef long, material string, supplier string, amount double); Here, the jdbc.url parameter connects the ShipmentDetails table to the FactoryMaterialDB database that you previously created. The value for the transRef parameter is unique for each purchase event. Therefore, it is specified as the primary key for the table. The records are indexed by the supplier. To store the information arriving at the RawMaterialStream stream to the ShipmentDetails table, write a Siddhi query as follows: To specify that events to be stored are taken from the RawMaterialStream input stream, add the from clause as follows. from RawMaterialStream To derive the exact attribute names with which the records are stored in the table, add the select statement as follows. select transRef, material, supplier If a record from the stream already exists in a the ShipmentDetails table, it should be updated with the latest information from the stream. If it does not already exist, it must be inserted as a new record. To specify this, add an update and insert clause as follows. update or insert into ShipmentDetails on ShipmentDetails.transRef == transRef; Each record is identified by the transaction reference which is the primary key. Therefore, the condition on ShipmentDetails.transRef == transRef means that if a record with the same value for the transRef attribute exists in the table, the record from the stream updates it. If not, the record from the stream is inserted as a new record.","title":"Configuring data stores inline"},{"location":"guides/storage-Integration/#performing-crud-operations","text":"","title":"Performing CRUD operations"},{"location":"guides/storage-Integration/#performing-crud-operations-via-streams","text":"If you tried out the example in the previous sections of this guide, you have already performed an update or insert operation via streams when you updated or inserted a record relating to the purchase records into the ShipmentDetails table. In this section, you can try more CRUD operations via streams as follows: If you want to insert all the records from the RawMaterialStream stream to the ShipmentDetails table, you can write the Siddhi query as follows. from RawMaterialStream select * insert into ShipmentDetails If you want to search for all the records by a specific supplier, you can write the Siddhi query as follows. from RawMaterialStream on supplier == 'Lakspur landing' in ShipmentDetails select * insert into SearchResultsStream Here, you search for records by entering the search condition in the from clause and directing the filtered results into an output stream named SearchResultsStream . If you want to delete all the records for a specific material, you can write the Siddhi query as follows: from DeleteMaterialStream on material == 'honey' in ShipmentDetails select * delete ShipmentDeails on ShipmentDetails.material == matarial;","title":"Performing CRUD operations via streams"},{"location":"guides/storage-Integration/#performing-crud-operations-via-rest-api","text":"This section explains how to use REST API to perform the same CRUD operations that you previously performed via streams. Before you begin: The Siddhi store query endpoint can be configured as follows: In the siddhi.stores.query.api: section of the <SI_HOME>/conf/server/deployment.yaml file, configure the following properties. The following is a sample configuration with default values. siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7070 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 7443 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon transport properties server.bootstrap.socket.timeout : The number of seconds after which the connection socket of the bootstrap server times out. client.bootstrap.socket.timeout : The number of seconds after which the connection socket of the bootstrap server times out. latency.metrics.enabled : If this is set to true , the latency metrics are enabled and logged for the HTTP transport. listenerConfigurations : Multiple listeners can be configured as shown in the above sample. id : A unique ID for the listener. host : The host of the listener. port : The port of the listener. scheme : This specifies whether the transport scheme is HTTP or HTTPS. keyStoreFile : If the transport scheme is HTTPS, this parameter specifies the path to the key store file. keyStorePassword : If the transport scheme is HTTPS, this parameter specifies the key store password. In the siddhi.stores.query.api: section of the <SI_HOME>/conf/server/deployment.yaml file, the following properties are configured by default: siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7370 The same parameter descriptions provided for the <SI_HOME>/conf/server/deployment.yaml file apply to this configuration.","title":"Performing CRUD operations via REST API"},{"location":"guides/storage-Integration/#inserting-records","text":"This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select <attribute name>, <attribute name>, ... insert into <table>; Sample CURL command The following CURL command submits a query that inserts a new record with the specified attribute values to a table named RoomOccupancyTable . For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k","title":"Inserting records"},{"location":"guides/storage-Integration/#retrieving-records","text":"This is the store query to retrieve records from a table or a window.","title":"Retrieving records"},{"location":"guides/storage-Integration/#retrieving-records-from-tables-and-windows","text":"Syntax from <table/window> <on condition>? select <attribute name>, <attribute name>, ... <group by>? <having>? <order by>? <limit>? Sample CURL command The following CURL command submits a query that retrieves room numbers and types of the rooms starting from room no 10 , from a table named roomTypeTable . The roomTypeTable table must be defined in the RoomService Siddhi application. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo >= 10 select roomNo, type; \" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo >= 10 select roomNo, type; \" }' -k Sample response The following is a sample response to the sample CURL command given above. {\"records\":[ [10,\"single\"], [11, \"triple\"], [12, \"double\"] ]}","title":"Retrieving records from tables and windows"},{"location":"guides/storage-Integration/#retrieving-records-from-aggregations","text":"This is the store query to retrieve records from an aggregation. Syntax from <aggregation> <on condition>? within <time range> per <time granularity> select <attribute name>, <attribute name>, ... <groupby>? <having>? <order by>? <limit>? Sample CURL command The following CURL command submits a query that retrieves average price of a stock. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k Sample response The following is a sample response to the sample CURL command given above. {\"records\":[ [1531180800, 'FB', 10000.0, 250.0], [1531184400, 'FB', 11000.0, 260.0], [1531188000, 'FB', 9000.0, 240.0] ]}","title":"Retrieving records from aggregations"},{"location":"guides/storage-Integration/#updating-records","text":"This store query updates selected attributes stored in a specific table based on a given condition. Syntax select <attribute name>, <attribute name>, ...? update <table> set <table>.<attribute name> = (<attribute name>|<expression>)?, <table>.<attribute name> = (<attribute name>|<expression>)?, ... on <condition> Sample CURL command The following CURL command updates the room occupancy for selected records in the table named RoomOccupancyTable . The records that are updated are ones of which the roon number is greater than 10 . The room occupancy is updated by adding 1 to the existing value of the people attribute. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k","title":"Updating records"},{"location":"guides/storage-Integration/#deleting-records","text":"This store query deletes selected records from a specified table. Syntax <select>? delete <table> on <conditional expresssion> Sample CURL command The following CURL command submits a query that deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection that has 10 as the actual value. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k","title":"Deleting records"},{"location":"guides/storage-Integration/#insertingupdating-records","text":"Syntax select <attribute name>, <attribute name>, ... update or insert into <table> set <table>.<attribute name> = <expression>, <table>.<attribute name> = <expression>, ... on <condition> Sample CURL command The following CURL command submits a query that attempts to update selected records in the RoomAssigneeTable table. The records that are selected to be updated are ones with room numbers that match the numbers specified in the select clause. If matching records are not found, it inserts a new record with the values provided in the select clause. For Streaming Integrator server: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For Streaming Integrator Tooling: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For more information, see Siddhi Query Guide - Store Query .","title":"Inserting/updating records"},{"location":"guides/storage-Integration/#manupulating-data-in-multiple-tables","text":"The Streaming Integrator allows you to perform CRUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension . Performing CRUD operations for multiple tables In order to perform CRUD operations, the system parameter named perform.CRUD.operations needs to be set to true in the deployment.yaml file. The syntax for a Siddhi query to perform a CRUD operation in multiple tables is as follows. from TriggerStream#rdbms:cud(<STRING> datasource.name, <STRING> query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a data source named, SAMPLE_DB a Siddhi query can be written as follows. from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream Retrieving data from multiple tables In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query(<STRING> dataource.name, <STRING> query, <STRING> stream definition) select <attributes> insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows. from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderData insert into OutputStream;","title":"Manupulating data in multiple tables"},{"location":"guides/summarizing-data/","text":"Summarizing Data \u00b6 Introduction \u00b6 Summarizing data refers to obtaining aggregates in an incremental manner for a specified set of time periods. Performing clock-time-based summarization \u00b6 Performing clock-time based based summarization involves calculating, storing, and then retrieving aggregations for a selected range of time granularities. This process is carried out in two parts: Calculating the aggregations for the selected time granularities and storing the results. Retrieving previously calculated aggregations for selected time granularities. To understand data summarization further, consider the scenario where a business that sells multiple brands stores its sales data in a physical database for the purpose of retrieving them later to perform sales analysis. Each sales transaction is received with the following details: symbol : The symbol that represents the brand of the items sold. price : the price at which each item was sold. amount : The number of items sold. The Sales Analyst needs to retrieve the total number of items sold of each brand per month, per week, per day etc., and then retrieve these totals for specific time durations to prepare sales analysis reports. Info It is not always required to maintain a physical database for incremental analysis, but it enables you to try out your aggregations with ease. The following sections explain how to calculate and store time-based aggregations for this scenarios, and then retrieve them. Calculate and store clock-time-based aggregate values \u00b6 To calculate and store time-based aggregation values for the scenario explained above, follow the procedure below. Start creating a new Siddhi application. You can name it TradeApp For instructions, see Creating a Siddhi Application . @App:name(\"TradeApp\"); To capture the input events based on which the aggregations are calculated, define an input stream as follows. define stream TradeStream (symbol string, price double, quantity long, timestamp long); Info In addition to the symbol , price , and quantity attributes to capture the input details already mentioned, the above stream definition includes an attribute named timestamp to capture the time at which the sales transaction occurs. The aggregations are executed based on this time. This attribute's value could either be a long value (reflecting the Unix timestamp in milliseconds), or a string value adhering to one of the following formats. <YYYY>-<MM>-<dd> <HH>:<mm>:<ss> <Z> : This format can be used if the timezone needs to be specified explicitly. Here the ISO 8601 UTC offset must be provided for . e.g., +05:30 reflects the India Time Zone. If time is not in GMT, this value must be provided.) <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> : This format can be used if the timezone is in GMT. To persist the aggregates that are calculated via your Siddhi application, include a store definition as follows. if not the data is stored in-memory and lost when Siddhi app is stopped. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") Info If the store definition is not provided, the data is stored in-memory, and then there is a risk of it being lost when the Siddhi application is stopped. Define an aggregation as follows. You can name it TradeAggregation . Info When you save aggregate values in a store, the system uses the aggregation name you define here as part of the database table name. Table name is <Aggregation_Name>_<Granularity> . Some database types have length limitations for table names (e.g., for Oracle, it is 30 characters). Therefore, you need to check whether the database type you have used for the store has such table name length limitation, and make sure that the aggregation name does not exceed that maximum length. define aggregation TradeAggregation; To calculate aggregations, include a query as follows: To get input events from the TradeStream stream that you previously defined, add a from clause as follows. from TradeStream To select attributes to be included in the output event, add a select clause as follows. select symbol, avg(price) as avgPrice, sum(quantity) as total Here, the avg() fuction is applied to the price attribute to derive the average price. The sum() function is applied to the quantity attribute to derive the total quantity. To group the output by the symbol, add a group by clause as follows. group by symbol The timestamp included in each input event allows you to calculate aggregates for the range of time granularities seconds-years. Therefore, to calculate aggregates for each time granularity within this range, add the aggregate by clause to this aggregate query as follows. aggregate by timestamp every sec ... year; The completed Siddhi application is as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; Retrieve the stored aggregate values \u00b6 This section involves retrieving the aggregate values that you calculated and persisted in the Calculate and store clock-time-based aggregate values subsection . To do this, let's add the Siddhi definitions and queries required for retrieval to the TradeApp Siddhi application that you have already created in the previous section. Open the TradeApp Siddhi application. To retrieve aggregations, you need to make retrieval requests. To capture these requests as events, let's define a stream as follows. define stream TradeSummaryRetrievalStream (symbol string); To process the events captured via the TradeSummaryRetrievalStream stream you defined, add a new query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; The completed Siddhi application is as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); define stream TradeSummaryRetrievalStream (symbol string); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation @info(name = 'CalculatingAggregates') from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; @info(name = 'RetrievingAggregates') from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; Summarizing data based on built in windowing criterias \u00b6 This section explains how to apply Siddhi logic to process a subset of events received to a stream based on time or the number of events. This is achieved via Siddi Windows . The window can apply to a batch of events or in a sliding manner. This is further explained in the following sections. Performing a time-based summarization in a sliding manner \u00b6 This subsection demonstrates how to summarize data for a short term based on time and well as how to do a summarization in a sliding manner. To demonstrate this, consider a factory manager who wants to be able to check the production for the last hour at any given time. Every event represents a production run. For this purpose, a Siddhi application can be created as follows: Start creating a new Siddhi application. You can name it PastHourProductionApp For instructions, see Creating a Siddhi Application . @App:name('PastHourProductionApp'); To capture details about each production run, define an input stream as follows. define stream ProductionStream (name string, amount long, timestamp long); To publish the production for the last hour, define the output stream as follows. @sink(type='log', prefix='Production totals over the past hour:') define stream PastHourProductionStream (name string, pastHourTotal long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To define how the output is derived, add the select statement as follows: select name, sum(amount) as pastHourTotal Here, the total is derived by applying the sum() function to the amount attribute of the ProductionStream input stream. To specify that the processing done as defined via the select statement applies to a time window, add the from clause and include the time window as shown below. This must be added above the select clause. from ProductionStream#window.time(1 hour) window.time indicates that the window added is a time window. The time considered is one hour. The window is a sliding window which considers the last hour at any given time (e.g., Once Siddhi calculates the total production during the time 13.00-14.00, next it calculates the total production during the time 13.01-14.01 after the 13.01 minute as elapsed.) For details about other window types supported, see Siddhi Extentions - Siddhi Execution Unique . ) To group by the product name, add the group by clause as follows. group by name To insert the results into the PastHourProductionStream output stream, add the insert into clause as follows. insert into PastHourProductionStream; The completed Siddhi application is as follows: @App:name('PastHourProductionApp') define stream ProductionStream (name string, amount long, timestamp long); @sink(type='log', prefix='Production totals over the past hour:') define stream PastHourProductionStream (name string, pastHourTotal long); from ProductionStream#window.time(1 hour) select name, sum(amount) as pastHourTotal group by name insert into PastHourProductionStream; Performing a length-based summarization to a batch of events \u00b6 This subsection demonstrates how to summarize data for a specific number of events as well as how to do that summarization for batches of events. To demonstrate this, assume that a factory manager wants to track the maximum production in every 10 production runs. IOn order to do so, let's create a Siddhi application as follows: Start creating a new Siddhi application. You can name it ProductionApp For instructions, see Creating a Siddhi Application . @App:name('MaximumProductionApp'); Define an input stream as follows to capture details about the production. define stream ProductionStream (name string, amount long); To output the maximum production detected every 10 production runs, define an output stream as follows. @sink(type='log', prefix='Maximum production in last 10 runs') define stream DetectedMaximumProductionStream (name string, maximumValue long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To define the subset of events to be considered based on the number of events, add the from clause with a lengthBatch window as follows. from ProductionStream#window.lengthBatch(10) window.lengthBatch indicates that the window added is a length window that considers events in batches when determin ing subsets. The number of events in each batch is 10 . For details about other window types supported, see Siddhi Extentions - Siddhi Execution Unique . To derive the values for the DetectedMaximumProductionStream output stream, add the select statement as follows. select name, max(amount) as maximumValue Here, the max() function is applied to the amount attribute to derive the maximum value. To group by the product name, add the group by clause as follows. group by name To insert the maximum production detected into the DetectedMaximumProductionStream output stream, add the insert into clause as follows. insert into DetectedMaximumProductionStream; The completed Siddhi application is as follows. @App:name('MaximumProductionApp') define stream ProductionStream (name string, amount long); @sink(type='log', prefix='Maximum production in last 10 runs') define stream DetectedMaximumProductionStream (name string, maximumValue long); from ProductionStream#window.lengthBatch(10) select name, max(amount) as maximumValue group by name insert into DetectedMaximumProductionStream;","title":"Summarizing Data"},{"location":"guides/summarizing-data/#summarizing-data","text":"","title":"Summarizing Data"},{"location":"guides/summarizing-data/#introduction","text":"Summarizing data refers to obtaining aggregates in an incremental manner for a specified set of time periods.","title":"Introduction"},{"location":"guides/summarizing-data/#performing-clock-time-based-summarization","text":"Performing clock-time based based summarization involves calculating, storing, and then retrieving aggregations for a selected range of time granularities. This process is carried out in two parts: Calculating the aggregations for the selected time granularities and storing the results. Retrieving previously calculated aggregations for selected time granularities. To understand data summarization further, consider the scenario where a business that sells multiple brands stores its sales data in a physical database for the purpose of retrieving them later to perform sales analysis. Each sales transaction is received with the following details: symbol : The symbol that represents the brand of the items sold. price : the price at which each item was sold. amount : The number of items sold. The Sales Analyst needs to retrieve the total number of items sold of each brand per month, per week, per day etc., and then retrieve these totals for specific time durations to prepare sales analysis reports. Info It is not always required to maintain a physical database for incremental analysis, but it enables you to try out your aggregations with ease. The following sections explain how to calculate and store time-based aggregations for this scenarios, and then retrieve them.","title":"Performing clock-time-based summarization"},{"location":"guides/summarizing-data/#calculate-and-store-clock-time-based-aggregate-values","text":"To calculate and store time-based aggregation values for the scenario explained above, follow the procedure below. Start creating a new Siddhi application. You can name it TradeApp For instructions, see Creating a Siddhi Application . @App:name(\"TradeApp\"); To capture the input events based on which the aggregations are calculated, define an input stream as follows. define stream TradeStream (symbol string, price double, quantity long, timestamp long); Info In addition to the symbol , price , and quantity attributes to capture the input details already mentioned, the above stream definition includes an attribute named timestamp to capture the time at which the sales transaction occurs. The aggregations are executed based on this time. This attribute's value could either be a long value (reflecting the Unix timestamp in milliseconds), or a string value adhering to one of the following formats. <YYYY>-<MM>-<dd> <HH>:<mm>:<ss> <Z> : This format can be used if the timezone needs to be specified explicitly. Here the ISO 8601 UTC offset must be provided for . e.g., +05:30 reflects the India Time Zone. If time is not in GMT, this value must be provided.) <yyyy>-<MM>-<dd> <HH>:<mm>:<ss> : This format can be used if the timezone is in GMT. To persist the aggregates that are calculated via your Siddhi application, include a store definition as follows. if not the data is stored in-memory and lost when Siddhi app is stopped. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") Info If the store definition is not provided, the data is stored in-memory, and then there is a risk of it being lost when the Siddhi application is stopped. Define an aggregation as follows. You can name it TradeAggregation . Info When you save aggregate values in a store, the system uses the aggregation name you define here as part of the database table name. Table name is <Aggregation_Name>_<Granularity> . Some database types have length limitations for table names (e.g., for Oracle, it is 30 characters). Therefore, you need to check whether the database type you have used for the store has such table name length limitation, and make sure that the aggregation name does not exceed that maximum length. define aggregation TradeAggregation; To calculate aggregations, include a query as follows: To get input events from the TradeStream stream that you previously defined, add a from clause as follows. from TradeStream To select attributes to be included in the output event, add a select clause as follows. select symbol, avg(price) as avgPrice, sum(quantity) as total Here, the avg() fuction is applied to the price attribute to derive the average price. The sum() function is applied to the quantity attribute to derive the total quantity. To group the output by the symbol, add a group by clause as follows. group by symbol The timestamp included in each input event allows you to calculate aggregates for the range of time granularities seconds-years. Therefore, to calculate aggregates for each time granularity within this range, add the aggregate by clause to this aggregate query as follows. aggregate by timestamp every sec ... year; The completed Siddhi application is as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year;","title":"Calculate and store clock-time-based aggregate values"},{"location":"guides/summarizing-data/#retrieve-the-stored-aggregate-values","text":"This section involves retrieving the aggregate values that you calculated and persisted in the Calculate and store clock-time-based aggregate values subsection . To do this, let's add the Siddhi definitions and queries required for retrieval to the TradeApp Siddhi application that you have already created in the previous section. Open the TradeApp Siddhi application. To retrieve aggregations, you need to make retrieval requests. To capture these requests as events, let's define a stream as follows. define stream TradeSummaryRetrievalStream (symbol string); To process the events captured via the TradeSummaryRetrievalStream stream you defined, add a new query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; The completed Siddhi application is as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); define stream TradeSummaryRetrievalStream (symbol string); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation @info(name = 'CalculatingAggregates') from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; @info(name = 'RetrievingAggregates') from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream;","title":"Retrieve the stored aggregate values"},{"location":"guides/summarizing-data/#summarizing-data-based-on-built-in-windowing-criterias","text":"This section explains how to apply Siddhi logic to process a subset of events received to a stream based on time or the number of events. This is achieved via Siddi Windows . The window can apply to a batch of events or in a sliding manner. This is further explained in the following sections.","title":"Summarizing data based on built in windowing criterias"},{"location":"guides/summarizing-data/#performing-a-time-based-summarization-in-a-sliding-manner","text":"This subsection demonstrates how to summarize data for a short term based on time and well as how to do a summarization in a sliding manner. To demonstrate this, consider a factory manager who wants to be able to check the production for the last hour at any given time. Every event represents a production run. For this purpose, a Siddhi application can be created as follows: Start creating a new Siddhi application. You can name it PastHourProductionApp For instructions, see Creating a Siddhi Application . @App:name('PastHourProductionApp'); To capture details about each production run, define an input stream as follows. define stream ProductionStream (name string, amount long, timestamp long); To publish the production for the last hour, define the output stream as follows. @sink(type='log', prefix='Production totals over the past hour:') define stream PastHourProductionStream (name string, pastHourTotal long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To define how the output is derived, add the select statement as follows: select name, sum(amount) as pastHourTotal Here, the total is derived by applying the sum() function to the amount attribute of the ProductionStream input stream. To specify that the processing done as defined via the select statement applies to a time window, add the from clause and include the time window as shown below. This must be added above the select clause. from ProductionStream#window.time(1 hour) window.time indicates that the window added is a time window. The time considered is one hour. The window is a sliding window which considers the last hour at any given time (e.g., Once Siddhi calculates the total production during the time 13.00-14.00, next it calculates the total production during the time 13.01-14.01 after the 13.01 minute as elapsed.) For details about other window types supported, see Siddhi Extentions - Siddhi Execution Unique . ) To group by the product name, add the group by clause as follows. group by name To insert the results into the PastHourProductionStream output stream, add the insert into clause as follows. insert into PastHourProductionStream; The completed Siddhi application is as follows: @App:name('PastHourProductionApp') define stream ProductionStream (name string, amount long, timestamp long); @sink(type='log', prefix='Production totals over the past hour:') define stream PastHourProductionStream (name string, pastHourTotal long); from ProductionStream#window.time(1 hour) select name, sum(amount) as pastHourTotal group by name insert into PastHourProductionStream;","title":"Performing a time-based summarization in a sliding manner"},{"location":"guides/summarizing-data/#performing-a-length-based-summarization-to-a-batch-of-events","text":"This subsection demonstrates how to summarize data for a specific number of events as well as how to do that summarization for batches of events. To demonstrate this, assume that a factory manager wants to track the maximum production in every 10 production runs. IOn order to do so, let's create a Siddhi application as follows: Start creating a new Siddhi application. You can name it ProductionApp For instructions, see Creating a Siddhi Application . @App:name('MaximumProductionApp'); Define an input stream as follows to capture details about the production. define stream ProductionStream (name string, amount long); To output the maximum production detected every 10 production runs, define an output stream as follows. @sink(type='log', prefix='Maximum production in last 10 runs') define stream DetectedMaximumProductionStream (name string, maximumValue long); Info A sink annotation is connected to the output stream to log the output events. For more information about adding sinks to publish events, see the Publishing Data guide . To define the subset of events to be considered based on the number of events, add the from clause with a lengthBatch window as follows. from ProductionStream#window.lengthBatch(10) window.lengthBatch indicates that the window added is a length window that considers events in batches when determin ing subsets. The number of events in each batch is 10 . For details about other window types supported, see Siddhi Extentions - Siddhi Execution Unique . To derive the values for the DetectedMaximumProductionStream output stream, add the select statement as follows. select name, max(amount) as maximumValue Here, the max() function is applied to the amount attribute to derive the maximum value. To group by the product name, add the group by clause as follows. group by name To insert the maximum production detected into the DetectedMaximumProductionStream output stream, add the insert into clause as follows. insert into DetectedMaximumProductionStream; The completed Siddhi application is as follows. @App:name('MaximumProductionApp') define stream ProductionStream (name string, amount long); @sink(type='log', prefix='Maximum production in last 10 runs') define stream DetectedMaximumProductionStream (name string, maximumValue long); from ProductionStream#window.lengthBatch(10) select name, max(amount) as maximumValue group by name insert into DetectedMaximumProductionStream;","title":"Performing a length-based summarization to a batch of events"},{"location":"guides/transforming-data/","text":"Transforming Data \u00b6 Introduction \u00b6 The Streaming Integrator allows you to perform a wide range of transformations to the input data received. Some of these transformations are carried out via operators that are defined inline within the Siddhi application. For the rest of the transformations, you can use Siddhi extensions that are available to be downloaded via the Siddhi Extension Store . Most of these extensions are shipped with the Streaming Integrator by default. Transform data using inline operators \u00b6 The operators that you can configure inline within Siddhi applications in order to carry out data transformations are listed in the Siddhi Query Guide - Inbuild Aggregation Functions section . To show how an inline operators are configured, let's consider an example where readings from a sensor that indicates the temperature of a room every second are transformed to indicate the average tempertature and the average humidity as at each second. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) In this example, let's name the application TemperatureApp . Let's define the input stream to define the schema based on which data is selected to the streaming integration flow. In this example, let's assume that each event indicates the device ID, the room ID, and the temperature. Therefore, let's define an input stream as follows: define stream TempStream (deviceID long, roomNo int, temp double); Info For more information about defining input streams to receive events, see the Consuming Data guide . To do the required transformation, let's add the query as follows: Add the from clause with the name of the input stream to indicate that the events to be processed are taken from the input stream. from TempStream Add the insert into clause with the name of the output stream to indicate that the processed events are directed to that stream. from TempStream insert into OutputStream; 3.Add a select clause in a line between the from and insert into clauses. To derive the average temperature from the temperature, apply the avg() to the temp attribute, and then specify avgTemp as the name with which the result should be output. from TempStream select roomNo, deviceID, avg(temp) as avgTemp insert into OutputStream; To group by a specific attribute (by the roomNo attribute in this example), specify it via the group by clause as shown below. from TempStream select roomNo, deviceID, avg(temp) as avgTemp group by roomNo insert into OutputStream; Save the Siddhi application. The completed Siddhi application is as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, deviceID, avg(temp) as avgTemp group by roomNo insert into OutputStream; Give an example and point to existing inline math and logical operators. Transform data using in-built extensions \u00b6 The Streaming Integrator offers a variety of options to carry out data transformations via in-built extensions. The following table describes the complete list of extensions that provide data transformation functionality. Extension Description Siddhi-execution-math Transforms data by performing mathematical operations. Siddhi-execution-unitconversion Performs unit conversions ranging from length, weight, volume, etc. Siddhi-execution-string Performs string manipulations. Siddhi-execution-time Performs time-based transformations such as converting time zones. Siddhi-execution-map Converts events into maps and performs transformations such as concatenating and removing attributes. Siddhi-execution-reorder Rearranges the order of the incoming event flow. Siddhi-execution-json Performs manipulations to JSON strings. These extensions are shipped with the Streaming Integrator by default. If you want to use a Siddhi extension that is not shipped by default, see Downloading and Installing Siddhi Extensions Transform data using custom function calls \u00b6 To write custom function calls with Siddhi-script-js, follow the procedure below: Note In this section, you can reuse the TemperatureApp Siddhi application that you previously created. For this section, assume that you need to derive a unique ID for each temperature reading by combining the room number and the device ID. In the TemperatureApp Siddhi application, add a script definition as follows. define function <function name>[<language name>] return <return type> { <operation of the function> }; In this example, you can write a function that can be used to concatanate the room number and device ID as follows. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; Add another Siddhi query to apply the script you wrote to the relevant attributes of the input stream definition. from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Save the Siddhi application. Transforming message formats (XML to JSON etc) \u00b6 These transformations involve converting the message format to a different format after a the message is received, or converting the format before publishing the message. This is managed via mapping. For detailed instructions to convert message formats via mapping, see the following guides: Consuming Messages - Supported Message Formats Publishing Messages - Supported Message Formats","title":"Transforming Data"},{"location":"guides/transforming-data/#transforming-data","text":"","title":"Transforming Data"},{"location":"guides/transforming-data/#introduction","text":"The Streaming Integrator allows you to perform a wide range of transformations to the input data received. Some of these transformations are carried out via operators that are defined inline within the Siddhi application. For the rest of the transformations, you can use Siddhi extensions that are available to be downloaded via the Siddhi Extension Store . Most of these extensions are shipped with the Streaming Integrator by default.","title":"Introduction"},{"location":"guides/transforming-data/#transform-data-using-inline-operators","text":"The operators that you can configure inline within Siddhi applications in order to carry out data transformations are listed in the Siddhi Query Guide - Inbuild Aggregation Functions section . To show how an inline operators are configured, let's consider an example where readings from a sensor that indicates the temperature of a room every second are transformed to indicate the average tempertature and the average humidity as at each second. Open the Streaming Integrator Studio and start creating a new Siddhi application. For more information, see Creating a Siddhi Application . Enter a name for the Siddhi application as shown below. @App:name(\"<Siddhi_Application_Name>) In this example, let's name the application TemperatureApp . Let's define the input stream to define the schema based on which data is selected to the streaming integration flow. In this example, let's assume that each event indicates the device ID, the room ID, and the temperature. Therefore, let's define an input stream as follows: define stream TempStream (deviceID long, roomNo int, temp double); Info For more information about defining input streams to receive events, see the Consuming Data guide . To do the required transformation, let's add the query as follows: Add the from clause with the name of the input stream to indicate that the events to be processed are taken from the input stream. from TempStream Add the insert into clause with the name of the output stream to indicate that the processed events are directed to that stream. from TempStream insert into OutputStream; 3.Add a select clause in a line between the from and insert into clauses. To derive the average temperature from the temperature, apply the avg() to the temp attribute, and then specify avgTemp as the name with which the result should be output. from TempStream select roomNo, deviceID, avg(temp) as avgTemp insert into OutputStream; To group by a specific attribute (by the roomNo attribute in this example), specify it via the group by clause as shown below. from TempStream select roomNo, deviceID, avg(temp) as avgTemp group by roomNo insert into OutputStream; Save the Siddhi application. The completed Siddhi application is as follows. @App:name(\"TemperatureApp\") @App:description(\"Description of the plan\") define stream TempStream (deviceID long, roomNo int, temp double); from TempStream select roomNo, deviceID, avg(temp) as avgTemp group by roomNo insert into OutputStream; Give an example and point to existing inline math and logical operators.","title":"Transform data using inline operators"},{"location":"guides/transforming-data/#transform-data-using-in-built-extensions","text":"The Streaming Integrator offers a variety of options to carry out data transformations via in-built extensions. The following table describes the complete list of extensions that provide data transformation functionality. Extension Description Siddhi-execution-math Transforms data by performing mathematical operations. Siddhi-execution-unitconversion Performs unit conversions ranging from length, weight, volume, etc. Siddhi-execution-string Performs string manipulations. Siddhi-execution-time Performs time-based transformations such as converting time zones. Siddhi-execution-map Converts events into maps and performs transformations such as concatenating and removing attributes. Siddhi-execution-reorder Rearranges the order of the incoming event flow. Siddhi-execution-json Performs manipulations to JSON strings. These extensions are shipped with the Streaming Integrator by default. If you want to use a Siddhi extension that is not shipped by default, see Downloading and Installing Siddhi Extensions","title":"Transform data using in-built extensions"},{"location":"guides/transforming-data/#transform-data-using-custom-function-calls","text":"To write custom function calls with Siddhi-script-js, follow the procedure below: Note In this section, you can reuse the TemperatureApp Siddhi application that you previously created. For this section, assume that you need to derive a unique ID for each temperature reading by combining the room number and the device ID. In the TemperatureApp Siddhi application, add a script definition as follows. define function <function name>[<language name>] return <return type> { <operation of the function> }; In this example, you can write a function that can be used to concatanate the room number and device ID as follows. define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var responce = str1 + str2 + str3; return responce; }; Add another Siddhi query to apply the script you wrote to the relevant attributes of the input stream definition. from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; Save the Siddhi application.","title":"Transform data using custom function calls"},{"location":"guides/transforming-data/#transforming-message-formats-xml-to-json-etc","text":"These transformations involve converting the message format to a different format after a the message is received, or converting the format before publishing the message. This is managed via mapping. For detailed instructions to convert message formats via mapping, see the following guides: Consuming Messages - Supported Message Formats Publishing Messages - Supported Message Formats","title":"Transforming message formats (XML to JSON etc)"},{"location":"guides/triggering-integration-flows/","text":"Triggering Integration Flows \u00b6 Introduction \u00b6 Once the Streaming Integrator processes streaming data and generates an output, you are often required to take some action based on that output. The action required could be executing some code, calling an external service or triggering a complex integration flow. When it is required to trigger an integration flow, the Streaming Integrator can send a request to the Micro integrator to initiate such action. Triggering integration via Streaming Integrator as fire and forget manner \u00b6 In order to allow the STreaming Integrator to trigger an integration flow in the Micro Integrator, you need to do the following: Design a Siddhi application with a grpc-call sink that allows an output event to be generated as a request that is sent to the Micro Integrator. Deploy the required artifacts in the Micro Integrator so that the Micro Integrator is triggered to take the required action when it receives the request from the Streaming Integrator. Designing the Siddhi application in the Streaming Integrator \u00b6 gRPC sink is a Siddhi extension via which you can send messages in a fire and forget manner from SI to MI and trigger a sequence. The following is a sample Siddhi application with a gRPC sink that triggers a sequence named inSeq in the micro integrator. @App:name(\"grpc-call\") @App:description(\"This siddhi application will trigger inSeq in the MicroIntegrator\") @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(message string, headers string); @sink( type='grpc', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/consume/inSeq', headers='Content-Type:json', metadata='Authorization:Basic YWRtaW46YWRtaW4=', @map(type='json') ) define stream FooStream (message string, headers string); from InputStream select * insert into FooStream; Note the following about the grpc-call sink configuration: consume in the publisher URL path: This indicates that the gRPC request invokes the consume method of the Micro Integrator's gRPC inbound endpoint. This method does not send a response back to the client. headers parameter: This is required to pass the content type so that the system can construct and read the message from the Micro Integrator. After creating the Siddhi application: To deploy the above Siddhi application, save it as a .siddhi file in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory. Deploying the required artifacts in the Micro Integrator \u00b6 The following artifacts need to be deployed in the Micro Integrator. To start gRPC server in the Micro Integrator son that it can receive the gRPC event sent by the Streaming Integrator, you need to deploy a gRPC inbound endpoint (similar to the sample configuration given below) by saving it as a .xml file in the <MI_HOME>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Both the inbound endpoint and the grpc-call sink in the Siddhi application refers to a sequence ( inSeq in this example). A sequence with the same name and the required configuration should be added to the <MI_HOME>/repository/deployment/server/synapse-configs/default/sequences directory. The following is a sample configuration. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> Triggering integration via Streaming Integrator and receiving a response from MI \u00b6 The gRPC-call sink allows the Streaming Integrator to send messages to the Micro Integrator, trigger a sequence, and get a response back. In order to receive that response, the Streaming Integrator needs to use the grpc-call-response source. The following is a sample Siddhi application with a gRPC-call sink that triggers a sequence named inSeq in the Micro Integrator and then uses the grpc-call response source to process the response received from the Micro Integrator. @App:name(\"grpc-call-response\") @App:description(\"Description of the plan\") @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json')) define stream FooStream (message string, headers string); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (message string); @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(message string, headers string); from InputStream select * insert into FooStream; from BarStream select * insert into TempStream; Note the following about the grpc-call sink configuration: process in the publisher URL path: This indicates that the gRPC request invokes the process method of the gRPC server of the Micro Integrator's inbound endpoint that sends a response back to the client. sink.id parameter: This is required when using the gRPC-call sink in order to map the request with its corresponding response. After creating the Siddhi application: To deploy the above Siddhi application, save it as a .siddhi file in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory. Once the Siddhib application is created and deployed, deploy the following artifacts in the Micro Integrator: In order to start a gRPC server in the Micro Integrator to receive the gRPC event sent by the Streaming Integrator, deploy a GRPC inbound endpoint by adding the following sample configuration as a .xml file to the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Add the following sample inSeq sequence to the <MI_HOME>/repository/deployment/server/synapse-configs/default/sequences directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> The respond mediator sends the response back to the Streaming Integrator.","title":"Triggering Integration Flows"},{"location":"guides/triggering-integration-flows/#triggering-integration-flows","text":"","title":"Triggering Integration Flows"},{"location":"guides/triggering-integration-flows/#introduction","text":"Once the Streaming Integrator processes streaming data and generates an output, you are often required to take some action based on that output. The action required could be executing some code, calling an external service or triggering a complex integration flow. When it is required to trigger an integration flow, the Streaming Integrator can send a request to the Micro integrator to initiate such action.","title":"Introduction"},{"location":"guides/triggering-integration-flows/#triggering-integration-via-streaming-integrator-as-fire-and-forget-manner","text":"In order to allow the STreaming Integrator to trigger an integration flow in the Micro Integrator, you need to do the following: Design a Siddhi application with a grpc-call sink that allows an output event to be generated as a request that is sent to the Micro Integrator. Deploy the required artifacts in the Micro Integrator so that the Micro Integrator is triggered to take the required action when it receives the request from the Streaming Integrator.","title":"Triggering integration via Streaming Integrator as fire and forget manner"},{"location":"guides/triggering-integration-flows/#designing-the-siddhi-application-in-the-streaming-integrator","text":"gRPC sink is a Siddhi extension via which you can send messages in a fire and forget manner from SI to MI and trigger a sequence. The following is a sample Siddhi application with a gRPC sink that triggers a sequence named inSeq in the micro integrator. @App:name(\"grpc-call\") @App:description(\"This siddhi application will trigger inSeq in the MicroIntegrator\") @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(message string, headers string); @sink( type='grpc', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/consume/inSeq', headers='Content-Type:json', metadata='Authorization:Basic YWRtaW46YWRtaW4=', @map(type='json') ) define stream FooStream (message string, headers string); from InputStream select * insert into FooStream; Note the following about the grpc-call sink configuration: consume in the publisher URL path: This indicates that the gRPC request invokes the consume method of the Micro Integrator's gRPC inbound endpoint. This method does not send a response back to the client. headers parameter: This is required to pass the content type so that the system can construct and read the message from the Micro Integrator. After creating the Siddhi application: To deploy the above Siddhi application, save it as a .siddhi file in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory.","title":"Designing the Siddhi application in the Streaming Integrator"},{"location":"guides/triggering-integration-flows/#deploying-the-required-artifacts-in-the-micro-integrator","text":"The following artifacts need to be deployed in the Micro Integrator. To start gRPC server in the Micro Integrator son that it can receive the gRPC event sent by the Streaming Integrator, you need to deploy a gRPC inbound endpoint (similar to the sample configuration given below) by saving it as a .xml file in the <MI_HOME>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Both the inbound endpoint and the grpc-call sink in the Siddhi application refers to a sequence ( inSeq in this example). A sequence with the same name and the required configuration should be added to the <MI_HOME>/repository/deployment/server/synapse-configs/default/sequences directory. The following is a sample configuration. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence>","title":"Deploying the required artifacts in the Micro Integrator"},{"location":"guides/triggering-integration-flows/#triggering-integration-via-streaming-integrator-and-receiving-a-response-from-mi","text":"The gRPC-call sink allows the Streaming Integrator to send messages to the Micro Integrator, trigger a sequence, and get a response back. In order to receive that response, the Streaming Integrator needs to use the grpc-call-response source. The following is a sample Siddhi application with a gRPC-call sink that triggers a sequence named inSeq in the Micro Integrator and then uses the grpc-call response source to process the response received from the Micro Integrator. @App:name(\"grpc-call-response\") @App:description(\"Description of the plan\") @sink( type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='Content-Type:json', @map(type='json')) define stream FooStream (message string, headers string); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (message string); @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream InputStream(message string, headers string); from InputStream select * insert into FooStream; from BarStream select * insert into TempStream; Note the following about the grpc-call sink configuration: process in the publisher URL path: This indicates that the gRPC request invokes the process method of the gRPC server of the Micro Integrator's inbound endpoint that sends a response back to the client. sink.id parameter: This is required when using the gRPC-call sink in order to map the request with its corresponding response. After creating the Siddhi application: To deploy the above Siddhi application, save it as a .siddhi file in the <SI_HOME>/WSO2/server/deployment/siddhi-files directory. Once the Siddhib application is created and deployed, deploy the following artifacts in the Micro Integrator: In order to start a gRPC server in the Micro Integrator to receive the gRPC event sent by the Streaming Integrator, deploy a GRPC inbound endpoint by adding the following sample configuration as a .xml file to the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Add the following sample inSeq sequence to the <MI_HOME>/repository/deployment/server/synapse-configs/default/sequences directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> The respond mediator sends the response back to the Streaming Integrator.","title":"Triggering integration via Streaming Integrator and receiving a response from MI"},{"location":"overview/about_this_release/","text":"About this Release \u00b6 WSO2 EI 7.0.0 is the successor of WSO2 EI 6.5.0.","title":"About this Release"},{"location":"overview/about_this_release/#about-this-release","text":"WSO2 EI 7.0.0 is the successor of WSO2 EI 6.5.0.","title":"About this Release"},{"location":"overview/architecture/","text":"Architecture \u00b6 Stream integration refers to collecting, processing and, integrate or acting on data generated during business activities by various sources. This description is paramount when designing a solution to address a streaming integration use case. Collecting : Receiving or capturing data from various data sources. Processing : Manipulation of data to identify interesting patterns and to extract information. Integrating : Making processed data available for consumers to consume in an expected format via a given protocol or a medium. Acting : Taking actions based on the results and findings done via processing the data. The action can be executing some random code, calling an external service, or triggering a complex integration. The Streaming Integrator architecture reflects this natural flow in its design as illustrated below. The Streaming Integrator contains Siddhi.io as its core to collect, and 60+ connectors to connect with various sources and destinations. The following are the major components and constructs of the Streaming Integrator. Source \u00b6 The source is the construct that is used in Siddhi to receive data from an external source. A stream must be attached to the source so that the data received at the source is passed into the stream to be accessed in subsequent queries. A source handles all the transport-related functionality when connecting to a data source. Source Mapper \u00b6 Data received by the source is passed into the source mappers and it maps incoming data to a format understandable by the Siddhi engine. Siddhi \u00b6 Siddhi is a stream processing library written in Java. Users can write stream processing logic in the Siddhi Query Language (SiddhiQL). The Siddhi engine can run such queries against continuous streams of data. Siddhi App \u00b6 A Siddhi application is a flat-file with siddhi extensions. It includes stream processing logic written in SiddhiQL. This is the deployable artifact in Siddhi which developers compile by writing stream processing logic. SiddhiQL \u00b6 An SQL-like language that lets users write stream processing logic that can be read by the Siddhi engine. Store API \u00b6 A REST API hosted in the SI server to let users fetch data stored in a persistent store or in-memory on-demand using ad-hoc siddhi queries. Sink \u00b6 The sink is a construct in Siddhi that publishes data to an external destination. You can attach a stream to a sink so that all the events flowing through this stream are published via the sink. The sink handles all transport-related functionality. Sink Mapper \u00b6 Event streams to be published in an external destination have to be mapped to a data format (e.g., JSON, XML) required by the destination. The sink mapper does this mapping.","title":"Architecture"},{"location":"overview/architecture/#architecture","text":"Stream integration refers to collecting, processing and, integrate or acting on data generated during business activities by various sources. This description is paramount when designing a solution to address a streaming integration use case. Collecting : Receiving or capturing data from various data sources. Processing : Manipulation of data to identify interesting patterns and to extract information. Integrating : Making processed data available for consumers to consume in an expected format via a given protocol or a medium. Acting : Taking actions based on the results and findings done via processing the data. The action can be executing some random code, calling an external service, or triggering a complex integration. The Streaming Integrator architecture reflects this natural flow in its design as illustrated below. The Streaming Integrator contains Siddhi.io as its core to collect, and 60+ connectors to connect with various sources and destinations. The following are the major components and constructs of the Streaming Integrator.","title":"Architecture"},{"location":"overview/architecture/#source","text":"The source is the construct that is used in Siddhi to receive data from an external source. A stream must be attached to the source so that the data received at the source is passed into the stream to be accessed in subsequent queries. A source handles all the transport-related functionality when connecting to a data source.","title":"Source"},{"location":"overview/architecture/#source-mapper","text":"Data received by the source is passed into the source mappers and it maps incoming data to a format understandable by the Siddhi engine.","title":"Source Mapper"},{"location":"overview/architecture/#siddhi","text":"Siddhi is a stream processing library written in Java. Users can write stream processing logic in the Siddhi Query Language (SiddhiQL). The Siddhi engine can run such queries against continuous streams of data.","title":"Siddhi"},{"location":"overview/architecture/#siddhi-app","text":"A Siddhi application is a flat-file with siddhi extensions. It includes stream processing logic written in SiddhiQL. This is the deployable artifact in Siddhi which developers compile by writing stream processing logic.","title":"Siddhi App"},{"location":"overview/architecture/#siddhiql","text":"An SQL-like language that lets users write stream processing logic that can be read by the Siddhi engine.","title":"SiddhiQL"},{"location":"overview/architecture/#store-api","text":"A REST API hosted in the SI server to let users fetch data stored in a persistent store or in-memory on-demand using ad-hoc siddhi queries.","title":"Store API"},{"location":"overview/architecture/#sink","text":"The sink is a construct in Siddhi that publishes data to an external destination. You can attach a stream to a sink so that all the events flowing through this stream are published via the sink. The sink handles all transport-related functionality.","title":"Sink"},{"location":"overview/architecture/#sink-mapper","text":"Event streams to be published in an external destination have to be mapped to a data format (e.g., JSON, XML) required by the destination. The sink mapper does this mapping.","title":"Sink Mapper"},{"location":"overview/overview/","text":"Introduction \u00b6 WSO2 Streaming Integrator(SI) is a streaming data processing server that integrates streaming data and takes action based on streaming data. The streaming integration capabilities of EI are delivered via this runtime WSO2 SI can be effectively used for: Realtime ETL : CDC for DBs, tailing files, scraping HTTP Endpoints, etc. Work with streaming messaging systems : It is fully compatible with Kafka and NATS, and provides advanced stream processing capabilities required to utilize the full potential of streaming data. Streaming data Integration : Allows you to treat all data sources as streams and connect them with any destination. Execute complex integrations based on streaming data : SI has native support to work hand-in-hand with WSO2 Micro integrator to trigger complex integration flows based on decisions derived via stateful stream processing logic. Try it out! To try out each of the above use cases with SI, see Tutorials Key Features \u00b6 WSO2 SI is powered by Siddhi.io , a well known cloud native open source stream processing engine. Siddhi allows you to write complex stream processing logic using an intuitive SQL-like language known as SiddhiQL . You can perform the following actions on the fly using Siddhi queries and constructs. Transforming your data from one format to another (e.g., to/from XML, JSON, AVRO, etc.). Enriching data received from a specific source by combining it with databases, services, etc., via inline calculations and custom functions. Correlating data streams by joining multiple streams to create an aggregate stream. Cleaning data by filtering it and by modifying the content (e.g., obfuscating) in messages. Deriving insights by identifying interesting patterns and sequences of events in data streams. Summarizing data as and when it is generated using temporal windows and incremental time series aggregations. With 60+ prebuilt and well tested collection of connectors, WSO2 SI allows you to connect any data source with any destination regardless of the different protocols and data formats used by the different endpoints. The SI Store API exposes aggregated and collected data streams to in-memory and persistence storages via a REST API, allowing you to execute queries and generate summarized information on demand. Synapse integration flows deployed in WSO2 Micro Integration(MI) can be executed directly by SI. This allows you to build robust data processing and integration pipelines by combining powerful stream processing and integration capabilities. Tooling \u00b6 WSO2 SI is coupled with the Streaming Integrator Tooling ; a comprehensive streaming integration flow designer for developing Siddhi applications by writing queries or via the drag-and-drop functionality, testing them thoroughly before using them in production, and then deploying them in the SI server or exporting them to be deployed as a K8 or a Docker image . Centralized and Decentralized Deployment \u00b6 Being container-friendly by design with a small image size, low resource footprint, a startup time less than two seconds, etc., WSO2 SI can be easily deployed in VMs, Docker or K8s. Its native support for Kubernetes with a K8s Operator provides a convenient way to deploy SI on a K8s cluster with a single command, thereby eliminating the need for manual configurations. Deploying SI as a highly available minimum HA cluster allows you to achieve zero data loss with just two SI nodes. What's Next Get started with WSO2 SI in 5 minutes Learn SI functionality in 30 minutes","title":"Introduction"},{"location":"overview/overview/#introduction","text":"WSO2 Streaming Integrator(SI) is a streaming data processing server that integrates streaming data and takes action based on streaming data. The streaming integration capabilities of EI are delivered via this runtime WSO2 SI can be effectively used for: Realtime ETL : CDC for DBs, tailing files, scraping HTTP Endpoints, etc. Work with streaming messaging systems : It is fully compatible with Kafka and NATS, and provides advanced stream processing capabilities required to utilize the full potential of streaming data. Streaming data Integration : Allows you to treat all data sources as streams and connect them with any destination. Execute complex integrations based on streaming data : SI has native support to work hand-in-hand with WSO2 Micro integrator to trigger complex integration flows based on decisions derived via stateful stream processing logic. Try it out! To try out each of the above use cases with SI, see Tutorials","title":"Introduction"},{"location":"overview/overview/#key-features","text":"WSO2 SI is powered by Siddhi.io , a well known cloud native open source stream processing engine. Siddhi allows you to write complex stream processing logic using an intuitive SQL-like language known as SiddhiQL . You can perform the following actions on the fly using Siddhi queries and constructs. Transforming your data from one format to another (e.g., to/from XML, JSON, AVRO, etc.). Enriching data received from a specific source by combining it with databases, services, etc., via inline calculations and custom functions. Correlating data streams by joining multiple streams to create an aggregate stream. Cleaning data by filtering it and by modifying the content (e.g., obfuscating) in messages. Deriving insights by identifying interesting patterns and sequences of events in data streams. Summarizing data as and when it is generated using temporal windows and incremental time series aggregations. With 60+ prebuilt and well tested collection of connectors, WSO2 SI allows you to connect any data source with any destination regardless of the different protocols and data formats used by the different endpoints. The SI Store API exposes aggregated and collected data streams to in-memory and persistence storages via a REST API, allowing you to execute queries and generate summarized information on demand. Synapse integration flows deployed in WSO2 Micro Integration(MI) can be executed directly by SI. This allows you to build robust data processing and integration pipelines by combining powerful stream processing and integration capabilities.","title":"Key Features"},{"location":"overview/overview/#tooling","text":"WSO2 SI is coupled with the Streaming Integrator Tooling ; a comprehensive streaming integration flow designer for developing Siddhi applications by writing queries or via the drag-and-drop functionality, testing them thoroughly before using them in production, and then deploying them in the SI server or exporting them to be deployed as a K8 or a Docker image .","title":"Tooling"},{"location":"overview/overview/#centralized-and-decentralized-deployment","text":"Being container-friendly by design with a small image size, low resource footprint, a startup time less than two seconds, etc., WSO2 SI can be easily deployed in VMs, Docker or K8s. Its native support for Kubernetes with a K8s Operator provides a convenient way to deploy SI on a K8s cluster with a single command, thereby eliminating the need for manual configurations. Deploying SI as a highly available minimum HA cluster allows you to achieve zero data loss with just two SI nodes. What's Next Get started with WSO2 SI in 5 minutes Learn SI functionality in 30 minutes","title":"Centralized and Decentralized Deployment"},{"location":"quick-start-guide/getting-started-with-si/","text":"Getting Started with Streaming Integrator in Five Minutes \u00b6 Introduction \u00b6 This quick start guide gets you started with the Streaming Integrator (SI), in just 5 minutes. In this guide, you will download the SI distribution, start it and then try out a simple Siddhi application. Tutorial Outline \u00b6 Downloading Streaming Integrator Starting the server Deploying a simple Siddhi app Downloading Streaming Integrator \u00b6 Download the Streaming Integrator distribution from WSO2 Streaming Integrator site and extract it to a location of your choice. Hereafter, the extracted location is referred to as <SI_HOME> . Starting the server \u00b6 Navigate to the <SI_HOME>/bin directory in the console and issue the appropriate command depending on your operating system to start the Streaming Integrator. - For Windows: server.bat - For Linux/MacOS: ./server.sh Deploying a simple Siddhi application \u00b6 Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then publishes the output to the SI console and to a user-specified file. Open a text file and copy-paste following Siddhi application into it. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='xml'), file.uri='/Users/foo/low_productions.txt') @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, roundedAmount long); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, math:round(amount) as roundedAmount insert into TransformedProductionStream; Note The output of this application is written into a the file, specified via the file.uri parameter. Change the value for this parameter accordingly. Save this file as MySimpleApp.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info Once you deploy the above Siddhi application, it creates a new HTTP endpoint at http://localhost:8006/productionStream and starts listening to the endpoint for incoming messages. The incoming messages are then published to: 1. Streaming Integrator logs 2. To a file specified by you in XML format The next step is to publish a message to the endpoint that you created, via a CURL command. Testing your Siddhi application \u00b6 To test the MySimpleApp Siddhi application you created, execute following CURL command on the console. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Note that you published a message with a lower case name, i.e., sugar . However, the output you observe in the SI console is similar to following. INFO {io.siddhi.core.stream.output.sink.LogSink} - MySimpleApp : TransformedProductionStream : Event{timestamp=1563539561686, data=[SUGAR, 21], isExpired=false} Note that the output message has an uppercase name: SUGAR . In addition to that, the amount has being rounded. This is because of the simple message transformation carried out by the Siddhi application. In addition to this, open low_productions.txt file (i.e., the file that you specified via the file.uri parameter). The file should contain the following text. <events><event><nameInUpperCase>SUGAR</nameInUpperCase><roundedAmount>21</roundedAmount></event></events> What's next? \u00b6 The Streaming Integrator works seamlessly with the Micro Integrator to trigger integration flows based on the output it generates for streaming data. To try out a scenario where you process streaming data and trigger an integration flow via the Micro Integrator in five minutes, see Getting SI Running with MI in Five Minutes .","title":"Quick Start Guide"},{"location":"quick-start-guide/getting-started-with-si/#getting-started-with-streaming-integrator-in-five-minutes","text":"","title":"Getting Started with Streaming Integrator in Five Minutes"},{"location":"quick-start-guide/getting-started-with-si/#introduction","text":"This quick start guide gets you started with the Streaming Integrator (SI), in just 5 minutes. In this guide, you will download the SI distribution, start it and then try out a simple Siddhi application.","title":"Introduction"},{"location":"quick-start-guide/getting-started-with-si/#tutorial-outline","text":"Downloading Streaming Integrator Starting the server Deploying a simple Siddhi app","title":"Tutorial Outline"},{"location":"quick-start-guide/getting-started-with-si/#downloading-streaming-integrator","text":"Download the Streaming Integrator distribution from WSO2 Streaming Integrator site and extract it to a location of your choice. Hereafter, the extracted location is referred to as <SI_HOME> .","title":"Downloading Streaming Integrator"},{"location":"quick-start-guide/getting-started-with-si/#starting-the-server","text":"Navigate to the <SI_HOME>/bin directory in the console and issue the appropriate command depending on your operating system to start the Streaming Integrator. - For Windows: server.bat - For Linux/MacOS: ./server.sh","title":"Starting the server"},{"location":"quick-start-guide/getting-started-with-si/#deploying-a-simple-siddhi-application","text":"Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then publishes the output to the SI console and to a user-specified file. Open a text file and copy-paste following Siddhi application into it. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='xml'), file.uri='/Users/foo/low_productions.txt') @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, roundedAmount long); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, math:round(amount) as roundedAmount insert into TransformedProductionStream; Note The output of this application is written into a the file, specified via the file.uri parameter. Change the value for this parameter accordingly. Save this file as MySimpleApp.siddhi in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Info Once you deploy the above Siddhi application, it creates a new HTTP endpoint at http://localhost:8006/productionStream and starts listening to the endpoint for incoming messages. The incoming messages are then published to: 1. Streaming Integrator logs 2. To a file specified by you in XML format The next step is to publish a message to the endpoint that you created, via a CURL command.","title":"Deploying a simple Siddhi application"},{"location":"quick-start-guide/getting-started-with-si/#testing-your-siddhi-application","text":"To test the MySimpleApp Siddhi application you created, execute following CURL command on the console. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Note that you published a message with a lower case name, i.e., sugar . However, the output you observe in the SI console is similar to following. INFO {io.siddhi.core.stream.output.sink.LogSink} - MySimpleApp : TransformedProductionStream : Event{timestamp=1563539561686, data=[SUGAR, 21], isExpired=false} Note that the output message has an uppercase name: SUGAR . In addition to that, the amount has being rounded. This is because of the simple message transformation carried out by the Siddhi application. In addition to this, open low_productions.txt file (i.e., the file that you specified via the file.uri parameter). The file should contain the following text. <events><event><nameInUpperCase>SUGAR</nameInUpperCase><roundedAmount>21</roundedAmount></event></events>","title":"Testing your Siddhi application"},{"location":"quick-start-guide/getting-started-with-si/#whats-next","text":"The Streaming Integrator works seamlessly with the Micro Integrator to trigger integration flows based on the output it generates for streaming data. To try out a scenario where you process streaming data and trigger an integration flow via the Micro Integrator in five minutes, see Getting SI Running with MI in Five Minutes .","title":"What's next?"},{"location":"quick-start-guide/hello-world-with-docker/","text":"Getting the Streaming Integrator Running with Docker in 5 Minutes \u00b6 Introduction \u00b6 This guide shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment. Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here . Downloading and installing the Streaming Integrator \u00b6 In this scenario, you are downloading and installing the Streaming Integrator via Docker. WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 . To run the Streaming Integrator in the open source image that is available for it To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker run -it wso2/streaming-integrator Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports: The 9443 port where the Streaming Integrator server is run. The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages. To expose these ports, issue the following command. docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator Creating and deploying the Siddhi application \u00b6 Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console. Start the Streaming Integrator Tooling via one of the following methods depending on your operating system: On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si-tooling-<VERSION> On windows, go to Start Menu -> Programs -> WSO2 -> Streaming Integrator Tooling . A terminal opens. Then access the Streaming Integration Tooling via the http://<HOST_NAME>:<TOOLING_PORT>/editor URL. Info The default URL is http://<localhost:9390/editor . The Streaming Integration Tooling opens as shown below. Click New and copy-paste the following Siddhi application to the new file you opened. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, amount double); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, amount insert into TransformedProductionStream; Note Note the following about this Siddhi application. - The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is 0.0.0.0 . - The 8006 port of the receiver URL is the same HTTP port that you previously exposed via Docker. Save the Siddhi application by clicking File => Save . To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Field Value Host 0.0.0.0 Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. The following is logged in the console in which you started the Streaming Integrator in Docker. Trying-out the Siddhi application \u00b6 To try out the MySimpleApp Siddhi application you deployed in Docker, issue the following CURL command. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\" The following output appears in the console in which you started the Streaming Integrator in Docker.","title":"Getting the Streaming Integrator Running with Docker in 5 Minutes"},{"location":"quick-start-guide/hello-world-with-docker/#getting-the-streaming-integrator-running-with-docker-in-5-minutes","text":"","title":"Getting the Streaming Integrator Running with Docker in 5 Minutes"},{"location":"quick-start-guide/hello-world-with-docker/#introduction","text":"This guide shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment. Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here .","title":"Introduction"},{"location":"quick-start-guide/hello-world-with-docker/#downloading-and-installing-the-streaming-integrator","text":"In this scenario, you are downloading and installing the Streaming Integrator via Docker. WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 . To run the Streaming Integrator in the open source image that is available for it To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker run -it wso2/streaming-integrator Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports: The 9443 port where the Streaming Integrator server is run. The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages. To expose these ports, issue the following command. docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator","title":"Downloading and installing the Streaming Integrator"},{"location":"quick-start-guide/hello-world-with-docker/#creating-and-deploying-the-siddhi-application","text":"Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console. Start the Streaming Integrator Tooling via one of the following methods depending on your operating system: On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si-tooling-<VERSION> On windows, go to Start Menu -> Programs -> WSO2 -> Streaming Integrator Tooling . A terminal opens. Then access the Streaming Integration Tooling via the http://<HOST_NAME>:<TOOLING_PORT>/editor URL. Info The default URL is http://<localhost:9390/editor . The Streaming Integration Tooling opens as shown below. Click New and copy-paste the following Siddhi application to the new file you opened. @App:name('MySimpleApp') @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TransformedProductionStream (nameInUpperCase string, amount double); -- Simple Siddhi query to transform the name to upper case. from SweetProductionStream select str:upper(name) as nameInUpperCase, amount insert into TransformedProductionStream; Note Note the following about this Siddhi application. - The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is 0.0.0.0 . - The 8006 port of the receiver URL is the same HTTP port that you previously exposed via Docker. Save the Siddhi application by clicking File => Save . To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Field Value Host 0.0.0.0 Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. The following is logged in the console in which you started the Streaming Integrator in Docker.","title":"Creating and deploying the Siddhi application"},{"location":"quick-start-guide/hello-world-with-docker/#trying-out-the-siddhi-application","text":"To try out the MySimpleApp Siddhi application you deployed in Docker, issue the following CURL command. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\" The following output appears in the console in which you started the Streaming Integrator in Docker.","title":"Trying-out the Siddhi application"},{"location":"quick-start-guide/hello-world-with-kubernetes/","text":"Getting the Streaming Integrator Running in Kubernetes in 5 Minutes \u00b6 Introduction \u00b6 This quick start guide gets you to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes. Before you begin: Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows. Install Minikube and start a cluster by following the Minikube Documentation . Enable ingress on Minikube by issuing the following command. minikube addons enable ingress Make sure that you have admin privileges to install the Siddhi operator . Installing the Siddhi Operator for the Streaming Integrator \u00b6 To install the Siddhi Operator, follow the procedure below: To install the Siddhi Kubernetes operator for streaming integrator issue the following commands: kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml To verify whether the Siddhi operator is successfully installed, issue the following command. kubectl get deployment If the installation is successful, the following deployments should be running in the Kubernetes cluster. Deploying Siddhi applications in Kubernetes \u00b6 You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster. First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for power . @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named spec as shown below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" Here, you are specifying that Siddhi applications running within the container should receive events to the http://0.0.0.0:8080/checkPower URL and basic authentication is not enabled for them. Add a runner section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows. runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* To view the complete file, click here. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* Save the file as siddhi-process.yaml in a preferred location To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command. kubectl apply -f <PATH_to_siddhi-process.yaml> Info This file overrules the configurations in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. Invoking the Siddhi application \u00b6 To invoke the PowerSurgeDetection Siddhi application that you deployed in the Kubernetes cluster, follow the steps below. First, get the external IP of minikube by issuing the following command. minikube ip Add the IP it returns to the /etc/hosts file in your machine. Issue the following CURL command to invoke the PowerSurgeDetection Siddhi application. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command. `kubectl get pods' This returns the list of pods as shown in the example below. NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is streaming-integrator-app-0-b4dcf85-npgj7 . streaming-integrator-app-0-b4dcf85-npgj7","title":"Getting the Streaming Integrator Running in Kubernetes in 5 Minutes"},{"location":"quick-start-guide/hello-world-with-kubernetes/#getting-the-streaming-integrator-running-in-kubernetes-in-5-minutes","text":"","title":"Getting the Streaming Integrator Running in Kubernetes in 5 Minutes"},{"location":"quick-start-guide/hello-world-with-kubernetes/#introduction","text":"This quick start guide gets you to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes. Before you begin: Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows. Install Minikube and start a cluster by following the Minikube Documentation . Enable ingress on Minikube by issuing the following command. minikube addons enable ingress Make sure that you have admin privileges to install the Siddhi operator .","title":"Introduction"},{"location":"quick-start-guide/hello-world-with-kubernetes/#installing-the-siddhi-operator-for-the-streaming-integrator","text":"To install the Siddhi Operator, follow the procedure below: To install the Siddhi Kubernetes operator for streaming integrator issue the following commands: kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml To verify whether the Siddhi operator is successfully installed, issue the following command. kubectl get deployment If the installation is successful, the following deployments should be running in the Kubernetes cluster.","title":"Installing the Siddhi Operator for the Streaming Integrator"},{"location":"quick-start-guide/hello-world-with-kubernetes/#deploying-siddhi-applications-in-kubernetes","text":"You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster. First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for power . @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named spec as shown below. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed. container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" Here, you are specifying that Siddhi applications running within the container should receive events to the http://0.0.0.0:8080/checkPower URL and basic authentication is not enabled for them. Add a runner section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows. runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* To view the complete file, click here. apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata: name: streaming-integrator-app spec: apps: - script: | @App:name(\"PowerSurgeDetection\") @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\") /* Input: deviceType string and powerConsuption int(Watt) Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W. */ @source( type='http', receiver.url='${RECEIVER_URL}', basic.auth.enabled='false', @map(type='json') ) define stream DevicePowerStream(deviceType string, power int); @sink(type='log', prefix='LOGGER') define stream PowerSurgeAlertStream(deviceType string, power int); @info(name='surge-detector') from DevicePowerStream[deviceType == 'dryer' and power >= 600] select deviceType, power insert into PowerSurgeAlertStream; container: env: - name: RECEIVER_URL value: \"http://0.0.0.0:8080/checkPower\" - name: BASIC_AUTH_ENABLED value: \"false\" runner: | auth.configs: type: 'local' # Type of the IdP client used userManager: adminRole: admin # Admin role which is granted all permissions userStore: # User store users: - user: username: root password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: root restAPIAuthConfigs: exclude: - /simulation/* - /stores/* Save the file as siddhi-process.yaml in a preferred location To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command. kubectl apply -f <PATH_to_siddhi-process.yaml> Info This file overrules the configurations in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file.","title":"Deploying Siddhi applications in Kubernetes"},{"location":"quick-start-guide/hello-world-with-kubernetes/#invoking-the-siddhi-application","text":"To invoke the PowerSurgeDetection Siddhi application that you deployed in the Kubernetes cluster, follow the steps below. First, get the external IP of minikube by issuing the following command. minikube ip Add the IP it returns to the /etc/hosts file in your machine. Issue the following CURL command to invoke the PowerSurgeDetection Siddhi application. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command. `kubectl get pods' This returns the list of pods as shown in the example below. NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is streaming-integrator-app-0-b4dcf85-npgj7 . streaming-integrator-app-0-b4dcf85-npgj7","title":"Invoking the Siddhi application"},{"location":"quick-start-guide/hello-world-with-mi/","text":"Getting SI Running with MI in Five Minutes \u00b6 This quick start guide explains how to trigger an integration flow using a message received by the Streaming Integrator. In this example, the same message you send to the Micro Integrator goes through the inSeq defined, and uses the respond mediator to send back the response to the Streaming integrator. Before you begin: Download and install both the Streaming Integrator and the Streaming Integrator Tooling from here . Download and install both the the Micro Integrator from here . Start the Streaming Integrator via one of the following methods depending on your operating system. On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si On windows, go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator . This opens a terminal. Start Streaming Integrator profile. Start the Streaming Integrator Tooling via one of the following methods depending on your operating system.. On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si-tooling-<VERSION> On windows, go to Start Menu -> Programs -> WSO2 -> Streaming Integrator Tooling . A terminal opens. To create and deploy Siddhi application that triggers an integration flow, and then try it out by sending events, follow the procedure below: Access Streaming Integrator Tooling via the URL printed in the start up logs. Info The default URL is http://localhost:9390/editor. The Streaming Integrator Tooling opens as follows. Open a new Siddhi file by clicking New . Then copy and paste the following Siddhi application to it. @App:name(\"grpc-call-response\") @App:description(\"This siddhi app triggers integration flow from SI to MI\") @source(type='http', receiver.url='http://localhost:8006/inputstream', @map(type = 'json')) define stream InputStream (message String, headers string); @sink(type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='{{headers}}', @map(type='json')) define stream FooStream (message String, headers string); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (message String, headers string); @sink(type='log') define stream OutputStream (message String, headers string); from InputStream select * insert into FooStream; from BarStream select * insert into OutputStream Save the Siddhi application. Click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the grpc-call-response.siddhi Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. As a result, the grpc-call-response.siddhi Siddhi application is saved in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Deploy the required artifacts in the Micro Integrator as follows: Save the following GRPC inbound endpoint as grpcInboundEndpoint.xml in the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Save the following sequence that includes a respond mediator in the <MI_Home>/repository/deployment/server/synapse-configs/default/sequences directory. You can name the file InSeq.xml . <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> Issue the following CURL command to send an event to the Streaming Integrator. This is received via the http source configured in the grpc-call-response Siddhi application, and it triggers an integration flow with the Micro Integrator. curl -X POST -d \"{\\\"event\\\":{\\\"message\\\":\\\"http_curl\\\",\\\"headers\\\":\\\"'Content-Type:json'\\\"}}\" http://localhost:8006/inputstream --header \"Content-Type:application/json\" The following response is logged in the console in which you are running the SI server. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloMi : OutputStream : Event{timestamp=1573188022317, data=[http_curl, 'Content-Type:json'], isExpired=false}","title":"Running the Streaming Integrator with Micro Integrator"},{"location":"quick-start-guide/hello-world-with-mi/#getting-si-running-with-mi-in-five-minutes","text":"This quick start guide explains how to trigger an integration flow using a message received by the Streaming Integrator. In this example, the same message you send to the Micro Integrator goes through the inSeq defined, and uses the respond mediator to send back the response to the Streaming integrator. Before you begin: Download and install both the Streaming Integrator and the Streaming Integrator Tooling from here . Download and install both the the Micro Integrator from here . Start the Streaming Integrator via one of the following methods depending on your operating system. On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si On windows, go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator . This opens a terminal. Start Streaming Integrator profile. Start the Streaming Integrator Tooling via one of the following methods depending on your operating system.. On MacOS/Linux/CentOS, open a terminal and issue the following command: sudo wso2si-tooling-<VERSION> On windows, go to Start Menu -> Programs -> WSO2 -> Streaming Integrator Tooling . A terminal opens. To create and deploy Siddhi application that triggers an integration flow, and then try it out by sending events, follow the procedure below: Access Streaming Integrator Tooling via the URL printed in the start up logs. Info The default URL is http://localhost:9390/editor. The Streaming Integrator Tooling opens as follows. Open a new Siddhi file by clicking New . Then copy and paste the following Siddhi application to it. @App:name(\"grpc-call-response\") @App:description(\"This siddhi app triggers integration flow from SI to MI\") @source(type='http', receiver.url='http://localhost:8006/inputstream', @map(type = 'json')) define stream InputStream (message String, headers string); @sink(type='grpc-call', publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', sink.id= '1', headers='{{headers}}', @map(type='json')) define stream FooStream (message String, headers string); @source(type='grpc-call-response', sink.id= '1', @map(type='json')) define stream BarStream (message String, headers string); @sink(type='log') define stream OutputStream (message String, headers string); from InputStream select * insert into FooStream; from BarStream select * insert into OutputStream Save the Siddhi application. Click the Deploy menu option and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the grpc-call-response.siddhi Siddhi application and the server you added as shown below. Click Deploy . When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box. As a result, the grpc-call-response.siddhi Siddhi application is saved in the <SI_HOME>/wso2/server/deployment/siddhi-files directory. Deploy the required artifacts in the Micro Integrator as follows: Save the following GRPC inbound endpoint as grpcInboundEndpoint.xml in the <MI_Home>/repository/deployment/server/synapse-configs/default/inbound-endpoints directory. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"> <parameters> <parameter name=\"inbound.grpc.port\">8888</parameter> </parameters> </inboundEndpoint> Save the following sequence that includes a respond mediator in the <MI_Home>/repository/deployment/server/synapse-configs/default/sequences directory. You can name the file InSeq.xml . <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"> <log level=\"full\"/> <respond/> </sequence> Issue the following CURL command to send an event to the Streaming Integrator. This is received via the http source configured in the grpc-call-response Siddhi application, and it triggers an integration flow with the Micro Integrator. curl -X POST -d \"{\\\"event\\\":{\\\"message\\\":\\\"http_curl\\\",\\\"headers\\\":\\\"'Content-Type:json'\\\"}}\" http://localhost:8006/inputstream --header \"Content-Type:application/json\" The following response is logged in the console in which you are running the SI server. INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloMi : OutputStream : Event{timestamp=1573188022317, data=[http_curl, 'Content-Type:json'], isExpired=false}","title":"Getting SI Running with MI in Five Minutes"},{"location":"quick-start-guide/quick-start-guide-101/","text":"Quick Start Guide 101 \u00b6 Introduction to Streaming Integration \u00b6 The Streaming Integrator is one of the integrators in WSO2 Enterprise Integrator. It reads streaming data from files, cloud-based applications, streaming applications, and databases, and processes them. It also allows downstream applications to access streaming data by publishing information in a streaming manner. It can analyze streaming data, identify trends and patterns, and trigger integration flows. The purpose of this guide if for you to understand the basic functions of the Streaming Integrator in 30 minutes. To learn how to use the key functions of the Streaming Integrator, consider a laboratory that is observing the temperature of a range of rooms in a building via a sensor and needs to use the temperature readings as the input to derive other information. Before you begin: Install Oracle Java SE Development Kit (JDK) version 1.8 . Set the Java home environment variable. Download and install the following components of the Streaming Integrator: Streaming Integrator Tooling Streaming Integrator runtime Creating your first Siddhi application \u00b6 Create a basic Siddhi application for a simple use case. Extract the Streaming Integrator Tooling pack to a preferred location. Hereafter, the extracted location is referred to as <SI_TOOLING_HOME> . Navigate to the <SI_TOOLING_HOME>/bin directory and issue the appropriate command depending on your operating system to start the Streaming Integration tooling. For Windows: tooling.bat For Linux/MacOS: ./tooling.sh Access the Streaming Integration Tooling via the http://<HOST_NAME>:<TOOLING_PORT>/editor URL. Info The default URL is http://<localhost:9390/editor . The Streaming Integration Tooling opens as shown below. Open a new Siddhi file by clicking New . The new file opens as follows. Specify a name for the new Siddhi application via the @App:name annotation, and a description via the @App:description annotation. @App:name(\"TemperatureApp\") @App:description(\"This application captures the room temperature and analyzes it, and presents the results as logs in the output console.\") The details to be captures include the room ID, device ID, and the temperature. To specify this, define an input stream with attributes to capture each of these details. define stream TempStream(roomNo string, deviceNo long, temp double) The technicians need to know the average temperature with each new temperature reading. To publish this information, define an output stream including these details as attributes in the schema. define stream AverageTempStream(roomNo string, deviceNo long, avgTemp double) The average temperature needs to be logged. Therefore, connect a sink of the log type to the output stream as shown below. @sink(type = 'log', @map(type = 'passThrough')) define stream AverageTempStream (roomNo string, deviceID long, avgTemp double); passThrough is specified as the mapping type because in this scenario, the attribute names are received as they are defined in the stream and do not need to be mapped. To get the input events, calculate the average temperature and direct the results to the output stream, add a query below the stream definitions as follows: To name the query, add the @info annotation and enter CalculateAverageTemperature as the query name. @info(name = 'CalculateAvgTemp') To indicate that the input is taken from the TempStream stream, add the from clause as follows: from TempStream Specify how the values for the output stream attributes are derived by adding a select clause as follows. select roomNo, deviceNo, avg(temp) To insert the results into the output stream, add the insert into clause as follows. insert into AverageTempStream; The completed Siddhi application is as follows: @App:name('TemperatureApp') @App:description('This application captures the room temperature and analyzes it, and presents the results as logs in the output console.') define stream TempStream (roomNo string, deviceID long, temp double); @sink(type = 'log', @map(type = 'passThrough')) define stream AverageTempStream (roomNo string, deviceID long, avgTemp double); @info(name = 'CalculateAvgTemp') from TempStream select roomNo, deviceID, avg(temp) as avgTemp insert into AverageTempStream; Testing your Siddhi application \u00b6 The application you created needs to be tested before he uses it to process the actual data received. You can test it in the following methods: Simulating events \u00b6 To simulate events for the Siddhi application, you can use the event simulator available with in the Streaming Integration Tooling as explained in the procedure below. In the Streaming Integrator Tooling, click the following icon for event simulation on the side panel. The Simulation panel opens as shown below. In the Single Simulation tab of the simulation panel, select TemperatureApp from the list for the Siddhi App Name field. You need to send events to the input stream. Therefore, select TempStream\" in the **Stream Name field. As a result, the attribute list appears in the simulation panel. Then enter values for the attributes as follows: .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Attribute Value roomNo NW106 deviceID 262626367171371717 temp 26 Click Start and Send . The output is logged in the console as follows: Debugging \u00b6 To debug your Siddhi application, you need to mark debug points, and then simulate events as you did in the previous section. The complete procedure is as follows: Open the TemperatureApp Siddhi application. To run the Siddhi application in the debug mode, click Run => Debug , or click the following icon for debugging. As a result, the Debug console opens in a new tab below the Siddhi application as follows. Apply debug points in the lines with the from and insert into clauses. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into clauses as debug points. Now simulate a single event the same way you simulated it in the previous section, with the following values. .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Attribute Value roomNo NW106 deviceID 262626367171371717 temp 26 Click Send to send the event. When a debug point is hit, the line marked as a debug point is highlighted as shown below. The status of the event and the query is displayed in the debug console below the Siddhi application. Deploying Siddhi applications \u00b6 After creating and testing the TemperatureApp Siddhi application, you need to deploy it in the Streaming Integrator server, export it as a Docker image, or deploy in Kubernetes. Deploying in Streaming Integrator server \u00b6 To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below: Info To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <SI_HOME> and the home directory of Streaming Integrator Tooling is referred to as <SI_TOOLING_HOME> . Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing the appropriate command based on your operating system: For Windows: server.bat --run For Linux/Mac OS: ./server.sh In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as follows. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the TemperatureApp.siddhi Siddhi application and the server you added as shown below. Click Deploy . As a result, the TemperatureApp Siddhi application is saved in the <SI_HOME>/deployment/siddhi-files directory, and the following is message displayed in the dialog box. Deploying in Docker \u00b6 To export the TemperatureApp Siddhi application as a Docker artifact, follow the procedure below: Open the Streaming Integrator Tooling. Click Export in the top menu, and then click For Docker . As a result, Step 1 of the Export Siddhi Apps for Docker image wizard opens as follows. Select the TemperatureApp.siddhi check box and click Next . In Step 2 , you can template values of the Siddhi Application. Click Next without templating any value of the Siddhi application. Info For detailed information about templating the values of a Siddhi Application, see Exporting Siddhi Apps for Docker Image . In Step 3 , you can update configurations of the Streaming Integrator. Leave the default configurations, and click Next . In Step 4 , you can provide arguments for the values that were templated in Step 2 . There are no values to be configured because you did not template any values in Step 2 . Therefore click Next . In Step 5 , you can choose additional dependencies to be bundled. This is applicable when Sources, Sinks and etc. with additional dependencies are used in the Siddhi Application (e.g., a Kafka Source/Sink, or a MongoDB Store). In this scenario, there are no such dependencies. Therefore nothing is shown as additional JARs. Click Export . The Siddhi application is exported as a Docker artifact in a zip file to the default location in your machine, based on your operating system and browser settings. Extending the Streaming Integrator \u00b6 The Streaming Integrator is by default shipped with most of the available Siddhi extensions by default. If a Siddhi extension you require is not shipped by default, you can download and install it. In this scenario, let's assume that the laboratories require the siddhi-execution-extrema extension to carry out more advanced calculations for different types of time windows. To download and install it, follow the procedure below: Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. Search for the siddhi-execution-extrema extension. Click on the V4.1.1 for this scenario. As a result, the following page opens. To download the extension, click Download Extension . Then enter your email address in the dialog box that appears, and click Submit . As a result, a JAR fil is downloaded to a location in your machine (the location depends on your browser settings). To install the siddhi-execution-extrema extension in your Streaming Integrator, place the JAR file you downloaded in the <SI_HOME>/lib directory . Further references \u00b6 For a quicker demonstration of the Streaming Integrator, see Getting Started with the Streaming Integrator in Five Minutes . For a quick guide on how the Streaming Integrator works with the Micro Integrator to trigger integration flows, see [Getting SI Running with MI in 5 Minutes]. To get the Streaming Integrator running with Docker in five minutes, see Getting SI Running with Docker in 5 Minutes . To get the Streaming Integrator running in a Kubernetes cluster in five minutes, see Getting SI Running with Kubernetes in 5 Minutes .","title":"Streaming Integrator 101"},{"location":"quick-start-guide/quick-start-guide-101/#quick-start-guide-101","text":"","title":"Quick Start Guide 101"},{"location":"quick-start-guide/quick-start-guide-101/#introduction-to-streaming-integration","text":"The Streaming Integrator is one of the integrators in WSO2 Enterprise Integrator. It reads streaming data from files, cloud-based applications, streaming applications, and databases, and processes them. It also allows downstream applications to access streaming data by publishing information in a streaming manner. It can analyze streaming data, identify trends and patterns, and trigger integration flows. The purpose of this guide if for you to understand the basic functions of the Streaming Integrator in 30 minutes. To learn how to use the key functions of the Streaming Integrator, consider a laboratory that is observing the temperature of a range of rooms in a building via a sensor and needs to use the temperature readings as the input to derive other information. Before you begin: Install Oracle Java SE Development Kit (JDK) version 1.8 . Set the Java home environment variable. Download and install the following components of the Streaming Integrator: Streaming Integrator Tooling Streaming Integrator runtime","title":"Introduction to Streaming Integration"},{"location":"quick-start-guide/quick-start-guide-101/#creating-your-first-siddhi-application","text":"Create a basic Siddhi application for a simple use case. Extract the Streaming Integrator Tooling pack to a preferred location. Hereafter, the extracted location is referred to as <SI_TOOLING_HOME> . Navigate to the <SI_TOOLING_HOME>/bin directory and issue the appropriate command depending on your operating system to start the Streaming Integration tooling. For Windows: tooling.bat For Linux/MacOS: ./tooling.sh Access the Streaming Integration Tooling via the http://<HOST_NAME>:<TOOLING_PORT>/editor URL. Info The default URL is http://<localhost:9390/editor . The Streaming Integration Tooling opens as shown below. Open a new Siddhi file by clicking New . The new file opens as follows. Specify a name for the new Siddhi application via the @App:name annotation, and a description via the @App:description annotation. @App:name(\"TemperatureApp\") @App:description(\"This application captures the room temperature and analyzes it, and presents the results as logs in the output console.\") The details to be captures include the room ID, device ID, and the temperature. To specify this, define an input stream with attributes to capture each of these details. define stream TempStream(roomNo string, deviceNo long, temp double) The technicians need to know the average temperature with each new temperature reading. To publish this information, define an output stream including these details as attributes in the schema. define stream AverageTempStream(roomNo string, deviceNo long, avgTemp double) The average temperature needs to be logged. Therefore, connect a sink of the log type to the output stream as shown below. @sink(type = 'log', @map(type = 'passThrough')) define stream AverageTempStream (roomNo string, deviceID long, avgTemp double); passThrough is specified as the mapping type because in this scenario, the attribute names are received as they are defined in the stream and do not need to be mapped. To get the input events, calculate the average temperature and direct the results to the output stream, add a query below the stream definitions as follows: To name the query, add the @info annotation and enter CalculateAverageTemperature as the query name. @info(name = 'CalculateAvgTemp') To indicate that the input is taken from the TempStream stream, add the from clause as follows: from TempStream Specify how the values for the output stream attributes are derived by adding a select clause as follows. select roomNo, deviceNo, avg(temp) To insert the results into the output stream, add the insert into clause as follows. insert into AverageTempStream; The completed Siddhi application is as follows: @App:name('TemperatureApp') @App:description('This application captures the room temperature and analyzes it, and presents the results as logs in the output console.') define stream TempStream (roomNo string, deviceID long, temp double); @sink(type = 'log', @map(type = 'passThrough')) define stream AverageTempStream (roomNo string, deviceID long, avgTemp double); @info(name = 'CalculateAvgTemp') from TempStream select roomNo, deviceID, avg(temp) as avgTemp insert into AverageTempStream;","title":"Creating your first Siddhi application"},{"location":"quick-start-guide/quick-start-guide-101/#testing-your-siddhi-application","text":"The application you created needs to be tested before he uses it to process the actual data received. You can test it in the following methods:","title":"Testing your Siddhi application"},{"location":"quick-start-guide/quick-start-guide-101/#simulating-events","text":"To simulate events for the Siddhi application, you can use the event simulator available with in the Streaming Integration Tooling as explained in the procedure below. In the Streaming Integrator Tooling, click the following icon for event simulation on the side panel. The Simulation panel opens as shown below. In the Single Simulation tab of the simulation panel, select TemperatureApp from the list for the Siddhi App Name field. You need to send events to the input stream. Therefore, select TempStream\" in the **Stream Name field. As a result, the attribute list appears in the simulation panel. Then enter values for the attributes as follows: .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Attribute Value roomNo NW106 deviceID 262626367171371717 temp 26 Click Start and Send . The output is logged in the console as follows:","title":"Simulating events"},{"location":"quick-start-guide/quick-start-guide-101/#debugging","text":"To debug your Siddhi application, you need to mark debug points, and then simulate events as you did in the previous section. The complete procedure is as follows: Open the TemperatureApp Siddhi application. To run the Siddhi application in the debug mode, click Run => Debug , or click the following icon for debugging. As a result, the Debug console opens in a new tab below the Siddhi application as follows. Apply debug points in the lines with the from and insert into clauses. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into clauses as debug points. Now simulate a single event the same way you simulated it in the previous section, with the following values. .tg {border-collapse:collapse;border-spacing:0;} .tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Attribute Value roomNo NW106 deviceID 262626367171371717 temp 26 Click Send to send the event. When a debug point is hit, the line marked as a debug point is highlighted as shown below. The status of the event and the query is displayed in the debug console below the Siddhi application.","title":"Debugging"},{"location":"quick-start-guide/quick-start-guide-101/#deploying-siddhi-applications","text":"After creating and testing the TemperatureApp Siddhi application, you need to deploy it in the Streaming Integrator server, export it as a Docker image, or deploy in Kubernetes.","title":"Deploying Siddhi applications"},{"location":"quick-start-guide/quick-start-guide-101/#deploying-in-streaming-integrator-server","text":"To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below: Info To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <SI_HOME> and the home directory of Streaming Integrator Tooling is referred to as <SI_TOOLING_HOME> . Start the Streaming Integrator server by navigating to the <SI_HOME>/bin directory from the CLI, and issuing the appropriate command based on your operating system: For Windows: server.bat --run For Linux/Mac OS: ./server.sh In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server . The Deploy Siddhi Apps to Server dialog box opens as follows. In the Add New Server section, enter information as follows: Field Value Host Your host Port 9443 User Name admin Password admin Then click Add . Select the check boxes for the TemperatureApp.siddhi Siddhi application and the server you added as shown below. Click Deploy . As a result, the TemperatureApp Siddhi application is saved in the <SI_HOME>/deployment/siddhi-files directory, and the following is message displayed in the dialog box.","title":"Deploying in Streaming Integrator server"},{"location":"quick-start-guide/quick-start-guide-101/#deploying-in-docker","text":"To export the TemperatureApp Siddhi application as a Docker artifact, follow the procedure below: Open the Streaming Integrator Tooling. Click Export in the top menu, and then click For Docker . As a result, Step 1 of the Export Siddhi Apps for Docker image wizard opens as follows. Select the TemperatureApp.siddhi check box and click Next . In Step 2 , you can template values of the Siddhi Application. Click Next without templating any value of the Siddhi application. Info For detailed information about templating the values of a Siddhi Application, see Exporting Siddhi Apps for Docker Image . In Step 3 , you can update configurations of the Streaming Integrator. Leave the default configurations, and click Next . In Step 4 , you can provide arguments for the values that were templated in Step 2 . There are no values to be configured because you did not template any values in Step 2 . Therefore click Next . In Step 5 , you can choose additional dependencies to be bundled. This is applicable when Sources, Sinks and etc. with additional dependencies are used in the Siddhi Application (e.g., a Kafka Source/Sink, or a MongoDB Store). In this scenario, there are no such dependencies. Therefore nothing is shown as additional JARs. Click Export . The Siddhi application is exported as a Docker artifact in a zip file to the default location in your machine, based on your operating system and browser settings.","title":"Deploying in Docker"},{"location":"quick-start-guide/quick-start-guide-101/#extending-the-streaming-integrator","text":"The Streaming Integrator is by default shipped with most of the available Siddhi extensions by default. If a Siddhi extension you require is not shipped by default, you can download and install it. In this scenario, let's assume that the laboratories require the siddhi-execution-extrema extension to carry out more advanced calculations for different types of time windows. To download and install it, follow the procedure below: Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. Search for the siddhi-execution-extrema extension. Click on the V4.1.1 for this scenario. As a result, the following page opens. To download the extension, click Download Extension . Then enter your email address in the dialog box that appears, and click Submit . As a result, a JAR fil is downloaded to a location in your machine (the location depends on your browser settings). To install the siddhi-execution-extrema extension in your Streaming Integrator, place the JAR file you downloaded in the <SI_HOME>/lib directory .","title":"Extending the Streaming Integrator"},{"location":"quick-start-guide/quick-start-guide-101/#further-references","text":"For a quicker demonstration of the Streaming Integrator, see Getting Started with the Streaming Integrator in Five Minutes . For a quick guide on how the Streaming Integrator works with the Micro Integrator to trigger integration flows, see [Getting SI Running with MI in 5 Minutes]. To get the Streaming Integrator running with Docker in five minutes, see Getting SI Running with Docker in 5 Minutes . To get the Streaming Integrator running in a Kubernetes cluster in five minutes, see Getting SI Running with Kubernetes in 5 Minutes .","title":"Further references"},{"location":"ref/authentication-APIs/","text":"Authentication APIs \u00b6 Log in to a dashboard application Log out of the dashboard application Redirect URL for login using authorization grant type Log in to a dashboard application \u00b6 Overview \u00b6 Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard Parameter description \u00b6 Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true curl command syntax \u00b6 curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username}&password={password}&grantType={grantTypr}&rememberMe={rememberMe}\" Sample curl command \u00b6 curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin&password=admin&grantType=password\" Sample output \u00b6 {\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600} Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Log out of the dashboard application \u00b6 Overview \u00b6 Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard curl command syntax \u00b6 curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\" Sample curl command \u00b6 curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\" Sample output \u00b6 N/A Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Redirect URL for login using authorization grant type \u00b6 Overview \u00b6 Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord Parameter description \u00b6 Parameter Description {appName} The application of which the URL needs to be redirected. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/login/callback/portal\" Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Authentication APIs"},{"location":"ref/authentication-APIs/#authentication-apis","text":"Log in to a dashboard application Log out of the dashboard application Redirect URL for login using authorization grant type","title":"Authentication APIs"},{"location":"ref/authentication-APIs/#log-in-to-a-dashboard-application","text":"","title":"Log in to a dashboard application"},{"location":"ref/authentication-APIs/#overview","text":"Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard","title":"Overview"},{"location":"ref/authentication-APIs/#parameter-description","text":"Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true","title":"Parameter description"},{"location":"ref/authentication-APIs/#curl-command-syntax","text":"curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username}&password={password}&grantType={grantTypr}&rememberMe={rememberMe}\"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command","text":"curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin&password=admin&grantType=password\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output","text":"{\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600}","title":"Sample output"},{"location":"ref/authentication-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authentication-APIs/#log-out-of-the-dashboard-application","text":"","title":"Log out of the dashboard application"},{"location":"ref/authentication-APIs/#overview_1","text":"Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard","title":"Overview"},{"location":"ref/authentication-APIs/#curl-command-syntax_1","text":"curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command_1","text":"curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output_1","text":"N/A","title":"Sample output"},{"location":"ref/authentication-APIs/#response_1","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authentication-APIs/#redirect-url-for-login-using-authorization-grant-type","text":"","title":"Redirect URL for login using authorization grant type"},{"location":"ref/authentication-APIs/#overview_2","text":"Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord","title":"Overview"},{"location":"ref/authentication-APIs/#parameter-description_1","text":"Parameter Description {appName} The application of which the URL needs to be redirected.","title":"Parameter description"},{"location":"ref/authentication-APIs/#curl-command-syntax_2","text":"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command_2","text":"curl -X GET \"https://localhost:9643/login/callback/portal\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output_2","text":"","title":"Sample output"},{"location":"ref/authentication-APIs/#response_2","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authorization-Permission-Model/","text":"Authorization & Permission Model \u00b6 This page will provide information about the permission model of REST APIs available in each runtime of WSO2 Stream Processor. If you want the complete set of REST APIs which available in WSO2 SP, please find it here . You can find the REST APIs for each runtime and its permission model in the below pages. Worker Runtime - REST APIs Permission Model Manager Runtime - REST APIs Permission Model","title":"Authorization & Permission Model"},{"location":"ref/authorization-Permission-Model/#authorization-permission-model","text":"This page will provide information about the permission model of REST APIs available in each runtime of WSO2 Stream Processor. If you want the complete set of REST APIs which available in WSO2 SP, please find it here . You can find the REST APIs for each runtime and its permission model in the below pages. Worker Runtime - REST APIs Permission Model Manager Runtime - REST APIs Permission Model","title":"Authorization &amp; Permission Model"},{"location":"ref/business-Rules-APIs/","text":"Business Rules APIs \u00b6 Lists available business rule instances Delete business rule with given UUID Fetch template group with the given UUID Fetch rule templates of the template group with given UUID Fetch rule template of specific UUID available under a template group with specific UUID Fetch available template groups Fetch business rule instance with given UUID Create and save a business rule Update business rules instance with given UUID Lists available business rule instances \u00b6 Overview \u00b6 Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Delete business rule with given UUID \u00b6 Overview \u00b6 Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted. curl command syntax \u00b6 Sample curl command \u00b6 curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch template group with the given UUID \u00b6 Overview \u00b6 Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule templates of the template group with given UUID \u00b6 Overview \u00b6 Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule template of specific UUID available under a template group with specific UUID \u00b6 Overview \u00b6 Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch available template groups \u00b6 Overview \u00b6 Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch business rule instance with given UUID \u00b6 Overview \u00b6 Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Create and save a business rule \u00b6 Overview \u00b6 Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {deploymentStatus} curl command syntax \u00b6 Sample curl command \u00b6 curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Update business rules instance with given UUID \u00b6 Overview \u00b6 Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus} curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Business Rules APIs"},{"location":"ref/business-Rules-APIs/#business-rules-apis","text":"Lists available business rule instances Delete business rule with given UUID Fetch template group with the given UUID Fetch rule templates of the template group with given UUID Fetch rule template of specific UUID available under a template group with specific UUID Fetch available template groups Fetch business rule instance with given UUID Create and save a business rule Update business rules instance with given UUID","title":"Business Rules APIs"},{"location":"ref/business-Rules-APIs/#lists-available-business-rule-instances","text":"","title":"Lists available business rule instances"},{"location":"ref/business-Rules-APIs/#overview","text":"Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#curl-command-syntax","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command","text":"curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#delete-business-rule-with-given-uuid","text":"","title":"Delete business rule with given UUID"},{"location":"ref/business-Rules-APIs/#overview_1","text":"Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description","text":"Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_1","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_1","text":"curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_1","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_1","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-template-group-with-the-given-uuid","text":"","title":"Fetch template group with the given UUID"},{"location":"ref/business-Rules-APIs/#overview_2","text":"Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_1","text":"Parameter Description {templateGroupID} The UUID of the template group to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_2","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_2","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_2","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_2","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-rule-templates-of-the-template-group-with-given-uuid","text":"","title":"Fetch rule templates of the template group with given UUID"},{"location":"ref/business-Rules-APIs/#overview_3","text":"Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_2","text":"Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_3","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_3","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_3","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-rule-template-of-specific-uuid-available-under-a-template-group-with-specific-uuid","text":"","title":"Fetch rule template of specific UUID available under a template group with specific UUID"},{"location":"ref/business-Rules-APIs/#overview_4","text":"Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_3","text":"Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_4","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_4","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_4","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-available-template-groups","text":"","title":"Fetch available template groups"},{"location":"ref/business-Rules-APIs/#overview_5","text":"Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_5","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_5","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_5","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-business-rule-instance-with-given-uuid","text":"","title":"Fetch business rule instance with given UUID"},{"location":"ref/business-Rules-APIs/#overview_6","text":"Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_4","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_6","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_6","text":"curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_6","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_6","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#create-and-save-a-business-rule","text":"","title":"Create and save a business rule"},{"location":"ref/business-Rules-APIs/#overview_7","text":"Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_5","text":"Parameter Description {deploymentStatus}","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_7","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_7","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_7","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#update-business-rules-instance-with-given-uuid","text":"","title":"Update business rules instance with given UUID"},{"location":"ref/business-Rules-APIs/#overview_8","text":"Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_6","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus}","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_8","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_8","text":"curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_8","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_8","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/configuring-default-ports/","text":"Configuring Default Ports \u00b6 This page describes the default ports that are used for each runtime when the port offset is 0 . Common Ports \u00b6 The following ports are common to all runtimes. 7611 Thrift TCP port to receive events from clients. 7711 Thrift SSL port for secure transport where the client is authenticated. 9611 Binary TCP port to receive events from clients. 9711 Binary SSL port for secure transport where the client is authenticated. You can offset binary and thrift by configuring the offset parameter in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. The following is a sample configuration. # Carbon Configuration Parameters wso2.carbon: # value to uniquely identify a server id: wso2-si # server name name: WSO2 Streaming Integrator # server type type: wso2-si # ports used by this server ports: # port offset offset: 1 Server runtime \u00b6 9090 HTTP netty transport 9443 HTTPS netty transport Streaming Integrator Tooling runtime \u00b6 9390 HTTP netty transport 9743 HTTPS netty transport Dashboard runtime \u00b6 9290 HTTP netty transport 9643 HTTPS netty transport Tip The following example shows how to overide the default netty port for the Streaming Integrator Tooling by updating the required parameters in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. wso2.transport.http: transportProperties: listenerConfigurations: - id: \"default\" port: 9390 - id: \"msf4j-https\" port: 9743 Clustering Ports \u00b6 Ports that are required for clustering deployment: Minimum High Availability (HA) Deployment: \u00b6 Server node: \u00b6 9090 HTTP netty transport. 9090 Specify the port of the node for the advertisedPort parameter in the liveSync section. The HTTP netty transport port is considered the default port. 9443 HTTPS netty transport. Multi Datacenter High Availability Deployment \u00b6 Other than the ports used in clustering setups (i.e., a Minimum HA Deployment or a scalable cluster), the following is required: 9092 Ports of the two separate instances of the broker deployed in each data center (e.g., bootstrap.servers= 'host1:9092, host2:9092'. The default is 9092` where the external kafka servers start.)","title":"Configuring Default Ports"},{"location":"ref/configuring-default-ports/#configuring-default-ports","text":"This page describes the default ports that are used for each runtime when the port offset is 0 .","title":"Configuring Default Ports"},{"location":"ref/configuring-default-ports/#common-ports","text":"The following ports are common to all runtimes. 7611 Thrift TCP port to receive events from clients. 7711 Thrift SSL port for secure transport where the client is authenticated. 9611 Binary TCP port to receive events from clients. 9711 Binary SSL port for secure transport where the client is authenticated. You can offset binary and thrift by configuring the offset parameter in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. The following is a sample configuration. # Carbon Configuration Parameters wso2.carbon: # value to uniquely identify a server id: wso2-si # server name name: WSO2 Streaming Integrator # server type type: wso2-si # ports used by this server ports: # port offset offset: 1","title":"Common Ports"},{"location":"ref/configuring-default-ports/#server-runtime","text":"9090 HTTP netty transport 9443 HTTPS netty transport","title":"Server runtime"},{"location":"ref/configuring-default-ports/#streaming-integrator-tooling-runtime","text":"9390 HTTP netty transport 9743 HTTPS netty transport","title":"Streaming Integrator Tooling runtime"},{"location":"ref/configuring-default-ports/#dashboard-runtime","text":"9290 HTTP netty transport 9643 HTTPS netty transport Tip The following example shows how to overide the default netty port for the Streaming Integrator Tooling by updating the required parameters in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. wso2.transport.http: transportProperties: listenerConfigurations: - id: \"default\" port: 9390 - id: \"msf4j-https\" port: 9743","title":"Dashboard runtime"},{"location":"ref/configuring-default-ports/#clustering-ports","text":"Ports that are required for clustering deployment:","title":"Clustering Ports"},{"location":"ref/configuring-default-ports/#minimum-high-availability-ha-deployment","text":"","title":"Minimum High Availability (HA) Deployment:"},{"location":"ref/configuring-default-ports/#server-node","text":"9090 HTTP netty transport. 9090 Specify the port of the node for the advertisedPort parameter in the liveSync section. The HTTP netty transport port is considered the default port. 9443 HTTPS netty transport.","title":"Server node:"},{"location":"ref/configuring-default-ports/#multi-datacenter-high-availability-deployment","text":"Other than the ports used in clustering setups (i.e., a Minimum HA Deployment or a scalable cluster), the following is required: 9092 Ports of the two separate instances of the broker deployed in each data center (e.g., bootstrap.servers= 'host1:9092, host2:9092'. The default is 9092` where the external kafka servers start.)","title":"Multi Datacenter High Availability Deployment"},{"location":"ref/hTTP-Status-Codes/","text":"HTTP Status Codes \u00b6 When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. HTTP status codes indicating successful delivery \u00b6 Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application. Error HTTP status codes \u00b6 Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"hTTP Status Codes"},{"location":"ref/hTTP-Status-Codes/#http-status-codes","text":"When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned.","title":"HTTP Status Codes"},{"location":"ref/hTTP-Status-Codes/#http-status-codes-indicating-successful-delivery","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application.","title":"HTTP status codes indicating successful delivery"},{"location":"ref/hTTP-Status-Codes/#error-http-status-codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"Error HTTP status codes"},{"location":"ref/healthcheck-APIs/","text":"Healthcheck APIs \u00b6 Overview \u00b6 Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime curl command syntax \u00b6 Sample curl command \u00b6 curl -k -X GET http://localhost:9090/health Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Healthcheck APIs"},{"location":"ref/healthcheck-APIs/#healthcheck-apis","text":"","title":"Healthcheck APIs"},{"location":"ref/healthcheck-APIs/#overview","text":"Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime","title":"Overview"},{"location":"ref/healthcheck-APIs/#curl-command-syntax","text":"","title":"curl command syntax"},{"location":"ref/healthcheck-APIs/#sample-curl-command","text":"curl -k -X GET http://localhost:9090/health","title":"Sample curl command"},{"location":"ref/healthcheck-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/healthcheck-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/performance-analysis-results/","text":"Performance Analysis Results \u00b6 Note These performance statistics were taken when the load average was below 3.8 in the 4 core instance. Consume events using Kafka source \u00b6 Specifications of EC2 Instances \u00b6 Stream Processor : c5.xLarge Kafka server : c5.xLarge Kafka publisher : c5.xLarge Siddhi Application \u00b6 @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic') @source(type='kafka', topic.list='kafka_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='172.31.0.135:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream KafkaSourceThroughputStream(count long); from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into KafkaSourceThroughputStream; Results \u00b6 Average Publishing TPS to Kafka : 1.1M Average Consuming TPS from Kafka: 180K Consuming messages from an HTTP Source \u00b6 Specifications of EC2 Instances \u00b6 Stream Processor : c5.xLarge JMeter: c5.xLarge Siddhi Application \u00b6 @App:name(\"HttpSource\") @App:description('Consume events from http clients') @source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream HttpSourceThroughputStream(count long); from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into HttpSourceThroughputStream; Results \u00b6 Average Publishing TPS to Http Source : 30K Average Consuming TPS from Http Source: 30K Sending HTTP requests and consuming the responses \u00b6 Specifications of EC2 Instances \u00b6 Stream Processor : c5.xLarge JMeter: c5.xLarge Web server : c5.xLarge Siddhi Application \u00b6 @App:name(\"HttpRequestResponse\") @App:description('Consume events from an HTTP source, ') @source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='http-request', l, sink.id='production-request', publisher.url='http://172.17.0.1:8688//netty_echo_server', @map(type='json')) define stream HttpRequestStream (batchNumber double, lowTotal double); @source(type='http-response' , sink.id='production-request', http.status.code='200', @map(type='json')) define stream HttpResponseStream(batchNumber double, lowTotal double); @sink(type='log') define stream FinalThroughputStream(count long); @sink(type='log') define stream InputThroughputStream(count long); from SweetProductionStream select 1D as batchNumber, 1200D as lowTotal insert into HttpRequestStream; from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into InputThroughputStream; from HttpResponseStream#window.timeBatch(5 sec) select count(*)/5 as count insert into FinalThroughputStream; Results \u00b6 Average Publishing TPS to HTTP Source : 29K Average Publishing TPS from HTTP request sink: 29K Average Consuming TPS from HTTP response source: 29K","title":"Performance Analysis Results"},{"location":"ref/performance-analysis-results/#performance-analysis-results","text":"Note These performance statistics were taken when the load average was below 3.8 in the 4 core instance.","title":"Performance Analysis Results"},{"location":"ref/performance-analysis-results/#consume-events-using-kafka-source","text":"","title":"Consume events using Kafka source"},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances","text":"Stream Processor : c5.xLarge Kafka server : c5.xLarge Kafka publisher : c5.xLarge","title":"Specifications of EC2 Instances"},{"location":"ref/performance-analysis-results/#siddhi-application","text":"@App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic') @source(type='kafka', topic.list='kafka_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='172.31.0.135:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream KafkaSourceThroughputStream(count long); from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into KafkaSourceThroughputStream;","title":"Siddhi Application"},{"location":"ref/performance-analysis-results/#results","text":"Average Publishing TPS to Kafka : 1.1M Average Consuming TPS from Kafka: 180K","title":"Results"},{"location":"ref/performance-analysis-results/#consuming-messages-from-an-http-source","text":"","title":"Consuming messages from an HTTP Source"},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_1","text":"Stream Processor : c5.xLarge JMeter: c5.xLarge","title":"Specifications of EC2 Instances"},{"location":"ref/performance-analysis-results/#siddhi-application_1","text":"@App:name(\"HttpSource\") @App:description('Consume events from http clients') @source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream HttpSourceThroughputStream(count long); from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into HttpSourceThroughputStream;","title":"Siddhi Application"},{"location":"ref/performance-analysis-results/#results_1","text":"Average Publishing TPS to Http Source : 30K Average Consuming TPS from Http Source: 30K","title":"Results"},{"location":"ref/performance-analysis-results/#sending-http-requests-and-consuming-the-responses","text":"","title":"Sending HTTP requests and consuming the responses"},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_2","text":"Stream Processor : c5.xLarge JMeter: c5.xLarge Web server : c5.xLarge","title":"Specifications of EC2 Instances"},{"location":"ref/performance-analysis-results/#siddhi-application_2","text":"@App:name(\"HttpRequestResponse\") @App:description('Consume events from an HTTP source, ') @source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='http-request', l, sink.id='production-request', publisher.url='http://172.17.0.1:8688//netty_echo_server', @map(type='json')) define stream HttpRequestStream (batchNumber double, lowTotal double); @source(type='http-response' , sink.id='production-request', http.status.code='200', @map(type='json')) define stream HttpResponseStream(batchNumber double, lowTotal double); @sink(type='log') define stream FinalThroughputStream(count long); @sink(type='log') define stream InputThroughputStream(count long); from SweetProductionStream select 1D as batchNumber, 1200D as lowTotal insert into HttpRequestStream; from SweetProductionStream#window.timeBatch(5 sec) select count(*)/5 as count insert into InputThroughputStream; from HttpResponseStream#window.timeBatch(5 sec) select count(*)/5 as count insert into FinalThroughputStream;","title":"Siddhi Application"},{"location":"ref/performance-analysis-results/#results_2","text":"Average Publishing TPS to HTTP Source : 29K Average Publishing TPS from HTTP request sink: 29K Average Consuming TPS from HTTP response source: 29K","title":"Results"},{"location":"ref/public-APIs/","text":"Public APIs \u00b6 The following topics list the APIs supported for WSO2 Stream processor from its Worker, Manager, Editor and Dashboard runtimes. Siddhi Application Management APIs Event Simulation APIs Status Monitoring APIs Dashboard APIs Authentication APIs Permission APIs Business Rules APIs Store APIs Healthcheck APIs","title":"Public APIs"},{"location":"ref/public-APIs/#public-apis","text":"The following topics list the APIs supported for WSO2 Stream processor from its Worker, Manager, Editor and Dashboard runtimes. Siddhi Application Management APIs Event Simulation APIs Status Monitoring APIs Dashboard APIs Authentication APIs Permission APIs Business Rules APIs Store APIs Healthcheck APIs","title":"Public APIs"},{"location":"ref/si-profiles/","text":"Streaming Integrator Profiles \u00b6","title":"Si profiles"},{"location":"ref/si-profiles/#streaming-integrator-profiles","text":"","title":"Streaming Integrator Profiles"},{"location":"ref/si-rest-api-guide/","text":"Streaming Integrator REST API Guide \u00b6 The following sections cover the different categories of REST API available for the Streaming Integrator the HTTP status codes thyat they can return. Siddhi application management APIs \u00b6 Creating a Siddhi application \u00b6 Overview \u00b6 Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output \u00b6 The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Updating a Siddhi Application \u00b6 Overview \u00b6 Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @<SIDDHI_APPLICATION_NAME>.siddhi -u admin:admin -k Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output \u00b6 If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting a Siddhi application \u00b6 Overview \u00b6 Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application to be deleted. curl command syntax \u00b6 curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" } Response \u00b6 HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes . Listing all active Siddhi applications \u00b6 Overview \u00b6 Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the `isActive` parameter is set to `false` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. [] Response \u00b6 HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes . Retrieving a specific Siddhi application \u00b6 Overview \u00b6 Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application to be retrieved. curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetching the status of a Siddhi Application \u00b6 Overview \u00b6 Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched. curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Taking a snapshot of a Siddhi Application \u00b6 Overview \u00b6 Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken. curl command syntax \u00b6 curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Restoring a Siddhi Application via a snapshot \u00b6 Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application . Overview \u00b6 Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application that needs to be restored. curl command syntax \u00b6 curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Returning real-time statistics of a worker \u00b6 Overview \u00b6 Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling worker statistics \u00b6 Overview \u00b6 Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning general details of a worker \u00b6 Overview \u00b6 Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning detailed statistics of all Siddhi applications \u00b6 Overview \u00b6 Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of a specific Siddhi application \u00b6 Overview \u00b6 Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled. curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of all Siddhi applications \u00b6 Overview \u00b6 Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Event Simulation APIs \u00b6 Status Monitoring APIs \u00b6 Dashboard APIs \u00b6 Authentication APIs \u00b6 Log in to a dashboard application \u00b6 Overview \u00b6 Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard Parameter description \u00b6 Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true curl command syntax \u00b6 curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username}&password={password}&grantType={grantTypr}&rememberMe={rememberMe}\" Sample curl command \u00b6 curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin&password=admin&grantType=password\" Sample output \u00b6 {\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600} Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Log out of the dashboard application \u00b6 Overview \u00b6 Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard curl command syntax \u00b6 curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\" Sample curl command \u00b6 curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\" Sample output \u00b6 N/A Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Redirect URL for login using authorization grant type \u00b6 Overview \u00b6 Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord Parameter description \u00b6 Parameter Description {appName} The application of which the URL needs to be redirected. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/login/callback/portal\" Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Permission APIs \u00b6 Business Rules APIs \u00b6 Lists available business rule instances \u00b6 Overview \u00b6 Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Delete business rule with given UUID \u00b6 Overview \u00b6 Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted. curl command syntax \u00b6 Sample curl command \u00b6 curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch template group with the given UUID \u00b6 Overview \u00b6 Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule templates of the template group with given UUID \u00b6 Overview \u00b6 Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule template of specific UUID available under a template group with specific UUID \u00b6 Overview \u00b6 Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch available template groups \u00b6 Overview \u00b6 Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch business rule instance with given UUID \u00b6 Overview \u00b6 Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched. curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Create and save a business rule \u00b6 Overview \u00b6 Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {deploymentStatus} curl command syntax \u00b6 Sample curl command \u00b6 curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Update business rules instance with given UUID \u00b6 Overview \u00b6 Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description \u00b6 Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus} curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Store APIs \u00b6 Query records in Siddhi store \u00b6 Overview \u00b6 Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker curl command syntax \u00b6 curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k Sample curl command \u00b6 curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Healthcheck APIs \u00b6 Overview \u00b6 Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime curl command syntax \u00b6 Sample curl command \u00b6 curl -k -X GET http://localhost:9090/health Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . HTTP Status Codes \u00b6 When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. HTTP status codes indicating successful delivery \u00b6 Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application. Error HTTP status codes \u00b6 Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"Streaming Integration REST API Guide"},{"location":"ref/si-rest-api-guide/#streaming-integrator-rest-api-guide","text":"The following sections cover the different categories of REST API available for the Streaming Integrator the HTTP status codes thyat they can return.","title":"Streaming Integrator REST API Guide"},{"location":"ref/si-rest-api-guide/#siddhi-application-management-apis","text":"","title":"Siddhi application management APIs"},{"location":"ref/si-rest-api-guide/#creating-a-siddhi-application","text":"","title":"Creating a Siddhi application"},{"location":"ref/si-rest-api-guide/#overview","text":"Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output","text":"The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response","text":"HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#updating-a-siddhi-application","text":"","title":"Updating a Siddhi Application"},{"location":"ref/si-rest-api-guide/#overview_1","text":"Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_1","text":"curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @<SIDDHI_APPLICATION_NAME>.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_1","text":"curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_1","text":"If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_1","text":"HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#deleting-a-siddhi-application","text":"","title":"Deleting a Siddhi application"},{"location":"ref/si-rest-api-guide/#overview_2","text":"Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description","text":"Parameter Description {appName} The name of the Siddhi application to be deleted.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_2","text":"curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_2","text":"curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_2","text":"The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_2","text":"HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#listing-all-active-siddhi-applications","text":"","title":"Listing all active Siddhi applications"},{"location":"ref/si-rest-api-guide/#overview_3","text":"Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_3","text":"curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_3","text":"Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the `isActive` parameter is set to `false` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. []","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_3","text":"HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#retrieving-a-specific-siddhi-application","text":"","title":"Retrieving a specific Siddhi application"},{"location":"ref/si-rest-api-guide/#overview_4","text":"Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_1","text":"Parameter Description {appName} The name of the Siddhi application to be retrieved.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_4","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_4","text":"The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_4","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetching-the-status-of-a-siddhi-application","text":"","title":"Fetching the status of a Siddhi Application"},{"location":"ref/si-rest-api-guide/#overview_5","text":"Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_2","text":"Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_5","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_5","text":"If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_5","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#taking-a-snapshot-of-a-siddhi-application","text":"","title":"Taking a snapshot of a Siddhi Application"},{"location":"ref/si-rest-api-guide/#overview_6","text":"Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_3","text":"Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_6","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_6","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_6","text":"The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_6","text":"HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#restoring-a-siddhi-application-via-a-snapshot","text":"Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application .","title":"Restoring a\u00a0Siddhi Application via a snapshot"},{"location":"ref/si-rest-api-guide/#overview_7","text":"Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_4","text":"Parameter Description {appName} The name of the Siddhi application that needs to be restored.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_7","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_7","text":"The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_7","text":"HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#returning-real-time-statistics-of-a-worker","text":"","title":"Returning real-time statistics of a worker"},{"location":"ref/si-rest-api-guide/#overview_8","text":"Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_5","text":"","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_8","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_8","text":"curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_8","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_8","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#enablingdisabling-worker-statistics","text":"","title":"Enabling/disabling worker statistics"},{"location":"ref/si-rest-api-guide/#overview_9","text":"Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_6","text":"","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_9","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_9","text":"curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_9","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_9","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#returning-general-details-of-a-worker","text":"","title":"Returning general details of a worker"},{"location":"ref/si-rest-api-guide/#overview_10","text":"Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_7","text":"","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_10","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_10","text":"curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_10","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_10","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#returning-detailed-statistics-of-all-siddhi-applications","text":"","title":"Returning detailed statistics of all Siddhi applications"},{"location":"ref/si-rest-api-guide/#overview_11","text":"Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_8","text":"","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_11","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_11","text":"curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_11","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_11","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#enablingdisabling-the-statistics-of-a-specific-siddhi-application","text":"","title":"Enabling/disabling the statistics of a specific Siddhi application"},{"location":"ref/si-rest-api-guide/#overview_12","text":"Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_9","text":"Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled.","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_12","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_12","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_12","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_12","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#enablingdisabling-the-statistics-of-all-siddhi-applications","text":"","title":"Enabling/disabling the statistics of all Siddhi applications"},{"location":"ref/si-rest-api-guide/#overview_13","text":"Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_10","text":"","title":"Parameter Description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_13","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_13","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_13","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_13","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#event-simulation-apis","text":"","title":"Event Simulation APIs"},{"location":"ref/si-rest-api-guide/#status-monitoring-apis","text":"","title":"Status Monitoring APIs"},{"location":"ref/si-rest-api-guide/#dashboard-apis","text":"","title":"Dashboard APIs"},{"location":"ref/si-rest-api-guide/#authentication-apis","text":"","title":"Authentication APIs"},{"location":"ref/si-rest-api-guide/#log-in-to-a-dashboard-application","text":"","title":"Log in to a dashboard application"},{"location":"ref/si-rest-api-guide/#overview_14","text":"Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_11","text":"Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_14","text":"curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username}&password={password}&grantType={grantTypr}&rememberMe={rememberMe}\"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_14","text":"curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin&password=admin&grantType=password\"","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_14","text":"{\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600}","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_14","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#log-out-of-the-dashboard-application","text":"","title":"Log out of the dashboard application"},{"location":"ref/si-rest-api-guide/#overview_15","text":"Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_15","text":"curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_15","text":"curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\"","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_15","text":"N/A","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_15","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#redirect-url-for-login-using-authorization-grant-type","text":"","title":"Redirect URL for login using authorization grant type"},{"location":"ref/si-rest-api-guide/#overview_16","text":"Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_12","text":"Parameter Description {appName} The application of which the URL needs to be redirected.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_16","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_16","text":"curl -X GET \"https://localhost:9643/login/callback/portal\"","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_16","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_16","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#permission-apis","text":"","title":"Permission APIs"},{"location":"ref/si-rest-api-guide/#business-rules-apis","text":"","title":"Business Rules APIs"},{"location":"ref/si-rest-api-guide/#lists-available-business-rule-instances","text":"","title":"Lists available business rule instances"},{"location":"ref/si-rest-api-guide/#overview_17","text":"Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_17","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_17","text":"curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_17","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_17","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#delete-business-rule-with-given-uuid","text":"","title":"Delete business rule with given UUID"},{"location":"ref/si-rest-api-guide/#overview_18","text":"Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_13","text":"Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_18","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_18","text":"curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_18","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_18","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetch-template-group-with-the-given-uuid","text":"","title":"Fetch template group with the given UUID"},{"location":"ref/si-rest-api-guide/#overview_19","text":"Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_14","text":"Parameter Description {templateGroupID} The UUID of the template group to be fetched.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_19","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_19","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_19","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_19","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetch-rule-templates-of-the-template-group-with-given-uuid","text":"","title":"Fetch rule templates of the template group with given UUID"},{"location":"ref/si-rest-api-guide/#overview_20","text":"Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_15","text":"Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_20","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_20","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_20","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_20","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetch-rule-template-of-specific-uuid-available-under-a-template-group-with-specific-uuid","text":"","title":"Fetch rule template of specific UUID available under a template group with specific UUID"},{"location":"ref/si-rest-api-guide/#overview_21","text":"Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_16","text":"Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_21","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_21","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_21","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_21","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetch-available-template-groups","text":"","title":"Fetch available template groups"},{"location":"ref/si-rest-api-guide/#overview_22","text":"Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_22","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_22","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_22","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_22","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#fetch-business-rule-instance-with-given-uuid","text":"","title":"Fetch business rule instance with given UUID"},{"location":"ref/si-rest-api-guide/#overview_23","text":"Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_17","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched.","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_23","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_23","text":"curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_23","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_23","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#create-and-save-a-business-rule","text":"","title":"Create and save a business rule"},{"location":"ref/si-rest-api-guide/#overview_24","text":"Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_18","text":"Parameter Description {deploymentStatus}","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_24","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_24","text":"curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_24","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_24","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#update-business-rules-instance-with-given-uuid","text":"","title":"Update business rules instance with given UUID"},{"location":"ref/si-rest-api-guide/#overview_25","text":"Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/si-rest-api-guide/#parameter-description_19","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus}","title":"Parameter description"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_25","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_25","text":"curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_25","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_25","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#store-apis","text":"","title":"Store APIs"},{"location":"ref/si-rest-api-guide/#query-records-in-siddhi-store","text":"","title":"Query records in Siddhi store"},{"location":"ref/si-rest-api-guide/#overview_26","text":"Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_26","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_26","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_26","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_26","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#healthcheck-apis","text":"","title":"Healthcheck APIs"},{"location":"ref/si-rest-api-guide/#overview_27","text":"Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime","title":"Overview"},{"location":"ref/si-rest-api-guide/#curl-command-syntax_27","text":"","title":"curl command syntax"},{"location":"ref/si-rest-api-guide/#sample-curl-command_27","text":"curl -k -X GET http://localhost:9090/health","title":"Sample curl command"},{"location":"ref/si-rest-api-guide/#sample-output_27","text":"","title":"Sample output"},{"location":"ref/si-rest-api-guide/#response_27","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/si-rest-api-guide/#http-status-codes","text":"When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned.","title":"HTTP Status Codes"},{"location":"ref/si-rest-api-guide/#http-status-codes-indicating-successful-delivery","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application.","title":"HTTP status codes indicating successful delivery"},{"location":"ref/si-rest-api-guide/#error-http-status-codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"Error HTTP status codes"},{"location":"ref/siddhi-Application-Management-APIs/","text":"Siddhi Application Management APIs \u00b6 Updating a Siddhi Application Deleting a Siddhi application Listing all active Siddhi applications Retrieving a specific Siddhi application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a worker Enabling/disabling worker statistics Returning general details of a worker Returning detailed statistics of all Siddhi applications Enabling/disabling the statistics of a specific Siddhi application Enabling/disabling the statistics of all Siddhi applications Creating a Siddhi application \u00b6 Overview \u00b6 Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output \u00b6 The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Updating a Siddhi Application \u00b6 Overview \u00b6 Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @<SIDDHI_APPLICATION_NAME>.siddhi -u admin:admin -k Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output \u00b6 If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting a Siddhi application \u00b6 Overview \u00b6 Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application to be deleted. curl command syntax \u00b6 curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" } Response \u00b6 HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes . Listing all active Siddhi applications \u00b6 Overview \u00b6 Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the ` isActive ` parameter is set to ` false ` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. [] Response \u00b6 HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes . Retrieving a specific Siddhi application \u00b6 Overview \u00b6 Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application to be retrieved. curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetching the status of a Siddhi Application \u00b6 Overview \u00b6 Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched. curl command syntax \u00b6 curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Taking a snapshot of a Siddhi Application \u00b6 Overview \u00b6 Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken. curl command syntax \u00b6 curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Restoring a Siddhi Application via a snapshot \u00b6 Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application . Overview \u00b6 Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description {appName} The name of the Siddhi application that needs to be restored. curl command syntax \u00b6 curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k Sample curl command \u00b6 curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } Response \u00b6 HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Returning real-time statistics of a worker \u00b6 Overview \u00b6 Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling worker statistics \u00b6 Overview \u00b6 Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning general details of a worker \u00b6 Overview \u00b6 Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning detailed statistics of all Siddhi applications \u00b6 Overview \u00b6 Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of a specific Siddhi application \u00b6 Overview \u00b6 Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled. curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of all Siddhi applications \u00b6 Overview \u00b6 Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description \u00b6 curl command syntax \u00b6 Sample curl command \u00b6 curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Siddhi Application Management APIs"},{"location":"ref/siddhi-Application-Management-APIs/#siddhi-application-management-apis","text":"Updating a Siddhi Application Deleting a Siddhi application Listing all active Siddhi applications Retrieving a specific Siddhi application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a worker Enabling/disabling worker statistics Returning general details of a worker Returning detailed statistics of all Siddhi applications Enabling/disabling the statistics of a specific Siddhi application Enabling/disabling the statistics of all Siddhi applications","title":"Siddhi Application Management APIs"},{"location":"ref/siddhi-Application-Management-APIs/#creating-a-siddhi-application","text":"","title":"Creating a Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview","text":"Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output","text":"The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response","text":"HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#updating-a-siddhi-application","text":"","title":"Updating a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_1","text":"Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_1","text":"curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @<SIDDHI_APPLICATION_NAME>.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_1","text":"curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_1","text":"If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_1","text":"HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#deleting-a-siddhi-application","text":"","title":"Deleting a Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_2","text":"Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description","text":"Parameter Description {appName} The name of the Siddhi application to be deleted.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_2","text":"curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_2","text":"curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_2","text":"The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_2","text":"HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#listing-all-active-siddhi-applications","text":"","title":"Listing all active Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_3","text":"Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_3","text":"curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_3","text":"Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the ` isActive ` parameter is set to ` false ` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. []","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_3","text":"HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#retrieving-a-specific-siddhi-application","text":"","title":"Retrieving a specific Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_4","text":"Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_1","text":"Parameter Description {appName} The name of the Siddhi application to be retrieved.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_4","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_4","text":"The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_4","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#fetching-the-status-of-a-siddhi-application","text":"","title":"Fetching the status of a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_5","text":"Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_2","text":"Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_5","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_5","text":"If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_5","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#taking-a-snapshot-of-a-siddhi-application","text":"","title":"Taking a snapshot of a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_6","text":"Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_3","text":"Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_6","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_6","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_6","text":"The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_6","text":"HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#restoring-a-siddhi-application-via-a-snapshot","text":"Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application .","title":"Restoring a\u00a0Siddhi Application via a snapshot"},{"location":"ref/siddhi-Application-Management-APIs/#overview_7","text":"Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_4","text":"Parameter Description {appName} The name of the Siddhi application that needs to be restored.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_7","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_7","text":"The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": <error-message> }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_7","text":"HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-real-time-statistics-of-a-worker","text":"","title":"Returning real-time statistics of a worker"},{"location":"ref/siddhi-Application-Management-APIs/#overview_8","text":"Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_5","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_8","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_8","text":"curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_8","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_8","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-worker-statistics","text":"","title":"Enabling/disabling worker statistics"},{"location":"ref/siddhi-Application-Management-APIs/#overview_9","text":"Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_6","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_9","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_9","text":"curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_9","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_9","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-general-details-of-a-worker","text":"","title":"Returning general details of a worker"},{"location":"ref/siddhi-Application-Management-APIs/#overview_10","text":"Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_7","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_10","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_10","text":"curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_10","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_10","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-detailed-statistics-of-all-siddhi-applications","text":"","title":"Returning detailed statistics of all Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_11","text":"Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_8","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_11","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_11","text":"curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_11","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_11","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-a-specific-siddhi-application","text":"","title":"Enabling/disabling the statistics of a specific Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_12","text":"Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_9","text":"Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_12","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_12","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_12","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_12","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-all-siddhi-applications","text":"","title":"Enabling/disabling the statistics of all Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_13","text":"Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_10","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_13","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_13","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_13","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_13","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/store-APIs/","text":"Store APIs \u00b6 Query records in Siddhi store Query records in Siddhi store \u00b6 Overview \u00b6 Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker curl command syntax \u00b6 curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k Sample curl command \u00b6 curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k Sample output \u00b6 Response \u00b6 HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Store APIs"},{"location":"ref/store-APIs/#store-apis","text":"Query records in Siddhi store","title":"Store APIs"},{"location":"ref/store-APIs/#query-records-in-siddhi-store","text":"","title":"Query records in Siddhi store"},{"location":"ref/store-APIs/#overview","text":"Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker","title":"Overview"},{"location":"ref/store-APIs/#curl-command-syntax","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k","title":"curl command syntax"},{"location":"ref/store-APIs/#sample-curl-command","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k","title":"Sample curl command"},{"location":"ref/store-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/store-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/stream-Processor-REST-API-Guide/","text":"Stream Processor REST API Guide \u00b6 The following topics cover information relating to the public APIs exposed from WSO2 Stream Processor. Public APIs HTTP Status Codes","title":"Stream Processor REST API Guide"},{"location":"ref/stream-Processor-REST-API-Guide/#stream-processor-rest-api-guide","text":"The following topics cover information relating to the public APIs exposed from WSO2 Stream Processor. Public APIs HTTP Status Codes","title":"Stream Processor REST API Guide"},{"location":"ref/wSO2-Stream-Processor-Profiles/","text":"WSO2 Stream Processor Profiles \u00b6 WSO2 Stream Processor has four profiles in order to run different functions. When you start and run any of these profiles, you are running only the subset of Stream Processor features that are specific to that profile. Some use cases supported require you to run two or more of these profiles in parallel. The four profiles are as follows. Editor profile Dashboard profile Worker profile Manager profile Editor profile \u00b6 Purpose This runs the developer environment where the following can be carried out: Creating Siddhi applications/Siddhi application templates. Testing and debugging Siddhi applications to determine whether they are ready to be used in a production environment. !!! info For more information about creating and testing Siddhi applications, see Understanding the Development Environment . For more information about creating business templates, see Creating a Business Rule Template . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: editor.bat For Linux: ./editor.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi file in the <SP_HOME>/wso2/editor/deployment/workspace directory. !!! info The Siddhi files created and saved via the Stream Processor Studio are stored in the <SP_HOME>/wso2/editor/deployment/workspace directory by default. Tools shipped When you start WSO2 SP in the editor profile, the URLs to access the following tools appear in the start-up logs. Stream Processor Studio Template Editor Dashboard profile \u00b6 Purpose This profile is available for the following purposes: Visualizing processed data via dashboards. For more information, see Visualizing Data . Deploying and managing business templates and business rules. For more information, see Creating Business Rules . Running the Status Dashboard to monitor the health of your WSO2 SP deployment. For more information, see Monitoring Stream Processor . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: dashboard.bat For Linux: ./dashboard.sh Deployment The following deployments are possible: Custom widgets can be deployed by placing the compiled react code in the <SP_HOME>/wso2/dashboard/deployment/web-ui-apps/portal/extensions/widgets directory. For more information, see Creating Custom Widgets . Dashboards that need to be imported can be deployed by placing the JSON file with the dashboard configuration in the <SP_HOME>/wso2/dashboard/resources/dashboards directory. For more information, see Importing and Exporting Dashboards . Business rules that need to deployed can be added in the <SP_HOME>/wso2/dashboard/resources/business-rules directory. For more information, see Managing Business Rules - Deploying Business Rules . Tools When you start WSO2 SP in the dashboard profile, the URLs to access the following tools appear in the start-up logs. Dashboard Portal Business Rules Manager Status Dashboard Worker profile \u00b6 Purpose This profile runs the Siddhi applications in a production environment when WSO2 SP is deployed in a single node or as a minimum HA cluster. For more information, see Deploying Streaming Applications . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: worker.bat For Linux: ./worker.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi in the <SP_HOME>/wso2/worker/deployment/siddhi-files directory. Manager profile \u00b6 Purpose This profile runs the distributed Siddhi applications in a production environment when WSO2 SP is set up as a fully distributed deployment. For more information, see Fully Distributed Deployment . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: manager.bat For Linux: ./manager.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi file in the <SP_HOME>/wso2/manager/deployment/siddhi-files directory.","title":"WSO2 Stream Processor Profiles"},{"location":"ref/wSO2-Stream-Processor-Profiles/#wso2-stream-processor-profiles","text":"WSO2 Stream Processor has four profiles in order to run different functions. When you start and run any of these profiles, you are running only the subset of Stream Processor features that are specific to that profile. Some use cases supported require you to run two or more of these profiles in parallel. The four profiles are as follows. Editor profile Dashboard profile Worker profile Manager profile","title":"WSO2 Stream Processor Profiles"},{"location":"ref/wSO2-Stream-Processor-Profiles/#editor-profile","text":"Purpose This runs the developer environment where the following can be carried out: Creating Siddhi applications/Siddhi application templates. Testing and debugging Siddhi applications to determine whether they are ready to be used in a production environment. !!! info For more information about creating and testing Siddhi applications, see Understanding the Development Environment . For more information about creating business templates, see Creating a Business Rule Template . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: editor.bat For Linux: ./editor.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi file in the <SP_HOME>/wso2/editor/deployment/workspace directory. !!! info The Siddhi files created and saved via the Stream Processor Studio are stored in the <SP_HOME>/wso2/editor/deployment/workspace directory by default. Tools shipped When you start WSO2 SP in the editor profile, the URLs to access the following tools appear in the start-up logs. Stream Processor Studio Template Editor","title":"Editor profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#dashboard-profile","text":"Purpose This profile is available for the following purposes: Visualizing processed data via dashboards. For more information, see Visualizing Data . Deploying and managing business templates and business rules. For more information, see Creating Business Rules . Running the Status Dashboard to monitor the health of your WSO2 SP deployment. For more information, see Monitoring Stream Processor . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: dashboard.bat For Linux: ./dashboard.sh Deployment The following deployments are possible: Custom widgets can be deployed by placing the compiled react code in the <SP_HOME>/wso2/dashboard/deployment/web-ui-apps/portal/extensions/widgets directory. For more information, see Creating Custom Widgets . Dashboards that need to be imported can be deployed by placing the JSON file with the dashboard configuration in the <SP_HOME>/wso2/dashboard/resources/dashboards directory. For more information, see Importing and Exporting Dashboards . Business rules that need to deployed can be added in the <SP_HOME>/wso2/dashboard/resources/business-rules directory. For more information, see Managing Business Rules - Deploying Business Rules . Tools When you start WSO2 SP in the dashboard profile, the URLs to access the following tools appear in the start-up logs. Dashboard Portal Business Rules Manager Status Dashboard","title":"Dashboard profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#worker-profile","text":"Purpose This profile runs the Siddhi applications in a production environment when WSO2 SP is deployed in a single node or as a minimum HA cluster. For more information, see Deploying Streaming Applications . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: worker.bat For Linux: ./worker.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi in the <SP_HOME>/wso2/worker/deployment/siddhi-files directory.","title":"Worker profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#manager-profile","text":"Purpose This profile runs the distributed Siddhi applications in a production environment when WSO2 SP is set up as a fully distributed deployment. For more information, see Fully Distributed Deployment . Starting and running the profile Navigate to the <SP_HOME>/bin directory and issue one of the following commands: For Windows: manager.bat For Linux: ./manager.sh Deployment To deploy a Siddhi application in this profile, place the relevant <SIDDHI_APPLICATION_NAME>.siddhi file in the <SP_HOME>/wso2/manager/deployment/siddhi-files directory.","title":"Manager profile"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/","text":"Worker Runtime - REST APIs Permission Model \u00b6 There are two sets of REST APIs available in worker runtime. Stream Processor APIs and Event Simulator APIs have following permission model. You need to have appropriate permission to invoke these APIS. Stream Processor APIs \u00b6 Method API Context Required Permission POST /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP DELETE /siddhi-apps/{appName} PermissionString - siddhiApp.manage AppName - SAPP GET /siddhi-apps PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName} PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName}/status PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP POST /siddhi-apps/{appName}/backup PermissionString - siddhiApp.manage AppName - SAPP POST /siddhi-apps/{appName}/restore /siddhi-apps/{appName}/restore?version= PermissionString - siddhiApp.manage AppName - SAPP GET /statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /statistics PermissionString - siddhiApp.manage AppName - SAPP GET /system-details PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /siddhi-apps/{appName}/statistics PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps/statistics PermissionString - siddhiApp.manage AppName - SAPP Event Simulator APIs \u00b6 Method API Context Required Permission POST /simulation/single PermissionString - simulator.manage AppName - SIM POST /simulation/feed PermissionString - simulator.manage AppName - SIM GET /simulation/feed PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName} PermissionString - simulator.manage or simulator.view AppName - SIM DELETE /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=run PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=pause PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=stop PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName}/status PermissionString - simulator.manage or simulator.view AppName - SIM POST /simulation/files PermissionString - simulator.manage AppName - SIM GET /simulation/files PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM DELETE /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase / retrieveTableNames PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase/{tableName}/ retrieveColumnNames PermissionString - simulator.manage AppName - SIM simulator","title":"Worker Runtime - REST APIs Permission Model"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#worker-runtime-rest-apis-permission-model","text":"There are two sets of REST APIs available in worker runtime. Stream Processor APIs and Event Simulator APIs have following permission model. You need to have appropriate permission to invoke these APIS.","title":"Worker Runtime - REST APIs Permission Model"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#stream-processor-apis","text":"Method API Context Required Permission POST /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP DELETE /siddhi-apps/{appName} PermissionString - siddhiApp.manage AppName - SAPP GET /siddhi-apps PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName} PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName}/status PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP POST /siddhi-apps/{appName}/backup PermissionString - siddhiApp.manage AppName - SAPP POST /siddhi-apps/{appName}/restore /siddhi-apps/{appName}/restore?version= PermissionString - siddhiApp.manage AppName - SAPP GET /statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /statistics PermissionString - siddhiApp.manage AppName - SAPP GET /system-details PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /siddhi-apps/{appName}/statistics PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps/statistics PermissionString - siddhiApp.manage AppName - SAPP","title":"Stream Processor\u00a0 APIs"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#event-simulator-apis","text":"Method API Context Required Permission POST /simulation/single PermissionString - simulator.manage AppName - SIM POST /simulation/feed PermissionString - simulator.manage AppName - SIM GET /simulation/feed PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName} PermissionString - simulator.manage or simulator.view AppName - SIM DELETE /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=run PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=pause PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=stop PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName}/status PermissionString - simulator.manage or simulator.view AppName - SIM POST /simulation/files PermissionString - simulator.manage AppName - SIM GET /simulation/files PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM DELETE /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase / retrieveTableNames PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase/{tableName}/ retrieveColumnNames PermissionString - simulator.manage AppName - SIM simulator","title":"Event Simulator APIs"},{"location":"ref/working-with-data-providers/","text":"Working with Data Providers \u00b6 Data providers are the sources from which information is fetched to be displayed in widgets. This section describes how to configure the data providers that are currently supported in WSO2 Stream Processor are as follows. These configurations determine the parameters that are available to be configured for each data provider when creating widgets via the Widget Generation wizard. For more information, see Generating Widgets . RDBMS Batch Data Provider RDBMS Streaming Data Provider Siddhi Store Data Provider Web Socket Provider RDBMS Batch Data Provider \u00b6 This data provider queries static tables. The following configuration is an example of an RDBMS Batch Data Provider: \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select type as Sentiment, count(TweetID) as Rate from sentiment where PARSEDATETIME(timestamp, 'yyyy-mm-dd hh:mm:ss','en') > CURRENT_TIMESTAMP()-86400 group by type\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"Sentiment\", \"publishingInterval\": 60 } RDBMS Streaming Data Provider \u00b6 This data provider queries dynamic tables. Here, the newer records are published as soon as the table is updated. The following configuration is an example of an RDBMS Streaming Data Provider: \"configs\": { \"type\": \"RDBMSStreamingDataProvider\", \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select id,TweetID from sentiment\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"id\", \"publishingInterval\": 5, \"publishingLimit\": 5, \"purgingInterval\": 6, \"purgingLimit\": 6, \"isPurgingEnable\": false } } Siddhi Store Data Provider \u00b6 This data provider runs a siddhi store query. The following configuration is an example of a Siddhi Store Data Provider: Info The Siddhi application included in this query must not contain any source configurations. \"configs\":{ \"type\":\"SiddhiStoreDataProvider\", \"config\":{ \"siddhiApp\":\"@App:name(\\\"HTTPAnalytics\\\") define stream ProcessedRequestsStream(timestamp long, serverName string, serviceName string, serviceMethod string, responseTime double, httpRespGroup string, userAgent string, requestIP string); define aggregation RequestAggregation from ProcessedRequestsStream select serverName, serviceName, serviceMethod, httpRespGroup, count() as numRequests, avg(responseTime) as avgRespTime group by serverName, serviceName, serviceMethod, httpRespGroup aggregate by timestamp every sec...year;\", \"queryData\":{ \"query\":\"from RequestAggregation within \\\"2018-**-** **:**:**\\\" per \\\"days\\\" select AGG_TIMESTAMP, serverName, avg(avgRespTime) as avgRespTime\" }, \"publishingInterval\":60, \"timeColumns\": \"AGG_TIMESTAMP\" } } Web Socket Provider \u00b6 This data provider utilizes web siddhi-io-web socket sink to provide data to the clients. It creates endpoints as follows for the web socket sinks to connect and publish information. wss://host:port/websocket-provider/{topic} Info The host and port will be the host and port of the Portal Web application The following configuration is an example of a web socket data provider. { configs: { type: 'WebSocketProvider', config: { subscriberTopic: 'sampleStream', mapType: 'json' } } }","title":"Working with Data Providers"},{"location":"ref/working-with-data-providers/#working-with-data-providers","text":"Data providers are the sources from which information is fetched to be displayed in widgets. This section describes how to configure the data providers that are currently supported in WSO2 Stream Processor are as follows. These configurations determine the parameters that are available to be configured for each data provider when creating widgets via the Widget Generation wizard. For more information, see Generating Widgets . RDBMS Batch Data Provider RDBMS Streaming Data Provider Siddhi Store Data Provider Web Socket Provider","title":"Working with Data Providers"},{"location":"ref/working-with-data-providers/#rdbms-batch-data-provider","text":"This data provider queries static tables. The following configuration is an example of an RDBMS Batch Data Provider: \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select type as Sentiment, count(TweetID) as Rate from sentiment where PARSEDATETIME(timestamp, 'yyyy-mm-dd hh:mm:ss','en') > CURRENT_TIMESTAMP()-86400 group by type\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"Sentiment\", \"publishingInterval\": 60 }","title":"RDBMS Batch Data Provider"},{"location":"ref/working-with-data-providers/#rdbms-streaming-data-provider","text":"This data provider queries dynamic tables. Here, the newer records are published as soon as the table is updated. The following configuration is an example of an RDBMS Streaming Data Provider: \"configs\": { \"type\": \"RDBMSStreamingDataProvider\", \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select id,TweetID from sentiment\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"id\", \"publishingInterval\": 5, \"publishingLimit\": 5, \"purgingInterval\": 6, \"purgingLimit\": 6, \"isPurgingEnable\": false } }","title":"RDBMS Streaming Data Provider"},{"location":"ref/working-with-data-providers/#siddhi-store-data-provider","text":"This data provider runs a siddhi store query. The following configuration is an example of a Siddhi Store Data Provider: Info The Siddhi application included in this query must not contain any source configurations. \"configs\":{ \"type\":\"SiddhiStoreDataProvider\", \"config\":{ \"siddhiApp\":\"@App:name(\\\"HTTPAnalytics\\\") define stream ProcessedRequestsStream(timestamp long, serverName string, serviceName string, serviceMethod string, responseTime double, httpRespGroup string, userAgent string, requestIP string); define aggregation RequestAggregation from ProcessedRequestsStream select serverName, serviceName, serviceMethod, httpRespGroup, count() as numRequests, avg(responseTime) as avgRespTime group by serverName, serviceName, serviceMethod, httpRespGroup aggregate by timestamp every sec...year;\", \"queryData\":{ \"query\":\"from RequestAggregation within \\\"2018-**-** **:**:**\\\" per \\\"days\\\" select AGG_TIMESTAMP, serverName, avg(avgRespTime) as avgRespTime\" }, \"publishingInterval\":60, \"timeColumns\": \"AGG_TIMESTAMP\" } }","title":"Siddhi Store Data Provider"},{"location":"ref/working-with-data-providers/#web-socket-provider","text":"This data provider utilizes web siddhi-io-web socket sink to provide data to the clients. It creates endpoints as follows for the web socket sinks to connect and publish information. wss://host:port/websocket-provider/{topic} Info The host and port will be the host and port of the Portal Web application The following configuration is an example of a web socket data provider. { configs: { type: 'WebSocketProvider', config: { subscriberTopic: 'sampleStream', mapType: 'json' } } }","title":"Web Socket Provider"},{"location":"samples/AggregateDataIncrementally/","text":"Aggregating Data Incrementally \u00b6 Purpose \u00b6 This example demonstrates how to get running statistics using Siddhi. The sample Siddhi application aggregates the data relating to the raw material purchases of a sweet production factory. Before you begin: Install MySQL. Add the MySQL JDBC driver to your Streaming Integrator library as follows: Download the JDBC driver from the MySQL site . Extract the MySQL JDBC Driver zip file you downloaded. Then use the jarbundle tool in the <SI_TOOLING_HOME>/bin directory to convert the jars in it into OSGi bundles. To do this, issue one of the following commands: For Windows : <SI_TOOLING_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR Copy the converted bundles to the <SI_TOOLING_HOME>/lib directory. Create a data store named sweetFactoryDB in MySQL with relevant access privileges. Replace the values for the jdbc.url , username , and password parameters in the sample. e.g., jdbc.url - jdbc:mysql://localhost:3306/sweetFactoryDB username - root password - root In the Streaming Integrator Tooling, save the sample Siddhi application. Executing the Sample \u00b6 To execute the sample Siddhi application, open it in Streaming Integrator Tooling and click the Start button (shown below) or click Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AggregateDataIncrementally.siddhi - Started Successfully!. Testing the Sample \u00b6 To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the RawMaterialStream stream of the AggregateDataIncrementally Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name AggregateDataIncrementally StreamName RawMaterialStream As a result, the attributes of the RawMaterialStream stream appear as marked in the image above. Send four events by entering values as shown below. Click Send after each event. Event 1 name : chocolate cake amount : 100 Event 2 name : chocolate cake amount : 200 Event 3 name : chocolate ice cream amount : `50 Event 4 name : chocolate ice cream amount : 150 In the StreamName field, select TriggerStream* . In the triggerId field, enter 1 as the trigger ID, and then click Send . Viewing the Results: \u00b6 The input and the corresponding output is displayed in the console as follows. Info The timestamp displayed is different because it is derived based on the time at which you send the events. INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}] [INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}] Click here to view the sample Siddhi application. gi @App:name(\"AggregateDataIncrementally\") @App:description('Aggregates values every second until year and gets statistics') define stream RawMaterialStream (name string, amount double); @sink(type ='log') define stream RawMaterialStatStream (AGG_TIMESTAMP long, name string, avgAmount double); @store( type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation stockAggregation from RawMaterialStream select name, avg(amount) as avgAmount, sum(amount) as totalAmount group by name aggregate every sec...year; define stream TriggerStream (triggerId string); @info(name = 'query1') from TriggerStream as f join stockAggregation as s within \"2016-06-06 12:00:00 +05:30\", \"2020-06-06 12:00:00 +05:30\" per 'hours' select AGG_TIMESTAMP, s.name, avgAmount insert into RawMaterialStatStream;","title":"Aggregating Data Incrementally"},{"location":"samples/AggregateDataIncrementally/#aggregating-data-incrementally","text":"","title":"Aggregating Data Incrementally"},{"location":"samples/AggregateDataIncrementally/#purpose","text":"This example demonstrates how to get running statistics using Siddhi. The sample Siddhi application aggregates the data relating to the raw material purchases of a sweet production factory. Before you begin: Install MySQL. Add the MySQL JDBC driver to your Streaming Integrator library as follows: Download the JDBC driver from the MySQL site . Extract the MySQL JDBC Driver zip file you downloaded. Then use the jarbundle tool in the <SI_TOOLING_HOME>/bin directory to convert the jars in it into OSGi bundles. To do this, issue one of the following commands: For Windows : <SI_TOOLING_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux : <SI_TOOLING_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR Copy the converted bundles to the <SI_TOOLING_HOME>/lib directory. Create a data store named sweetFactoryDB in MySQL with relevant access privileges. Replace the values for the jdbc.url , username , and password parameters in the sample. e.g., jdbc.url - jdbc:mysql://localhost:3306/sweetFactoryDB username - root password - root In the Streaming Integrator Tooling, save the sample Siddhi application.","title":"Purpose"},{"location":"samples/AggregateDataIncrementally/#executing-the-sample","text":"To execute the sample Siddhi application, open it in Streaming Integrator Tooling and click the Start button (shown below) or click Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AggregateDataIncrementally.siddhi - Started Successfully!.","title":"Executing the Sample"},{"location":"samples/AggregateDataIncrementally/#testing-the-sample","text":"To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the RawMaterialStream stream of the AggregateDataIncrementally Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name AggregateDataIncrementally StreamName RawMaterialStream As a result, the attributes of the RawMaterialStream stream appear as marked in the image above. Send four events by entering values as shown below. Click Send after each event. Event 1 name : chocolate cake amount : 100 Event 2 name : chocolate cake amount : 200 Event 3 name : chocolate ice cream amount : `50 Event 4 name : chocolate ice cream amount : 150 In the StreamName field, select TriggerStream* . In the triggerId field, enter 1 as the trigger ID, and then click Send .","title":"Testing the Sample"},{"location":"samples/AggregateDataIncrementally/#viewing-the-results","text":"The input and the corresponding output is displayed in the console as follows. Info The timestamp displayed is different because it is derived based on the time at which you send the events. INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}] [INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}] Click here to view the sample Siddhi application. gi @App:name(\"AggregateDataIncrementally\") @App:description('Aggregates values every second until year and gets statistics') define stream RawMaterialStream (name string, amount double); @sink(type ='log') define stream RawMaterialStatStream (AGG_TIMESTAMP long, name string, avgAmount double); @store( type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\", username=\"root\", password=\"root\", jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation stockAggregation from RawMaterialStream select name, avg(amount) as avgAmount, sum(amount) as totalAmount group by name aggregate every sec...year; define stream TriggerStream (triggerId string); @info(name = 'query1') from TriggerStream as f join stockAggregation as s within \"2016-06-06 12:00:00 +05:30\", \"2020-06-06 12:00:00 +05:30\" per 'hours' select AGG_TIMESTAMP, s.name, avgAmount insert into RawMaterialStatStream;","title":"Viewing the Results:"},{"location":"samples/AggregateOverTime/","text":"Aggregating Data Over Time \u00b6 Purpose: \u00b6 This application demonstrates how to simulate random events via Feed Simulation and calculate running aggregates such as avg , min , max , etc. The aggregation is executed on events within a time window. A sliding time window of 10 seconds is used in this sample. For more information on windows see Siddhi Query Guide - Window . The group by clause helps to perform aggregation on events grouped by a certain attribute. In this sample, the trading information per trader is aggregated and summarized, for a window of 10 seconds. Before you begin: In the Streaming Integrator Tooling, save the sample Siddhi application. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AggregateOverTime.siddhi - Started Successfully!. Testing the Sample \u00b6 To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation -> Create . In the new panel that opens, enter information as follows: In the Simulation Name field, enter AggregateOverTime as the name for the simulation. Select Random as the simulation source and then click Add Simulation Source . In the Siddhi App Name field, select AggregateOverTime . In the Stream Name field, select Trade Stream . In the trader(STRING) field, select Regex based . Then in the Pattern field that appears, enter (Bob|Jane|Tom) as the pattern. Tip When you use the (Bob|Jane|Tom) pattern, only Bob , Jane , and Tom are selected as values for the trader attribute of the TradeStream . Using a few values for the trader attribute is helpful when you verify the output because the output is grouped by the trader. In the quantity(INT) field, select Primitive based . Save the simulator configuration by clicking Save . The newly created simulation is now listed under the Active Feed Simulations list as shown below. Click the start button next to the AggregateOverTime simulation to start generating random events. In the Run or Debug dialog box that appears, select Run and click Start Similation . Viewing the Results \u00b6 Once you start the simulator, the output is logged in the console as shown in the sample below. The output reflects the aggregation for the events sent during the last 10 seconds. Click here to view the sample Siddhi application. @App:name(\"AggregateOverTime\") @App:description('Simulate multiple random events and calculate aggregations over time with group by') define stream TradesStream(trader string, quantity int); @sink(type='log') define stream SummarizedTradingInformation(trader string, noOfTrades long, totalTradingQuantity long, minTradingQuantity int, maxTradingQuantity int, avgTradingQuantity double); --Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds @info(name='query1') from TradesStream#window.time(10 sec) select trader, count() as noOfTrades, sum(quantity) as totalTradingQuantity, min(quantity) as minTradingQuantity, max(quantity) as maxTradingQuantity, avg(quantity) as avgTradingQuantity group by trader insert into SummarizedTradingInformation;","title":"Calculating Aggregations Over Time"},{"location":"samples/AggregateOverTime/#aggregating-data-over-time","text":"","title":"Aggregating Data Over Time"},{"location":"samples/AggregateOverTime/#purpose","text":"This application demonstrates how to simulate random events via Feed Simulation and calculate running aggregates such as avg , min , max , etc. The aggregation is executed on events within a time window. A sliding time window of 10 seconds is used in this sample. For more information on windows see Siddhi Query Guide - Window . The group by clause helps to perform aggregation on events grouped by a certain attribute. In this sample, the trading information per trader is aggregated and summarized, for a window of 10 seconds. Before you begin: In the Streaming Integrator Tooling, save the sample Siddhi application.","title":"Purpose:"},{"location":"samples/AggregateOverTime/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AggregateOverTime.siddhi - Started Successfully!.","title":"Executing the Sample"},{"location":"samples/AggregateOverTime/#testing-the-sample","text":"To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation -> Create . In the new panel that opens, enter information as follows: In the Simulation Name field, enter AggregateOverTime as the name for the simulation. Select Random as the simulation source and then click Add Simulation Source . In the Siddhi App Name field, select AggregateOverTime . In the Stream Name field, select Trade Stream . In the trader(STRING) field, select Regex based . Then in the Pattern field that appears, enter (Bob|Jane|Tom) as the pattern. Tip When you use the (Bob|Jane|Tom) pattern, only Bob , Jane , and Tom are selected as values for the trader attribute of the TradeStream . Using a few values for the trader attribute is helpful when you verify the output because the output is grouped by the trader. In the quantity(INT) field, select Primitive based . Save the simulator configuration by clicking Save . The newly created simulation is now listed under the Active Feed Simulations list as shown below. Click the start button next to the AggregateOverTime simulation to start generating random events. In the Run or Debug dialog box that appears, select Run and click Start Similation .","title":"Testing the Sample"},{"location":"samples/AggregateOverTime/#viewing-the-results","text":"Once you start the simulator, the output is logged in the console as shown in the sample below. The output reflects the aggregation for the events sent during the last 10 seconds. Click here to view the sample Siddhi application. @App:name(\"AggregateOverTime\") @App:description('Simulate multiple random events and calculate aggregations over time with group by') define stream TradesStream(trader string, quantity int); @sink(type='log') define stream SummarizedTradingInformation(trader string, noOfTrades long, totalTradingQuantity long, minTradingQuantity int, maxTradingQuantity int, avgTradingQuantity double); --Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds @info(name='query1') from TradesStream#window.time(10 sec) select trader, count() as noOfTrades, sum(quantity) as totalTradingQuantity, min(quantity) as minTradingQuantity, max(quantity) as maxTradingQuantity, avg(quantity) as avgTradingQuantity group by trader insert into SummarizedTradingInformation;","title":"Viewing the Results"},{"location":"samples/AlertsAndThresholds/","text":"Receiving Email Alerts \u00b6 Purpose: \u00b6 This application demonstrates how to send a single event via Single Simulation, and to generate alerts using filters when the threshold value is exceeded. Furthermore, it shows how to configure WSO2 Streaming Integrator Tooling to publish an alerts via e-mail. An alert is generated as an email when a high value transaction (i.e., where the value is over 5000) takes place. Before you begin: Enable access to less secure apps in the gmail account you are using for this example via the Less SecureApps link. In the sample application, change the values for the following parameters in the @sink annotation as follows. username -> business.rules.manager (This is the sender's username.) password -> business-rules (This is the sender's password.) address -> business.rules.manager@wso2.com (This is the sender's address.) to -> template-manager@wso2.com (This is the receiver's address.) subject -> Alert for large value transaction: cardNo:{{creditCardNo}} (This is the subject of the email.) Once you update the values for the above parameters, save the sample Siddhib application. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AlertsAndThresholds.siddhi - Started Successfully!. Testing the Sample \u00b6 To test the sample Siddhi application, simulate a single event for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the TransactionStream stream of the AlertsAndThresholds Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name AlertsAndThresholds StreamName TransactionStream As a result, the attributes of the Transactiontream stream appear as marked in the image above. Enter values for the attributes as follows: Attribute Value creditCardNo 1234567898765432 country SL item mobile quantity 100 price 5000 Info To generate an email alert, you need to simulate an event where the transaction value (i.e., quantity * price ) exceeds 5000 . This is indicated by the [quantity * price > 5000] filter connected to the TransactionStream input stream. Click Send . Viewing the Results \u00b6 To view the results, check the receiver gmail inbox (i.e., gmail specified via the to parameter in the sink configuration). The following is displayed. Subject: Alert for large value transaction: cardNo:1234567898765432 Content: creditCardNo:\"1234567898765432\", country:\"SL\", item:\"mobile\", quantity:100, price:5000 Click here to view the sample Siddhi application. @App:name(\"AlertsAndThresholds\") @App:description('Simulate a single event and receive alerts as e-mail when a predefined threshold value is exceeded') define stream TransactionStream(creditCardNo string, country string, item string, quantity int, price double); @sink(type='email', username ='business.rules.manager', address ='business.rules.manager@wso2.com', password= 'business-rules', subject='Alert for large value transaction: cardNo:{{creditCardNo}}', to='receive.alert.account1@gmail.com, receive.alert.account2@gmail.com', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true', @map(type='text')) define stream AlertStream(creditCardNo string, country string, item string, quantity int, price double); --Filter events when quantity * price > 5000 condition is satisfied @info(name='query1') from TransactionStream[quantity * price > 5000] select * insert into AlertStream;","title":"Receiving Email Alerts"},{"location":"samples/AlertsAndThresholds/#receiving-email-alerts","text":"","title":"Receiving Email Alerts"},{"location":"samples/AlertsAndThresholds/#purpose","text":"This application demonstrates how to send a single event via Single Simulation, and to generate alerts using filters when the threshold value is exceeded. Furthermore, it shows how to configure WSO2 Streaming Integrator Tooling to publish an alerts via e-mail. An alert is generated as an email when a high value transaction (i.e., where the value is over 5000) takes place. Before you begin: Enable access to less secure apps in the gmail account you are using for this example via the Less SecureApps link. In the sample application, change the values for the following parameters in the @sink annotation as follows. username -> business.rules.manager (This is the sender's username.) password -> business-rules (This is the sender's password.) address -> business.rules.manager@wso2.com (This is the sender's address.) to -> template-manager@wso2.com (This is the receiver's address.) subject -> Alert for large value transaction: cardNo:{{creditCardNo}} (This is the subject of the email.) Once you update the values for the above parameters, save the sample Siddhib application.","title":"Purpose:"},{"location":"samples/AlertsAndThresholds/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AlertsAndThresholds.siddhi - Started Successfully!.","title":"Executing the Sample"},{"location":"samples/AlertsAndThresholds/#testing-the-sample","text":"To test the sample Siddhi application, simulate a single event for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the TransactionStream stream of the AlertsAndThresholds Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name AlertsAndThresholds StreamName TransactionStream As a result, the attributes of the Transactiontream stream appear as marked in the image above. Enter values for the attributes as follows: Attribute Value creditCardNo 1234567898765432 country SL item mobile quantity 100 price 5000 Info To generate an email alert, you need to simulate an event where the transaction value (i.e., quantity * price ) exceeds 5000 . This is indicated by the [quantity * price > 5000] filter connected to the TransactionStream input stream. Click Send .","title":"Testing the Sample"},{"location":"samples/AlertsAndThresholds/#viewing-the-results","text":"To view the results, check the receiver gmail inbox (i.e., gmail specified via the to parameter in the sink configuration). The following is displayed. Subject: Alert for large value transaction: cardNo:1234567898765432 Content: creditCardNo:\"1234567898765432\", country:\"SL\", item:\"mobile\", quantity:100, price:5000 Click here to view the sample Siddhi application. @App:name(\"AlertsAndThresholds\") @App:description('Simulate a single event and receive alerts as e-mail when a predefined threshold value is exceeded') define stream TransactionStream(creditCardNo string, country string, item string, quantity int, price double); @sink(type='email', username ='business.rules.manager', address ='business.rules.manager@wso2.com', password= 'business-rules', subject='Alert for large value transaction: cardNo:{{creditCardNo}}', to='receive.alert.account1@gmail.com, receive.alert.account2@gmail.com', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true', @map(type='text')) define stream AlertStream(creditCardNo string, country string, item string, quantity int, price double); --Filter events when quantity * price > 5000 condition is satisfied @info(name='query1') from TransactionStream[quantity * price > 5000] select * insert into AlertStream;","title":"Viewing the Results"},{"location":"samples/AmazonS3SinkSample/","text":"Publishing Aggregated Events to the Amazon AWS S3 Bucket \u00b6 Purpose: \u00b6 This application demonstrates how to publish aggregated events to Amazon AWS S3 bucket via the siddhi-io-s3 sink extension . In this sample the events received to the StockQuoteStream stream are aggregated by the StockQuoteWindow window, and then published to the Amazon S3 bucket specified via the bucket.name parameter. Before you begin: Create an AWS account and set the credentials following the instructions in the AWS Developer Guide . Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample: \u00b6 To execute the sample, follow the procedure below: In Streaming Integrator Tooling, click Open and then click AmazonS3SinkSample.siddhi in the workspace directory. Then update it as follows: Enter the credential.provider class name as the value for the credential.provider parameter. If the class is not specified, the default credential provider chain is used. For the bucket.name parameter, enter AWSBUCKET as the value. Save the Siddhi application. Start the Siddhi application by clicking the Start button (shown below) or by clicking by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AmazonS3SinkSample.siddhi - Started Successfully!. Testing the Sample \u00b6 To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation -> Create . ![Feed Simulation tab](../../images/aggregate-over-time-sample/feed-simulation-tab.png) In the new panel that opens, enter information as follows. In the Simulation Name field, enter AmazonS3SinkSample as the name for the simulation. Select Random as the simulation source and then click Add Simulation Source . In the Siddhi App Name field, select AmazonS3SinkSample . In the Stream Name field, select StockQuoteStream . In the symbol(STRING) field, select Regex based . Then in the Pattern field that appears, enter (IBM|WSO2|Dell) as the pattern. Tip When you use the (IBM|WSO2|Dell) pattern, only IBM , WSO2 , and Dell are selected as values for the symbol attribute of the StockQuoteStream . Using a few values for the symbol attribute is helpful when you verify the output because the output is grouped by the symbol. In the price(DOUBLE) field, select Static value . In the quantity(INT) field, select Primitive based . Save the simulator configuration by clicking Save . Th.e simulation is added to the list of saved feed simulations as shown below. To simulate random events, click the Start button next to the AmazonS3SinkSample simulator. Viewing results \u00b6 Once the events are sent, check the S3 bucket. Objects are created with 3 events in each. Click here to view the sample Siddhi application. @App:name(\"AmazonS3SinkSample\") @App:description(\"Publish events to Amazon AWS S3\") define window StockQuoteWindow(symbol string, price double, quantity int) lengthBatch(3) output all events; define stream StockQuoteStream(symbol string, price double, quantity int); @sink(type='s3', bucket.name='<BUCKET_NAME>', object.path='stocks', credential.provider='com.amazonaws.auth.profile.ProfileCredentialsProvider', node.id='zeus', @map(type='json', enclosing.element='$.stocks', @payload(\"\"\"{\"symbol\": \"{{symbol}}\", \"price\": \"{{price}}\", \"quantity\": \"{{quantity}}\"}\"\"\"))) define stream StorageOutputStream (symbol string, price double, quantity int); from StockQuoteStream insert into StockQuoteWindow; from StockQuoteWindow select * insert into StorageOutputStream;","title":"Publishing Aggregated Events to the Amazon AWS S3 Bucket"},{"location":"samples/AmazonS3SinkSample/#publishing-aggregated-events-to-the-amazon-aws-s3-bucket","text":"","title":"Publishing Aggregated Events to the Amazon AWS S3 Bucket"},{"location":"samples/AmazonS3SinkSample/#purpose","text":"This application demonstrates how to publish aggregated events to Amazon AWS S3 bucket via the siddhi-io-s3 sink extension . In this sample the events received to the StockQuoteStream stream are aggregated by the StockQuoteWindow window, and then published to the Amazon S3 bucket specified via the bucket.name parameter. Before you begin: Create an AWS account and set the credentials following the instructions in the AWS Developer Guide . Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose:"},{"location":"samples/AmazonS3SinkSample/#executing-the-sample","text":"To execute the sample, follow the procedure below: In Streaming Integrator Tooling, click Open and then click AmazonS3SinkSample.siddhi in the workspace directory. Then update it as follows: Enter the credential.provider class name as the value for the credential.provider parameter. If the class is not specified, the default credential provider chain is used. For the bucket.name parameter, enter AWSBUCKET as the value. Save the Siddhi application. Start the Siddhi application by clicking the Start button (shown below) or by clicking by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. AmazonS3SinkSample.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/AmazonS3SinkSample/#testing-the-sample","text":"To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation -> Create . ![Feed Simulation tab](../../images/aggregate-over-time-sample/feed-simulation-tab.png) In the new panel that opens, enter information as follows. In the Simulation Name field, enter AmazonS3SinkSample as the name for the simulation. Select Random as the simulation source and then click Add Simulation Source . In the Siddhi App Name field, select AmazonS3SinkSample . In the Stream Name field, select StockQuoteStream . In the symbol(STRING) field, select Regex based . Then in the Pattern field that appears, enter (IBM|WSO2|Dell) as the pattern. Tip When you use the (IBM|WSO2|Dell) pattern, only IBM , WSO2 , and Dell are selected as values for the symbol attribute of the StockQuoteStream . Using a few values for the symbol attribute is helpful when you verify the output because the output is grouped by the symbol. In the price(DOUBLE) field, select Static value . In the quantity(INT) field, select Primitive based . Save the simulator configuration by clicking Save . Th.e simulation is added to the list of saved feed simulations as shown below. To simulate random events, click the Start button next to the AmazonS3SinkSample simulator.","title":"Testing the Sample"},{"location":"samples/AmazonS3SinkSample/#viewing-results","text":"Once the events are sent, check the S3 bucket. Objects are created with 3 events in each. Click here to view the sample Siddhi application. @App:name(\"AmazonS3SinkSample\") @App:description(\"Publish events to Amazon AWS S3\") define window StockQuoteWindow(symbol string, price double, quantity int) lengthBatch(3) output all events; define stream StockQuoteStream(symbol string, price double, quantity int); @sink(type='s3', bucket.name='<BUCKET_NAME>', object.path='stocks', credential.provider='com.amazonaws.auth.profile.ProfileCredentialsProvider', node.id='zeus', @map(type='json', enclosing.element='$.stocks', @payload(\"\"\"{\"symbol\": \"{{symbol}}\", \"price\": \"{{price}}\", \"quantity\": \"{{quantity}}\"}\"\"\"))) define stream StorageOutputStream (symbol string, price double, quantity int); from StockQuoteStream insert into StockQuoteWindow; from StockQuoteWindow select * insert into StorageOutputStream;","title":"Viewing results"},{"location":"samples/CDCWithListeningMode/","text":"Capturing MySQL Inserts via CDC \u00b6 Purpose: \u00b6 This sample demonstrates how to capture change data from MySQL using Siddhi. The change events that can be captured include INSERT , UPDATE , and DELETE . Before you begin: Ensure that MySQL is installed on your computer. Add the MySQL JDBC driver to the <SI_TOOLING_HOME>/lib directory as follows: Download the JDBC driver from the MySQL website . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar JAR and place it in the <SI_TOOLING_HOME>/lib directory. Configure MySQL to enable binary logging . Enable state persistence in siddhi applications. To do this, open the <SI_TOOLING_HOME>/conf/server/deployment.yaml file and set the state.persistence enabled=true property. Create a database named production by issuing the following command. create database production; Create a user named wso2sp with wso2 as the password, and with SELECT , RELOAD , SHOW DATABASES , REPLICATION SLAVE , REPLICATION CLIENT privileges. To do this, issue the following command. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2sp' IDENTIFIED BY 'wso2'; Change the database by issuing the following command. use production; Create a table named SweetProductionTable . CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CDCWithListeningMode.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again. Testing the Sample \u00b6 To test the sample Siddhi application, insert a record to the SweetProductionTable table you created by issuing the following command: insert into SweetProductionTable values('chocolate',100.0); Viewing the results \u00b6 This insert is logged in the Streaming Integrator console as follows. Info Optionally, you can use this sample to also capture update and delete operations. - delete operation events include before_name and before amount keys. - update operation events include the before_name , name , before_amount , amount keys. Click here to view the sample Siddhi application. @App:name('CDCWithListeningMode') @App:description('Capture MySQL Inserts using cdc source listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2sp', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream insertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream logStream (name string, amount double); @info(name = 'query') from insertSweetProductionStream select name, amount insert into logStream;","title":"Capturing MySQL Inserts via CDC"},{"location":"samples/CDCWithListeningMode/#capturing-mysql-inserts-via-cdc","text":"","title":"Capturing MySQL Inserts via CDC"},{"location":"samples/CDCWithListeningMode/#purpose","text":"This sample demonstrates how to capture change data from MySQL using Siddhi. The change events that can be captured include INSERT , UPDATE , and DELETE . Before you begin: Ensure that MySQL is installed on your computer. Add the MySQL JDBC driver to the <SI_TOOLING_HOME>/lib directory as follows: Download the JDBC driver from the MySQL website . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar JAR and place it in the <SI_TOOLING_HOME>/lib directory. Configure MySQL to enable binary logging . Enable state persistence in siddhi applications. To do this, open the <SI_TOOLING_HOME>/conf/server/deployment.yaml file and set the state.persistence enabled=true property. Create a database named production by issuing the following command. create database production; Create a user named wso2sp with wso2 as the password, and with SELECT , RELOAD , SHOW DATABASES , REPLICATION SLAVE , REPLICATION CLIENT privileges. To do this, issue the following command. GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2sp' IDENTIFIED BY 'wso2'; Change the database by issuing the following command. use production; Create a table named SweetProductionTable . CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose:"},{"location":"samples/CDCWithListeningMode/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CDCWithListeningMode.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.","title":"Executing the Sample"},{"location":"samples/CDCWithListeningMode/#testing-the-sample","text":"To test the sample Siddhi application, insert a record to the SweetProductionTable table you created by issuing the following command: insert into SweetProductionTable values('chocolate',100.0);","title":"Testing the Sample"},{"location":"samples/CDCWithListeningMode/#viewing-the-results","text":"This insert is logged in the Streaming Integrator console as follows. Info Optionally, you can use this sample to also capture update and delete operations. - delete operation events include before_name and before amount keys. - update operation events include the before_name , name , before_amount , amount keys. Click here to view the sample Siddhi application. @App:name('CDCWithListeningMode') @App:description('Capture MySQL Inserts using cdc source listening mode.') @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2sp', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert', @map(type = 'keyvalue')) define stream insertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream logStream (name string, amount double); @info(name = 'query') from insertSweetProductionStream select name, amount insert into logStream;","title":"Viewing the results"},{"location":"samples/CDCWithPollingMode/","text":"Capturing MySQL Inserts and Updates via CDC Polling Mode \u00b6 Purpose: \u00b6 This sample demonstrates how to use the polling mode of the CDC source. In this example, you are capturing the inserts to a MySQL table. By changing the database type, you can also try out this example for the following databases. Oracle H2 MS SQL Server Postgresql Before you begin: Ensure that MySQL is installed on your computer. Add the MySQL JDBC driver to the <SI_TOOLING_HOME>/lib directory as follows: Download the JDBC driver from the MySQL website . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar JAR and place it in the <SI_HOME>/lib directory. Create a database named production by issuing the following command. CREATE SCHEMA production; Change the database by issuing the following command. use production; Create a table named SweetProductionTable by issuing the following command. CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2)); If you want to capture the changes from the last point of time the Siddhi application was stopped, enable state persistence by setting the state.persistence enabled=true pproperty in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. If you do not enable state persistence, only the changes since the Siddhi application started are captured. In the sample Siddhi application, update the username and password parameters in the source configuration by adding the username and password you use to log in to MySQL as the values. Then save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CDCWithPollingMode.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again. Testing the Sample \u00b6 To test the sample Siddhi application, insert a record to the SweetProductionTable table you created by issuing the following command. insert into SweetProductionTable(name,amount) values('chocolate',100.0); Viewing the results \u00b6 The insert operation is logged in the Streaming Integrator console as shown below. Info You can also update the existing row and observe the change data events. Tip For updates, the previous values of the row are not returned with the event. Use listening mode to obtain such details. The polling mode can also be used with Oracle, MS-SQL server, Postgres, H2. Click here to view the sample Siddhi application. ```sql @App:name(\"CDCWithPollingMode\") @App:description(\"Capture MySQL Inserts and Updates using cdc source polling mode.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '1', username = '', password = '', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream insertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream logStream (name string, amount double); @info(name = 'query') from insertSweetProductionStream select name, amount insert into logStream; ```","title":"Capturing MySQL Inserts and Updates via CDC Polling Mode"},{"location":"samples/CDCWithPollingMode/#capturing-mysql-inserts-and-updates-via-cdc-polling-mode","text":"","title":"Capturing MySQL Inserts and Updates via CDC Polling Mode"},{"location":"samples/CDCWithPollingMode/#purpose","text":"This sample demonstrates how to use the polling mode of the CDC source. In this example, you are capturing the inserts to a MySQL table. By changing the database type, you can also try out this example for the following databases. Oracle H2 MS SQL Server Postgresql Before you begin: Ensure that MySQL is installed on your computer. Add the MySQL JDBC driver to the <SI_TOOLING_HOME>/lib directory as follows: Download the JDBC driver from the MySQL website . Unzip the archive. Copy the mysql-connector-java-5.1.45-bin.jar JAR and place it in the <SI_HOME>/lib directory. Create a database named production by issuing the following command. CREATE SCHEMA production; Change the database by issuing the following command. use production; Create a table named SweetProductionTable by issuing the following command. CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2)); If you want to capture the changes from the last point of time the Siddhi application was stopped, enable state persistence by setting the state.persistence enabled=true pproperty in the <SI_TOOLING_HOME>/conf/server/deployment.yaml file. If you do not enable state persistence, only the changes since the Siddhi application started are captured. In the sample Siddhi application, update the username and password parameters in the source configuration by adding the username and password you use to log in to MySQL as the values. Then save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose:"},{"location":"samples/CDCWithPollingMode/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CDCWithPollingMode.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.","title":"Executing the Sample"},{"location":"samples/CDCWithPollingMode/#testing-the-sample","text":"To test the sample Siddhi application, insert a record to the SweetProductionTable table you created by issuing the following command. insert into SweetProductionTable(name,amount) values('chocolate',100.0);","title":"Testing the Sample"},{"location":"samples/CDCWithPollingMode/#viewing-the-results","text":"The insert operation is logged in the Streaming Integrator console as shown below. Info You can also update the existing row and observe the change data events. Tip For updates, the previous values of the row are not returned with the event. Use listening mode to obtain such details. The polling mode can also be used with Oracle, MS-SQL server, Postgres, H2. Click here to view the sample Siddhi application. ```sql @App:name(\"CDCWithPollingMode\") @App:description(\"Capture MySQL Inserts and Updates using cdc source polling mode.\") @source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production?useSSL=false', mode = 'polling', jdbc.driver.name = 'com.mysql.jdbc.Driver', polling.column = 'last_update', polling.interval = '1', username = '', password = '', table.name = 'SweetProductionTable', @map(type = 'keyvalue' )) define stream insertSweetProductionStream (name string, amount double); @sink(type = 'log') define stream logStream (name string, amount double); @info(name = 'query') from insertSweetProductionStream select name, amount insert into logStream; ```","title":"Viewing the results"},{"location":"samples/CSVCustomMapping/","text":"Receiving and Publishing Events in Custom CSV Format \u00b6 Purpose \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish and receive data events processed within Siddhi to files in CSV custom format. Before you begin: Edit the sample Siddhi application as follows: In the source configuration, update the value for the dir.uri parameter by replacing {WSO2SIHome} with the absolute path of your WSO2 SI Tooling directory. In the sink configuration, update the value for the file.uri parameter by replacing {WSO2SIHome} with the absolute path of your WSO2 SI Tooling directory. If required, you can provide a different path to publish the output to a location of your choice. Save the sample Siddhi application in Streaming Integrator Tooling. Executing and testing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CSVCustomMapping.siddhi - Started Successfully! Viewing the Results \u00b6 The source gets the input from the SI_HOME>/samples/artifacts/CSVMappingWithFile/new/example.csv file and produces the event. This file has data in below format. 1,WSO2,23.5 2,IBM,2.5 The sink gets the input from the source output and publishes the output in the outputOfCustom.csv file. The data is published in this file in the following format. WSO2,1,100.0 IBM,2,2.5 Click here to view the sample Siddhi application. @App:name(\"CSVCustomMapping\") @App:description('Publish and receive data events processed within Siddhi to files in CSV custom format.') @source(type='file', dir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new', action.after.process='NONE', @map(type='csv', @attributes(id='0', name='1', amount='2'))) define stream IntputStream (name string, id int, amount double); @sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfCustom.csv' , @map(type='csv',@payload(id='1', name='0', amount='2'))) define stream OutputStream (name string, id int, amount double); from IntputStream select * insert into OutputStream;","title":"Receiving and Publishing Events in Custom CSV Format"},{"location":"samples/CSVCustomMapping/#receiving-and-publishing-events-in-custom-csv-format","text":"","title":"Receiving and Publishing Events in Custom CSV Format"},{"location":"samples/CSVCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish and receive data events processed within Siddhi to files in CSV custom format. Before you begin: Edit the sample Siddhi application as follows: In the source configuration, update the value for the dir.uri parameter by replacing {WSO2SIHome} with the absolute path of your WSO2 SI Tooling directory. In the sink configuration, update the value for the file.uri parameter by replacing {WSO2SIHome} with the absolute path of your WSO2 SI Tooling directory. If required, you can provide a different path to publish the output to a location of your choice. Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose"},{"location":"samples/CSVCustomMapping/#executing-and-testing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. CSVCustomMapping.siddhi - Started Successfully!","title":"Executing and testing the Sample"},{"location":"samples/CSVCustomMapping/#viewing-the-results","text":"The source gets the input from the SI_HOME>/samples/artifacts/CSVMappingWithFile/new/example.csv file and produces the event. This file has data in below format. 1,WSO2,23.5 2,IBM,2.5 The sink gets the input from the source output and publishes the output in the outputOfCustom.csv file. The data is published in this file in the following format. WSO2,1,100.0 IBM,2,2.5 Click here to view the sample Siddhi application. @App:name(\"CSVCustomMapping\") @App:description('Publish and receive data events processed within Siddhi to files in CSV custom format.') @source(type='file', dir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new', action.after.process='NONE', @map(type='csv', @attributes(id='0', name='1', amount='2'))) define stream IntputStream (name string, id int, amount double); @sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfCustom.csv' , @map(type='csv',@payload(id='1', name='0', amount='2'))) define stream OutputStream (name string, id int, amount double); from IntputStream select * insert into OutputStream;","title":"Viewing the Results"},{"location":"samples/CSVDefaultMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to Publish and receive data events processed within Siddhi to files in CSV default format. Prerequisites: \u00b6 Edit the uri '{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/example.csv' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location. Save this sample. If there is no syntax error, the following messages would be shown on the console: CSVDefaultMapping.siddhi successfully deployed. Executing & testing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: CSVDefaultMapping.siddhi - Started Successfully! Viewing the Results: \u00b6 Source takes input from the '{WSO2SP-HOME}/samples/artifacts/CSVMappingWithFile/new/example.csv' then produce the event. example.csv has data in below format 1,WSO2,23.5 2,IBM,2.5 Sink takes input from source output and produces the output to outputOfCustom.csv in CSV custom format. outputOfCustom.csv's data appear in below format 1,WSO2,100.0 2,IBM,2.5 Click here to view the sample Siddhi application. @App:name(\"CSVDefaultMapping\") @App:description('Publish and receive data events processed within Siddhi to files in CSV default format.') @source(type='file', dir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new', action.after.process='NONE', @map(type='csv')) define stream InputStream (id int, name string, amount double); @sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfDefault.csv' , @map(type='csv')) define stream OutputStream (id int, name string, amount double); from InputStream select * insert into OutputStream;","title":"Publishing and Receiving CSV Events via Files"},{"location":"samples/CSVDefaultMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to Publish and receive data events processed within Siddhi to files in CSV default format.","title":"Purpose:"},{"location":"samples/CSVDefaultMapping/#prerequisites","text":"Edit the uri '{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/example.csv' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location. Save this sample. If there is no syntax error, the following messages would be shown on the console: CSVDefaultMapping.siddhi successfully deployed.","title":"Prerequisites:"},{"location":"samples/CSVDefaultMapping/#executing-testing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: CSVDefaultMapping.siddhi - Started Successfully!","title":"Executing &amp; testing the Sample:"},{"location":"samples/CSVDefaultMapping/#viewing-the-results","text":"Source takes input from the '{WSO2SP-HOME}/samples/artifacts/CSVMappingWithFile/new/example.csv' then produce the event. example.csv has data in below format 1,WSO2,23.5 2,IBM,2.5 Sink takes input from source output and produces the output to outputOfCustom.csv in CSV custom format. outputOfCustom.csv's data appear in below format 1,WSO2,100.0 2,IBM,2.5 Click here to view the sample Siddhi application. @App:name(\"CSVDefaultMapping\") @App:description('Publish and receive data events processed within Siddhi to files in CSV default format.') @source(type='file', dir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new', action.after.process='NONE', @map(type='csv')) define stream InputStream (id int, name string, amount double); @sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfDefault.csv' , @map(type='csv')) define stream OutputStream (id int, name string, amount double); from InputStream select * insert into OutputStream;","title":"Viewing the Results:"},{"location":"samples/ClusTreeTestApp/","text":"Performing Streaming Learning Using a Clustree Model \u00b6 Purpose \u00b6 This sample demonstrates how to perform unsupervised streaming learning on a set of data points using a Clustree model. Before you begin: Download siddhi-gpl-execution-streamingml-x.x.x.jar from here and place it in the <SI_TOOLING_HOME>/lib directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ClusTreeSample/clusTreeTestFeed.json file and place it in the <SI_TOOLING_HOME>/wso2/server/deployment/simulation-configs directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ClusTreeSample/clusTreeFileTest.csv file and place it in the <SI_TOOLING_HOME>/wso2/server/deployment/csv-files directory. Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. ClusTreeTestApp.siddhi - Started Successfully! Testing the Sample \u00b6 To test the sample Siddhi application, simulate multiple events via CSV file in the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Feed Simulation tab of the panel you can see that the clusTreeTestFeed.csv file is loaded. Press the play button to start simulation. Viewing the Results \u00b6 After clicking the play button see the output on the console, that are produced according to the simulation from csv file. Click here to view the sample Siddhi application. @App:name(\"ClusTreeTestApp\") define stream InputStream (x double, y double); @sink(type='log') define stream logStream (closestCentroidCoordinate1 double,closestCentroidCoordinate2 double,x double, y double); @info(name = 'query1') from InputStream#streamingml:clusTree(2, 10, 20, 5, 50, x, y) select closestCentroidCoordinate1, closestCentroidCoordinate2, x, y insert into logStream;","title":"Performing Streaming Learning Using a Clustree Model"},{"location":"samples/ClusTreeTestApp/#performing-streaming-learning-using-a-clustree-model","text":"","title":"Performing Streaming Learning Using a Clustree Model"},{"location":"samples/ClusTreeTestApp/#purpose","text":"This sample demonstrates how to perform unsupervised streaming learning on a set of data points using a Clustree model. Before you begin: Download siddhi-gpl-execution-streamingml-x.x.x.jar from here and place it in the <SI_TOOLING_HOME>/lib directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ClusTreeSample/clusTreeTestFeed.json file and place it in the <SI_TOOLING_HOME>/wso2/server/deployment/simulation-configs directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ClusTreeSample/clusTreeFileTest.csv file and place it in the <SI_TOOLING_HOME>/wso2/server/deployment/csv-files directory. Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose"},{"location":"samples/ClusTreeTestApp/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. ClusTreeTestApp.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/ClusTreeTestApp/#testing-the-sample","text":"To test the sample Siddhi application, simulate multiple events via CSV file in the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Feed Simulation tab of the panel you can see that the clusTreeTestFeed.csv file is loaded. Press the play button to start simulation.","title":"Testing the Sample"},{"location":"samples/ClusTreeTestApp/#viewing-the-results","text":"After clicking the play button see the output on the console, that are produced according to the simulation from csv file. Click here to view the sample Siddhi application. @App:name(\"ClusTreeTestApp\") define stream InputStream (x double, y double); @sink(type='log') define stream logStream (closestCentroidCoordinate1 double,closestCentroidCoordinate2 double,x double, y double); @info(name = 'query1') from InputStream#streamingml:clusTree(2, 10, 20, 5, 50, x, y) select closestCentroidCoordinate1, closestCentroidCoordinate2, x, y insert into logStream;","title":"Viewing the Results"},{"location":"samples/DataPreprocessing/","text":"Pre-processing Data Received via TCP \u00b6 Purpose \u00b6 This example demonstrates how to receive events via the TCP transport and carry out data pre-processing with numerous Siddhi extensions (e.g., string extension, time extension). In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event is changed from yyyy/MM/dd HH:mm:ss to dd-MM-yyyy HH:mm:ss . For more information about Siddhi extensions, see Siddhi Extensions . Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following messages appear in the console. Tcp Server started in 0.0.0.0:9892 DataPreprocessing.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again. Testing the Sample \u00b6 To test this Siddhi application, navigate to the <SI_TOOLING_HOME>/samples/sample-clients/tcp-client directory and run the ant command as follows. ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/DataPreprocessing/data_preprocessing_events.txt -DeventDefinition='{\"event\":{\"id\":\"{0}\",\"value\":{1},\"property\":{2},\"plugId\":{3},\"householdId\":{4},\"houseId\":{5},\"currentTime\":\"{6}\"}}' -Durl=tcp://localhost:9892/SmartHomeStream Viewing the Results \u00b6 Once the DataProcessing Siddhi application receives events from the TCP client, the following messages are displayed in the Streaming Integrator Tooling console: INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621173211, data=[HouseholdID:1::UniqueID:0001, 12.12, 13-08-2001 23:49:33], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621174202, data=[HouseholdID:2::UniqueID:0002, 13.12, 13-08-2004 23:49:34], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621175208, data=[HouseholdID:3::UniqueID:0003, 13.12, 13-08-2006 23:49:35], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621176214, data=[HouseholdID:4::UniqueID:0004, 14.12, 13-08-2008 23:49:36], isExpired=false} Click here to view the sample Siddhi application. @App:name(\"DataPreprocessing\") @App:description('Collect data via TCP transport and pre-process') @Source(type = 'tcp', context='SmartHomeStream', @map(type='json')) define stream SmartHomeStream (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); @sink(type='log') define stream ProcessedStream (compositeID string, value float, formattedTime string); --Process smart home data by concatenating the IDs and formatting the time @info(name='query1') from SmartHomeStream select str:concat(\"HouseholdID:\", convert(householdId, \"string\"), \"::\", \"UniqueID:\", id) as compositeID, value, str:concat(currentTime, \" \", time:currentTime()) as formattedTime insert into ProcessedStream;","title":"Preprocessing Data Received via TCP"},{"location":"samples/DataPreprocessing/#pre-processing-data-received-via-tcp","text":"","title":"Pre-processing Data Received via TCP"},{"location":"samples/DataPreprocessing/#purpose","text":"This example demonstrates how to receive events via the TCP transport and carry out data pre-processing with numerous Siddhi extensions (e.g., string extension, time extension). In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event is changed from yyyy/MM/dd HH:mm:ss to dd-MM-yyyy HH:mm:ss . For more information about Siddhi extensions, see Siddhi Extensions . Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose"},{"location":"samples/DataPreprocessing/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following messages appear in the console. Tcp Server started in 0.0.0.0:9892 DataPreprocessing.siddhi - Started Successfully! Note If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.","title":"Executing the Sample"},{"location":"samples/DataPreprocessing/#testing-the-sample","text":"To test this Siddhi application, navigate to the <SI_TOOLING_HOME>/samples/sample-clients/tcp-client directory and run the ant command as follows. ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/DataPreprocessing/data_preprocessing_events.txt -DeventDefinition='{\"event\":{\"id\":\"{0}\",\"value\":{1},\"property\":{2},\"plugId\":{3},\"householdId\":{4},\"houseId\":{5},\"currentTime\":\"{6}\"}}' -Durl=tcp://localhost:9892/SmartHomeStream","title":"Testing the Sample"},{"location":"samples/DataPreprocessing/#viewing-the-results","text":"Once the DataProcessing Siddhi application receives events from the TCP client, the following messages are displayed in the Streaming Integrator Tooling console: INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621173211, data=[HouseholdID:1::UniqueID:0001, 12.12, 13-08-2001 23:49:33], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621174202, data=[HouseholdID:2::UniqueID:0002, 13.12, 13-08-2004 23:49:34], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621175208, data=[HouseholdID:3::UniqueID:0003, 13.12, 13-08-2006 23:49:35], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621176214, data=[HouseholdID:4::UniqueID:0004, 14.12, 13-08-2008 23:49:36], isExpired=false} Click here to view the sample Siddhi application. @App:name(\"DataPreprocessing\") @App:description('Collect data via TCP transport and pre-process') @Source(type = 'tcp', context='SmartHomeStream', @map(type='json')) define stream SmartHomeStream (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); @sink(type='log') define stream ProcessedStream (compositeID string, value float, formattedTime string); --Process smart home data by concatenating the IDs and formatting the time @info(name='query1') from SmartHomeStream select str:concat(\"HouseholdID:\", convert(householdId, \"string\"), \"::\", \"UniqueID:\", id) as compositeID, value, str:concat(currentTime, \" \", time:currentTime()) as formattedTime insert into ProcessedStream;","title":"Viewing the Results"},{"location":"samples/ExtremaBottomK/","text":"Counting the Frequency of Values with BottomK \u00b6 Purpose: \u00b6 This application demonstrates how to use the siddhi-execution-extrema extension with the bottomK function. Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. ExtremaBottomK.siddhi - Started Successfully! Testing the Sample \u00b6 The ExtremaBottomK Siddhi application can be tested in three ways as follows. Option 1: Publish events via CURL \u00b6 You can publish events in the JSON format to the HTTP endpoint via CURL commands. The CURL commands should be in the format of the example given below. The values for name and amount parameters can change. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Option2: Publish events with Postman \u00b6 Install the Postman application from Chrome web store. Launch the application. Make a POST request to the http://localhost:8006/productionStream endpoint. Set the Content-Type to application/json and set the request body in JSON format as follows. { \"event\":{ \"name\":\"sugar\", \"amount\":20 } } Send some more events in the same format, with different values for the name and amount parameters. Option3: Publish events with http sample client \u00b6 Navigate to the <WSO2SIHome>/samples/sample-clients/http-client directory and run the following command. ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/ExtreamBottomK/ExtremaBottomKEvents.txt -DeventDefinition='{\"event\":{\"name\":\"{0}\",\"amount\":{1}}}' -Durl=http://localhost:8006/productionStream Viewing the Results \u00b6 The output is logged in the Streaming Integrator Tooling console as follows. INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498254202, data=[sugar, 20, sugar, 1, null, null, null, null], isExpired=false}, Event{timestamp=1529498254202, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498262769, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}, Event{timestamp=1529498262769, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498270897, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}, Event{timestamp=1529498270897, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498278304, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}, Event{timestamp=1529498278304, data=[toffee, 25, sugar, 1, toffee, 2, cake, 2], isExpired=false}] Click here to view the sample Siddhi application. @App:name(\"ExtremaBottomK\") @App:Description('Demonstrates how to use the siddhi-execution-extrema with bottomK function') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream inputStream (name string, amount long); @sink(type='log') define stream outputStream(name string, amount long, bottom1Element string, bottom1Frequency long, bottom2Element string, bottom2Frequency long, bottom3Element string, bottom3Frequency long); from inputStream#extrema:bottomK(name, 3) insert all events into outputStream;","title":"Counting the Frequency of Values with BottomK"},{"location":"samples/ExtremaBottomK/#counting-the-frequency-of-values-with-bottomk","text":"","title":"Counting the Frequency of Values with BottomK"},{"location":"samples/ExtremaBottomK/#purpose","text":"This application demonstrates how to use the siddhi-execution-extrema extension with the bottomK function. Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose:"},{"location":"samples/ExtremaBottomK/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. ExtremaBottomK.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/ExtremaBottomK/#testing-the-sample","text":"The ExtremaBottomK Siddhi application can be tested in three ways as follows.","title":"Testing the Sample"},{"location":"samples/ExtremaBottomK/#option-1-publish-events-via-curl","text":"You can publish events in the JSON format to the HTTP endpoint via CURL commands. The CURL commands should be in the format of the example given below. The values for name and amount parameters can change. curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\"","title":"Option 1: Publish events via CURL"},{"location":"samples/ExtremaBottomK/#option2-publish-events-with-postman","text":"Install the Postman application from Chrome web store. Launch the application. Make a POST request to the http://localhost:8006/productionStream endpoint. Set the Content-Type to application/json and set the request body in JSON format as follows. { \"event\":{ \"name\":\"sugar\", \"amount\":20 } } Send some more events in the same format, with different values for the name and amount parameters.","title":"Option2: Publish events with Postman"},{"location":"samples/ExtremaBottomK/#option3-publish-events-with-http-sample-client","text":"Navigate to the <WSO2SIHome>/samples/sample-clients/http-client directory and run the following command. ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/ExtreamBottomK/ExtremaBottomKEvents.txt -DeventDefinition='{\"event\":{\"name\":\"{0}\",\"amount\":{1}}}' -Durl=http://localhost:8006/productionStream","title":"Option3: Publish events with http sample client"},{"location":"samples/ExtremaBottomK/#viewing-the-results","text":"The output is logged in the Streaming Integrator Tooling console as follows. INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498254202, data=[sugar, 20, sugar, 1, null, null, null, null], isExpired=false}, Event{timestamp=1529498254202, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498262769, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}, Event{timestamp=1529498262769, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498270897, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}, Event{timestamp=1529498270897, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}]<br/><br/> INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498278304, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}, Event{timestamp=1529498278304, data=[toffee, 25, sugar, 1, toffee, 2, cake, 2], isExpired=false}] Click here to view the sample Siddhi application. @App:name(\"ExtremaBottomK\") @App:Description('Demonstrates how to use the siddhi-execution-extrema with bottomK function') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream inputStream (name string, amount long); @sink(type='log') define stream outputStream(name string, amount long, bottom1Element string, bottom1Frequency long, bottom2Element string, bottom2Frequency long, bottom3Element string, bottom3Frequency long); from inputStream#extrema:bottomK(name, 3) insert all events into outputStream;","title":"Viewing the Results"},{"location":"samples/GCSSinkSample/","text":"Publishing Events to a GCS Bucket \u00b6 Purpose \u00b6 This example shows how to publish events to a GCS bucket via the siddhi-io-gcs sink extension. The Siddhi application receives events by consuming them from the http://localhost:8006/inputStream endpoint. These events are then sent to the GCS sink that aggregates and commits the events to a Google Cloud Storage bucket. Before you begin: Create an account in Google Cloud . Download the credential file that is generated through the GCP console and save it in a directory of your choice. For more information, see Google Cloud Authentication Documentation . Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample, follow the steps below: Enter values for the credential.path and the bucket.name parameters of the GCS Sink definition in the sample Siddhi application. Info You can either set the credential.path parameter in the sink configuration as shown below or can set a system variable with GOOGLE_APPLICATION_CREDENTIALS as the name with the path to the credential file. Start the sample by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. GCSSinkSample.siddhi - Started Successfully! Testing the Sample \u00b6 To test the sample Siddhi application, open a terminal and issue the following curl command curl -H \"Content-Type: application/json\" -d '{\"event\":{\"key\":\"testFile\",\"payload\":\"message\", \"test\":\"wjson\"}}' http://localhost:8006/inputStream Click here to view the sample Siddhi application. @App:name(\"GCSSinkSample\") @App:description(\"Publish Events to a GCS bucket using a http-source\") @Source(type = 'http', receiver.url='http://localhost:8006/inputStream', @map(type='json')) define stream inputStream(key string, payload string, suffix string); @sink(type='google-cloud-storage', credential.path='<credential.path>', bucket.name='<bucket.name>', object.name='test-object-{{suffix}}', @map(type='text')) define stream outputStream(key string, payload string, suffix string); from inputStream select * insert into outputStream;","title":"Publishing Events to a GCS Bucket"},{"location":"samples/GCSSinkSample/#publishing-events-to-a-gcs-bucket","text":"","title":"Publishing Events to a GCS Bucket"},{"location":"samples/GCSSinkSample/#purpose","text":"This example shows how to publish events to a GCS bucket via the siddhi-io-gcs sink extension. The Siddhi application receives events by consuming them from the http://localhost:8006/inputStream endpoint. These events are then sent to the GCS sink that aggregates and commits the events to a Google Cloud Storage bucket. Before you begin: Create an account in Google Cloud . Download the credential file that is generated through the GCP console and save it in a directory of your choice. For more information, see Google Cloud Authentication Documentation . Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose"},{"location":"samples/GCSSinkSample/#executing-the-sample","text":"To execute the sample, follow the steps below: Enter values for the credential.path and the bucket.name parameters of the GCS Sink definition in the sample Siddhi application. Info You can either set the credential.path parameter in the sink configuration as shown below or can set a system variable with GOOGLE_APPLICATION_CREDENTIALS as the name with the path to the credential file. Start the sample by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. GCSSinkSample.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/GCSSinkSample/#testing-the-sample","text":"To test the sample Siddhi application, open a terminal and issue the following curl command curl -H \"Content-Type: application/json\" -d '{\"event\":{\"key\":\"testFile\",\"payload\":\"message\", \"test\":\"wjson\"}}' http://localhost:8006/inputStream Click here to view the sample Siddhi application. @App:name(\"GCSSinkSample\") @App:description(\"Publish Events to a GCS bucket using a http-source\") @Source(type = 'http', receiver.url='http://localhost:8006/inputStream', @map(type='json')) define stream inputStream(key string, payload string, suffix string); @sink(type='google-cloud-storage', credential.path='<credential.path>', bucket.name='<bucket.name>', object.name='test-object-{{suffix}}', @map(type='text')) define stream outputStream(key string, payload string, suffix string); from inputStream select * insert into outputStream;","title":"Testing the Sample"},{"location":"samples/GeoDistanceCalculation/","text":"Publishing and Receiving CSV Events via Files \u00b6 Purpose \u00b6 This example demonstrates how to calculate the distance between two locations via the siddhi-gpl-execution-geo extension. Before you begin: Download the siddhi-gpl-execution-geo-x.x.x.jar and place it in the <SI_TOOLING_HOME>/lib directory. Save this sample in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. GeoDistanceCalculation.siddhi - Started Successfully! Testing the Sample \u00b6 To test the sample application, simulate a single event for it as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the LocationPointsStream stream of the GeoDistanceCalculation Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name GeoDistanceCalculation StreamName LocationPointsStream As a result, the attributes of the GeoDistanceCalculation stream appear in the panel. Enter attribute values as follows. . Attribute Value latitude 8.116553 longitude 77.523679 prevLatitude 9.850047 prevLongitude 98.597177 Send the event Viewing the Results \u00b6 The following output is logged inthe Streaming Integrator console for the single event you simulated. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GeoDistanceCalculation: Event :, StreamEvent{ timestamp=1513616078228, beforeWindowData=null, onAfterWindowData=null, outputData=[2322119.848252557], type=CURRENT, next=null} Click here to view the sample Siddhi application. @App:name(\"GeoDistanceCalculation\") @App:description('This will demonstrate the distance between two locations') define stream LocationPointsStream (latitude double, longitude double, prevLatitude double, prevLongitude double); @sink(type='log') define stream DistanceStream (distance double); @info(name = 'query1') from LocationPointsStream select geo:distance(latitude, longitude, prevLatitude, prevLongitude) as distance insert into DistanceStream;","title":"Calculating the Distance Between Two Locations"},{"location":"samples/GeoDistanceCalculation/#publishing-and-receiving-csv-events-via-files","text":"","title":"Publishing and Receiving CSV Events via Files"},{"location":"samples/GeoDistanceCalculation/#purpose","text":"This example demonstrates how to calculate the distance between two locations via the siddhi-gpl-execution-geo extension. Before you begin: Download the siddhi-gpl-execution-geo-x.x.x.jar and place it in the <SI_TOOLING_HOME>/lib directory. Save this sample in Streaming Integrator Tooling.","title":"Purpose"},{"location":"samples/GeoDistanceCalculation/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. GeoDistanceCalculation.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/GeoDistanceCalculation/#testing-the-sample","text":"To test the sample application, simulate a single event for it as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the LocationPointsStream stream of the GeoDistanceCalculation Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name GeoDistanceCalculation StreamName LocationPointsStream As a result, the attributes of the GeoDistanceCalculation stream appear in the panel. Enter attribute values as follows. . Attribute Value latitude 8.116553 longitude 77.523679 prevLatitude 9.850047 prevLongitude 98.597177 Send the event","title":"Testing the Sample"},{"location":"samples/GeoDistanceCalculation/#viewing-the-results","text":"The following output is logged inthe Streaming Integrator console for the single event you simulated. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GeoDistanceCalculation: Event :, StreamEvent{ timestamp=1513616078228, beforeWindowData=null, onAfterWindowData=null, outputData=[2322119.848252557], type=CURRENT, next=null} Click here to view the sample Siddhi application. @App:name(\"GeoDistanceCalculation\") @App:description('This will demonstrate the distance between two locations') define stream LocationPointsStream (latitude double, longitude double, prevLatitude double, prevLongitude double); @sink(type='log') define stream DistanceStream (distance double); @info(name = 'query1') from LocationPointsStream select geo:distance(latitude, longitude, prevLatitude, prevLongitude) as distance insert into DistanceStream;","title":"Viewing the Results"},{"location":"samples/GplNLPFindNameEntityType/","text":"Purpose: \u00b6 Through this app, NamedEntity of given type \"organization\" is extracted from the provided string. Prerequisites: \u00b6 Download siddhi-gpl-execution-nlp-x.x.x.jar from the following link and copy the jar to {WSO2SIHome}/lib http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/nlp/siddhi-gpl-execution-nlp/ Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App GplNLPFindNameEntityType.siddhi successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console GplNLPFindNameEntityType.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : GplNLPFindNameEntityType * Stream Name : InputStream 3. Enter following string in the message and send ABC factory produces 20 donuts per day Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GplNLPFindNameEntityType: StreamEvent{ timestamp=1513573692406, beforeWindowData=null, onAfterWindowData=null, outputData=[ABC factory produces 20 donuts per day., ABC], type=CURRENT, next=null} ABC has been recognized as an organization. Note: \u00b6 Stop this Siddhi application, once you are done with the execution @App:name('GplNLPFindNameEntityType') @App:Description(\"NamedEntity of given type 'organization' is extracted from the provided string\") define stream InputStream (message string); @sink(type='log') define stream outputStream (message string, match string); from InputStream#nlp:findNameEntityType( 'ORGANIZATION', true, message ) select * insert into FindNameEntityTypeResult; from FindNameEntityTypeResult select * insert into outputStream;","title":"Extracting Values from a String"},{"location":"samples/GplNLPFindNameEntityType/#purpose","text":"Through this app, NamedEntity of given type \"organization\" is extracted from the provided string.","title":"Purpose:"},{"location":"samples/GplNLPFindNameEntityType/#prerequisites","text":"Download siddhi-gpl-execution-nlp-x.x.x.jar from the following link and copy the jar to {WSO2SIHome}/lib http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/nlp/siddhi-gpl-execution-nlp/ Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App GplNLPFindNameEntityType.siddhi successfully deployed.","title":"Prerequisites:"},{"location":"samples/GplNLPFindNameEntityType/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console GplNLPFindNameEntityType.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/GplNLPFindNameEntityType/#testing-the-sample","text":"You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : GplNLPFindNameEntityType * Stream Name : InputStream 3. Enter following string in the message and send ABC factory produces 20 donuts per day","title":"Testing the Sample:"},{"location":"samples/GplNLPFindNameEntityType/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GplNLPFindNameEntityType: StreamEvent{ timestamp=1513573692406, beforeWindowData=null, onAfterWindowData=null, outputData=[ABC factory produces 20 donuts per day., ABC], type=CURRENT, next=null} ABC has been recognized as an organization.","title":"Viewing the Results:"},{"location":"samples/GplNLPFindNameEntityType/#note","text":"Stop this Siddhi application, once you are done with the execution @App:name('GplNLPFindNameEntityType') @App:Description(\"NamedEntity of given type 'organization' is extracted from the provided string\") define stream InputStream (message string); @sink(type='log') define stream outputStream (message string, match string); from InputStream#nlp:findNameEntityType( 'ORGANIZATION', true, message ) select * insert into FindNameEntityTypeResult; from FindNameEntityTypeResult select * insert into outputStream;","title":"Note:"},{"location":"samples/HelloKafka/","text":"Purpose: \u00b6 This application demonstrates how to use the Kafka transport in Siddhi to receive and publish events. Events which are in JSON format are consumed from one Kafka topic and written to another Kafka topic in XML format. Prerequisites: \u00b6 The following steps must be executed to enable WSO2 SP to receive and publish events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding. Download the Kafka broker from here: https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {WSO2SIHome}/libs directory as follows. Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory named {Destination} in a preferred location in your machine. To convert all the Kafka jars you copied into the {Source} directory, issue the following command, For Windows: {WSO2SIHome}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path> For Linux: sh {WSO2SIHome}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path> Add the OSGI converted kafka libs from {Destination} directory to {WSO2SIHome}/lib . Add the original Kafka libs from {Source} to {WSO2SIHome}/samples/sample-clients/lib . Navigate to {KafkaHome} and start zookeeper node using following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start Kafka server node using following command. sh bin/kafka-server-start.sh config/server.properties Start the server using following command . sh streaming-integrator-tooling.sh Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console. HelloKafka.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run the following command. ant -DtopicList=kafka_result_topic -Dtype=xml -DpartitionList=0 Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run the following command. ant -DtopicName=kafka_topic -DfilePath={WSO2SIHome}/samples/artifacts/HelloKafka/kafka_sample.txt This command would publish the events in kafka_sample file to the Source Kafka Topic (named 'kafka_topic'). Viewing the Results: \u00b6 See the output events received by Sink Kafka Topic (named kafka_result_topic ) being logged on the kafka-consumer console. Note how the events have been converted from JSON to XML type. This conversion happens due to the Sink configuration's map type being XML. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic') /* SweetProductionStream definition. It receives events from \"kafka_topic\" in json format. Events in this stream will have information about the name of the sweet and how much of it is produced. */ @source(type='kafka', topic.list='kafka_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); /* Suppose that the factory packs sweets by taking last 3 sweet productions disregarding their individual amount. TotalStream will have total of the last 3 events in SweetProductionStream. This is calcuklated as follows; the sum of 1st, 2nd, and 3rd events of SweetProductionStream will be the 1st event of TotalStream and the sum of 4th, 5th, and 6th events of SweetProductionStream will be the 2nd event of TotalStream */ define stream TotalStream (total double); /* This stream counts the event number of TotalStream and sends that count along with total. This will help us find out the batch which has a low total weight by using the count as batch number as we will see in the LowProductionAlertStream */ define stream TotalStreamWithBatch(batchNumber long, total double); /* This stream will send an alert into kafka_result_topic if any batch has a total weight less than 10. Batch number of the low weight batch and the actual weight will be sent out. */ @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', partition.no='0', @map(type='xml')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); --summing events in SweetProductionStream in batches of 3 and sending to TotalStream @info(name='query1') from SweetProductionStream#window.lengthBatch(3) select sum(amount) as total insert into TotalStream; --count is included to indicate batch number from TotalStream select count() as batchNumber, total insert into TotalStreamWithBatch; --filtering out events with total less than 10 to create alert from TotalStreamWithBatch[total < 10] select batchNumber, total as lowTotal insert into LowProductionAlertStream;","title":"Consuming Events from a Kafka Topic and Publishing to Another Kafka Topic"},{"location":"samples/HelloKafka/#purpose","text":"This application demonstrates how to use the Kafka transport in Siddhi to receive and publish events. Events which are in JSON format are consumed from one Kafka topic and written to another Kafka topic in XML format.","title":"Purpose:"},{"location":"samples/HelloKafka/#prerequisites","text":"The following steps must be executed to enable WSO2 SP to receive and publish events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding. Download the Kafka broker from here: https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {WSO2SIHome}/libs directory as follows. Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory named {Destination} in a preferred location in your machine. To convert all the Kafka jars you copied into the {Source} directory, issue the following command, For Windows: {WSO2SIHome}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path> For Linux: sh {WSO2SIHome}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path> Add the OSGI converted kafka libs from {Destination} directory to {WSO2SIHome}/lib . Add the original Kafka libs from {Source} to {WSO2SIHome}/samples/sample-clients/lib . Navigate to {KafkaHome} and start zookeeper node using following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start Kafka server node using following command. sh bin/kafka-server-start.sh config/server.properties Start the server using following command . sh streaming-integrator-tooling.sh Save this sample.","title":"Prerequisites:"},{"location":"samples/HelloKafka/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console. HelloKafka.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/HelloKafka/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run the following command. ant -DtopicList=kafka_result_topic -Dtype=xml -DpartitionList=0 Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run the following command. ant -DtopicName=kafka_topic -DfilePath={WSO2SIHome}/samples/artifacts/HelloKafka/kafka_sample.txt This command would publish the events in kafka_sample file to the Source Kafka Topic (named 'kafka_topic').","title":"Testing the Sample:"},{"location":"samples/HelloKafka/#viewing-the-results","text":"See the output events received by Sink Kafka Topic (named kafka_result_topic ) being logged on the kafka-consumer console. Note how the events have been converted from JSON to XML type. This conversion happens due to the Sink configuration's map type being XML. @App:name(\"HelloKafka\") @App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic') /* SweetProductionStream definition. It receives events from \"kafka_topic\" in json format. Events in this stream will have information about the name of the sweet and how much of it is produced. */ @source(type='kafka', topic.list='kafka_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='localhost:9092', @map(type='json')) define stream SweetProductionStream (name string, amount double); /* Suppose that the factory packs sweets by taking last 3 sweet productions disregarding their individual amount. TotalStream will have total of the last 3 events in SweetProductionStream. This is calcuklated as follows; the sum of 1st, 2nd, and 3rd events of SweetProductionStream will be the 1st event of TotalStream and the sum of 4th, 5th, and 6th events of SweetProductionStream will be the 2nd event of TotalStream */ define stream TotalStream (total double); /* This stream counts the event number of TotalStream and sends that count along with total. This will help us find out the batch which has a low total weight by using the count as batch number as we will see in the LowProductionAlertStream */ define stream TotalStreamWithBatch(batchNumber long, total double); /* This stream will send an alert into kafka_result_topic if any batch has a total weight less than 10. Batch number of the low weight batch and the actual weight will be sent out. */ @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', partition.no='0', @map(type='xml')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); --summing events in SweetProductionStream in batches of 3 and sending to TotalStream @info(name='query1') from SweetProductionStream#window.lengthBatch(3) select sum(amount) as total insert into TotalStream; --count is included to indicate batch number from TotalStream select count() as batchNumber, total insert into TotalStreamWithBatch; --filtering out events with total less than 10 to create alert from TotalStreamWithBatch[total < 10] select batchNumber, total as lowTotal insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/HttpRequestResponseSample/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format, Receive response from the http server and process the response using siddhi. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App HttpRequestResponseSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: HttpRequestResponseSample.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'. Testing the Sample: \u00b6 Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run \"ant\" command without any arguments. Send events using one or more of the following methods. Send events with http server through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specifiy the values as follows: Siddhi App Name : HttpRequestResponseSample Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. Send more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"HttpRequestResponseSample\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"HttpRequestResponseSample\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" When publishing the events, http-request sink will send the request to the http server and the server will echo the received request as the response with a 200 http status code. Then, That successful response will be received by the defined http-response source which has the relevant http status code. Received response will be converted to a siddhi event using using json default mapping and pushed to the ResponseStream. Viewing the Results: \u00b6 The received responses will be logged in the terminal/editor console as following. INFO {io.siddhi.core.stream.output.sink.LogSink} - RequestResponse : responseStream : Event{timestamp=1555358941592, data=[toffees, 75.6], isExpired=false} Notes: \u00b6 If the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"HttpRequestResponseSample\") @App:description(\"Publish http requests, receive their responses and process them\") define stream SweetProductionStream (name string, amount double); @sink(type='http-request', sink.id='production-request', publisher.url='http://localhost:8080/abc', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); @sink(type='log') @source(type='http-response' , sink.id='production-request', http.status.code='200', @map(type='json')) define stream ResponseStream(name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing HTTP Requests, Receiving Responses, and Processing Them"},{"location":"samples/HttpRequestResponseSample/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format, Receive response from the http server and process the response using siddhi.","title":"Purpose:"},{"location":"samples/HttpRequestResponseSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App HttpRequestResponseSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/HttpRequestResponseSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: HttpRequestResponseSample.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.","title":"Executing the Sample:"},{"location":"samples/HttpRequestResponseSample/#testing-the-sample","text":"Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run \"ant\" command without any arguments. Send events using one or more of the following methods. Send events with http server through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specifiy the values as follows: Siddhi App Name : HttpRequestResponseSample Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. Send more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"HttpRequestResponseSample\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"HttpRequestResponseSample\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" When publishing the events, http-request sink will send the request to the http server and the server will echo the received request as the response with a 200 http status code. Then, That successful response will be received by the defined http-response source which has the relevant http status code. Received response will be converted to a siddhi event using using json default mapping and pushed to the ResponseStream.","title":"Testing the Sample:"},{"location":"samples/HttpRequestResponseSample/#viewing-the-results","text":"The received responses will be logged in the terminal/editor console as following. INFO {io.siddhi.core.stream.output.sink.LogSink} - RequestResponse : responseStream : Event{timestamp=1555358941592, data=[toffees, 75.6], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/HttpRequestResponseSample/#notes","text":"If the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"HttpRequestResponseSample\") @App:description(\"Publish http requests, receive their responses and process them\") define stream SweetProductionStream (name string, amount double); @sink(type='http-request', sink.id='production-request', publisher.url='http://localhost:8080/abc', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); @sink(type='log') @source(type='http-response' , sink.id='production-request', http.status.code='200', @map(type='json')) define stream ResponseStream(name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/IBMMessageQueue/","text":"Purpose: \u00b6 This application demonstrates how to consume events from IBM Message Queue and publish messages in to a IBM Queue Prerequisites: \u00b6 Ensure that there is a running IBM MQ instance. Create a queue manager named ESBQManager, Queue named Queue1 and channel named Channel1 Download com.ibm.mq.allclient_9.0.5.0_1.0.0.jar and javax.jms-api-2.0.1.jar and copy to /lib directory. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console IBMMessageQueueSample.siddhi - Started Successfully! Testing the Sample: \u00b6 Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'IBMMessageQueueSample' as 'Siddhi App Name' -> Select 'SweetProductionSinkStream' as 'Stream Name' -> Provide attribute values -> Send Viewing the Results: \u00b6 See the output on the console. @App:name(\"IBMMessageQueueSample\") @App:description('Consume event from IBM Message Queue') @source(type='ibmmq', destination.name='Queue1', host='192.168.56.3', port='1414', channel='Channel1', queue.manager = 'ESBQManager', username = 'mqm', password = '1920', @map(type='xml')) define stream SweetProductionSourceStream(name string); @sink(type='ibmmq', destination.name='Queue1', host='192.168.56.3', port='1414', channel='Channel1', queue.manager = 'ESBQManager', username = 'mqm', password = '1920', @map(type='xml')) define stream SweetProductionSinkStream(name string); @sink(type='log') define stream logStream(name string); from SweetProductionSourceStream select * insert into logStream;","title":"Consuming Messages from IBM Message Queues"},{"location":"samples/IBMMessageQueue/#purpose","text":"This application demonstrates how to consume events from IBM Message Queue and publish messages in to a IBM Queue","title":"Purpose:"},{"location":"samples/IBMMessageQueue/#prerequisites","text":"Ensure that there is a running IBM MQ instance. Create a queue manager named ESBQManager, Queue named Queue1 and channel named Channel1 Download com.ibm.mq.allclient_9.0.5.0_1.0.0.jar and javax.jms-api-2.0.1.jar and copy to /lib directory.","title":"Prerequisites:"},{"location":"samples/IBMMessageQueue/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console IBMMessageQueueSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/IBMMessageQueue/#testing-the-sample","text":"Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'IBMMessageQueueSample' as 'Siddhi App Name' -> Select 'SweetProductionSinkStream' as 'Stream Name' -> Provide attribute values -> Send","title":"Testing the Sample:"},{"location":"samples/IBMMessageQueue/#viewing-the-results","text":"See the output on the console. @App:name(\"IBMMessageQueueSample\") @App:description('Consume event from IBM Message Queue') @source(type='ibmmq', destination.name='Queue1', host='192.168.56.3', port='1414', channel='Channel1', queue.manager = 'ESBQManager', username = 'mqm', password = '1920', @map(type='xml')) define stream SweetProductionSourceStream(name string); @sink(type='ibmmq', destination.name='Queue1', host='192.168.56.3', port='1414', channel='Channel1', queue.manager = 'ESBQManager', username = 'mqm', password = '1920', @map(type='xml')) define stream SweetProductionSinkStream(name string); @sink(type='log') define stream logStream(name string); from SweetProductionSourceStream select * insert into logStream;","title":"Viewing the Results:"},{"location":"samples/JoinWithStoredData/","text":"Purpose: \u00b6 This application demonstrates how to perform join on streaming data with the data stored in RDBMS. The sample depicts a scenario, where a transaction by a credit card with which fraudulent activity has been previously done. The credit card numbers, which were noted for fraudulent activities are stored in an RDBMS table. Prerequisites: \u00b6 Ensure that MySQL is installed on your machine Create a database named 'fraudDB' in MySQL. This database is referred to with 'jdbc:mysql://localhost:3306/fraudDB' url. Create a table named 'FraudTable': CREATE TABLE FraudTable (creditCardNo VARCHAR(20)); Insert some values to the table : INSERT INTO FraudTable VALUES (\"143-90099-23433\"); In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console JoinWithStoredData.siddhi - Started Successfully! Testing the Sample: \u00b6 Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'JoinWithStoredData' as 'Siddhi App Name' -> Select 'TradeStream' as 'Stream Name' -> Provide attribute values -> Send Send at-least one event with the single event simulator, where the creditCardNo matches a creditCardNo value in the data we previously inserted to the FraudTable. This would satisfy the 'on' condition of our join query Viewing the Results: \u00b6 See the output for suspicious trades on the console. @App:name(\"JoinWithStoredData\") @App:description('Join streaming data with data stored in an RDBMS table') @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/fraudDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"creditCardNo\") define table FraudTable (creditCardNo string); define stream TradeStream(creditCardNo string, trader string, tradeInfo string); @sink(type='log') define stream SuspiciousTradeStream(creditCardNo string, suspiciousTrader string, suspiciousInfoTrade string); --Perform a join on credit card number, to capture transactions with credit cards that have previously been used for fraudulent activity @info(name='query1') from TradeStream as t join FraudTable as f on t.creditCardNo == f.creditCardNo select t.creditCardNo, t.trader as suspiciousTrader, t.tradeInfo as suspiciousInfoTrade insert into SuspiciousTradeStream;","title":"Joining Streaming Data with Stored Data in RDBMS"},{"location":"samples/JoinWithStoredData/#purpose","text":"This application demonstrates how to perform join on streaming data with the data stored in RDBMS. The sample depicts a scenario, where a transaction by a credit card with which fraudulent activity has been previously done. The credit card numbers, which were noted for fraudulent activities are stored in an RDBMS table.","title":"Purpose:"},{"location":"samples/JoinWithStoredData/#prerequisites","text":"Ensure that MySQL is installed on your machine Create a database named 'fraudDB' in MySQL. This database is referred to with 'jdbc:mysql://localhost:3306/fraudDB' url. Create a table named 'FraudTable': CREATE TABLE FraudTable (creditCardNo VARCHAR(20)); Insert some values to the table : INSERT INTO FraudTable VALUES (\"143-90099-23433\"); In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials Save this sample","title":"Prerequisites:"},{"location":"samples/JoinWithStoredData/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console JoinWithStoredData.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/JoinWithStoredData/#testing-the-sample","text":"Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'JoinWithStoredData' as 'Siddhi App Name' -> Select 'TradeStream' as 'Stream Name' -> Provide attribute values -> Send Send at-least one event with the single event simulator, where the creditCardNo matches a creditCardNo value in the data we previously inserted to the FraudTable. This would satisfy the 'on' condition of our join query","title":"Testing the Sample:"},{"location":"samples/JoinWithStoredData/#viewing-the-results","text":"See the output for suspicious trades on the console. @App:name(\"JoinWithStoredData\") @App:description('Join streaming data with data stored in an RDBMS table') @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/fraudDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"creditCardNo\") define table FraudTable (creditCardNo string); define stream TradeStream(creditCardNo string, trader string, tradeInfo string); @sink(type='log') define stream SuspiciousTradeStream(creditCardNo string, suspiciousTrader string, suspiciousInfoTrade string); --Perform a join on credit card number, to capture transactions with credit cards that have previously been used for fraudulent activity @info(name='query1') from TradeStream as t join FraudTable as f on t.creditCardNo == f.creditCardNo select t.creditCardNo, t.trader as suspiciousTrader, t.tradeInfo as suspiciousInfoTrade insert into SuspiciousTradeStream;","title":"Viewing the Results:"},{"location":"samples/KalmanFilterExecutionSample/","text":"Purpose: \u00b6 This Kalman Filter function uses measurements observed over time containing noise and other inaccuracies, and produces estimated values for the current measurement using Kalman algorithms Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App KalmanFilterExecutionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console KalmanFilterExecutionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : KalmanFilterExecutionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: cake * amount: 55.6 4. Enter following values in the fields and send * name: cake * amount: 20.5 Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - KalmanFilterExecutionSample: Event :, StreamEvent{ timestamp=1513378257580, beforeWindowData=null, onAfterWindowData=null, outputData=[55.6], type=CURRENT, next=null} INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - KalmanFilterExecutionSample: Event :, StreamEvent{ timestamp=1513378266843, beforeWindowData=null, onAfterWindowData=null, outputData=[38.05000877553216], type=CURRENT, next=null} @App:name(\"KalmanFilterExecutionSample\") @App:description('Produces estimated values for the current measurement of amount using Kalman algorithms.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream EstimatedAmountStream(name string, estimatedAmount double); from SweetProductionStream select name, kf:kalmanFilter(amount) as estimatedAmount insert into EstimatedAmountStream;","title":"Estimating Values Using Kalman Algorithms"},{"location":"samples/KalmanFilterExecutionSample/#purpose","text":"This Kalman Filter function uses measurements observed over time containing noise and other inaccuracies, and produces estimated values for the current measurement using Kalman algorithms","title":"Purpose:"},{"location":"samples/KalmanFilterExecutionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App KalmanFilterExecutionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/KalmanFilterExecutionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console KalmanFilterExecutionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/KalmanFilterExecutionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : KalmanFilterExecutionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: cake * amount: 55.6 4. Enter following values in the fields and send * name: cake * amount: 20.5","title":"Testing the Sample:"},{"location":"samples/KalmanFilterExecutionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - KalmanFilterExecutionSample: Event :, StreamEvent{ timestamp=1513378257580, beforeWindowData=null, onAfterWindowData=null, outputData=[55.6], type=CURRENT, next=null} INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - KalmanFilterExecutionSample: Event :, StreamEvent{ timestamp=1513378266843, beforeWindowData=null, onAfterWindowData=null, outputData=[38.05000877553216], type=CURRENT, next=null} @App:name(\"KalmanFilterExecutionSample\") @App:description('Produces estimated values for the current measurement of amount using Kalman algorithms.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream EstimatedAmountStream(name string, estimatedAmount double); from SweetProductionStream select name, kf:kalmanFilter(amount) as estimatedAmount insert into EstimatedAmountStream;","title":"Viewing the Results:"},{"location":"samples/MapExtensionSample/","text":"Purpose: \u00b6 This function creates a map and added values and checks whether values are available Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App MapExtensionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MapExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : MapExtensionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: chocolate cake * amount: 50.50 Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MapExtensionSample: Event: , StreamEvent{ timestamp=1513384974698, beforeWindowData=null, onAfterWindowData=null, outputData=[true, false], type=CURRENT, next=null} @APP:name(\"MapExtensionSample\") @App:description('Insert values into a map and access') @sink(type='log') define stream CheckedMapStream(isMap1 bool, isMap2 bool); define stream SweetProductionStream (name string, amount double); @info(name = 'query1') from SweetProductionStream select name, amount, map:create() as tmpMap insert into tmpStream; @info(name = 'query2') from tmpStream select name, amount, map:put(tmpMap,name,amount) as map1 insert into outputStream; @info(name = 'query3') from outputStream select map:isMap(map1) as isMap1, map:isMap(name) as isMap2 insert into CheckedMapStream;","title":"Inserting and Accessing Data in a Map"},{"location":"samples/MapExtensionSample/#purpose","text":"This function creates a map and added values and checks whether values are available","title":"Purpose:"},{"location":"samples/MapExtensionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App MapExtensionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/MapExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MapExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/MapExtensionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : MapExtensionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: chocolate cake * amount: 50.50","title":"Testing the Sample:"},{"location":"samples/MapExtensionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MapExtensionSample: Event: , StreamEvent{ timestamp=1513384974698, beforeWindowData=null, onAfterWindowData=null, outputData=[true, false], type=CURRENT, next=null} @APP:name(\"MapExtensionSample\") @App:description('Insert values into a map and access') @sink(type='log') define stream CheckedMapStream(isMap1 bool, isMap2 bool); define stream SweetProductionStream (name string, amount double); @info(name = 'query1') from SweetProductionStream select name, amount, map:create() as tmpMap insert into tmpStream; @info(name = 'query2') from tmpStream select name, amount, map:put(tmpMap,name,amount) as map1 insert into outputStream; @info(name = 'query3') from outputStream select map:isMap(map1) as isMap1, map:isMap(name) as isMap2 insert into CheckedMapStream;","title":"Viewing the Results:"},{"location":"samples/MarkovChainSample/","text":"Purpose: \u00b6 This application demonstrates state transition based on an existing Markov matrix, when the transition probability is less than or equal to the alert threshold probability, notify state is changed to 'true' in the event. Prerequisites: \u00b6 Please find the below resource files in {WSO2_SI_Home}/samples/artifacts/MarkovChainSample directory. Copy MarkovChainSimulator.json file to {WSO2_SI_Home}/wso2/server/deployment/simulation-configs Copy MarkovChainTest.csv file to {WSO2_SI_Home}/wso2/server/deployment/csv-files Copy MarkovChainMatrix.csv file to {WSO2_SI_Home}/wso2/server/deployment/csv-files Copy MarkovChainSample.siddhi file to {WSO2_SI_Home}/wso2/server/deployment/siddhi-files/ and replace the absolute file path for MarkovChainMatrix.csv in 'MarkovChainSample' Siddhi App. Start editor runtime. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MarkovChainSample.siddhi - Started Successfully!. Testing the Sample: \u00b6 Click 'Feed Simulation' Run by click on the play button in the 'MarkovChainSimulator' Viewing the Results: \u00b6 Messages similar to the following would be shown on the editor console. INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624961461, data=[1, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624962461, data=[2, null, testState02, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624963461, data=[1, testState01, testState03, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624964461, data=[3, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624965461, data=[3, testState01, testState02, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624966461, data=[1, testState03, testState01, 0.6000000000000001, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624967461, data=[1, testState01, testState02, 0.5333333333333333, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624968461, data=[2, testState02, testState01, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624969461, data=[2, testState01, testState03, 0.325, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624970461, data=[4, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624971461, data=[4, testState01, testState03, 0.45999999999999996, false], isExpired=false} You will notice that when the transition probability is less than or equal to the alert threshold probability, notify state is changed to 'true' @App:name(\"MarkovChainSample\") @App:description('Identify abnormal patterns based on an existing Markov matrix.') define stream StateStream (id string, state string); @sink(type=\"log\") define stream StatePatternStream (id string, lastState string, state string, transitionProbability double, notify bool); @info(name = 'query1') from StateStream#markov:markovChain(id, state, 60 min, 0.3, '{WSO2_SI_Home}/wso2/editor/deployment/csv-files/MarkovChainMatrix.csv', false) select id, lastState, state, transitionProbability, notify insert into StatePatternStream;","title":"Identifying Abnormal Patterns via a Markov Matrix"},{"location":"samples/MarkovChainSample/#purpose","text":"This application demonstrates state transition based on an existing Markov matrix, when the transition probability is less than or equal to the alert threshold probability, notify state is changed to 'true' in the event.","title":"Purpose:"},{"location":"samples/MarkovChainSample/#prerequisites","text":"Please find the below resource files in {WSO2_SI_Home}/samples/artifacts/MarkovChainSample directory. Copy MarkovChainSimulator.json file to {WSO2_SI_Home}/wso2/server/deployment/simulation-configs Copy MarkovChainTest.csv file to {WSO2_SI_Home}/wso2/server/deployment/csv-files Copy MarkovChainMatrix.csv file to {WSO2_SI_Home}/wso2/server/deployment/csv-files Copy MarkovChainSample.siddhi file to {WSO2_SI_Home}/wso2/server/deployment/siddhi-files/ and replace the absolute file path for MarkovChainMatrix.csv in 'MarkovChainSample' Siddhi App. Start editor runtime.","title":"Prerequisites:"},{"location":"samples/MarkovChainSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MarkovChainSample.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/MarkovChainSample/#testing-the-sample","text":"Click 'Feed Simulation' Run by click on the play button in the 'MarkovChainSimulator'","title":"Testing the Sample:"},{"location":"samples/MarkovChainSample/#viewing-the-results","text":"Messages similar to the following would be shown on the editor console. INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624961461, data=[1, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624962461, data=[2, null, testState02, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624963461, data=[1, testState01, testState03, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624964461, data=[3, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624965461, data=[3, testState01, testState02, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624966461, data=[1, testState03, testState01, 0.6000000000000001, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624967461, data=[1, testState01, testState02, 0.5333333333333333, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624968461, data=[2, testState02, testState01, 0.3, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624969461, data=[2, testState01, testState03, 0.325, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624970461, data=[4, null, testState01, 0.0, false], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - MarkovChainSample : StatePatternStream : Event{timestamp=1513624971461, data=[4, testState01, testState03, 0.45999999999999996, false], isExpired=false} You will notice that when the transition probability is less than or equal to the alert threshold probability, notify state is changed to 'true' @App:name(\"MarkovChainSample\") @App:description('Identify abnormal patterns based on an existing Markov matrix.') define stream StateStream (id string, state string); @sink(type=\"log\") define stream StatePatternStream (id string, lastState string, state string, transitionProbability double, notify bool); @info(name = 'query1') from StateStream#markov:markovChain(id, state, 60 min, 0.3, '{WSO2_SI_Home}/wso2/editor/deployment/csv-files/MarkovChainMatrix.csv', false) select id, lastState, state, transitionProbability, notify insert into StatePatternStream;","title":"Viewing the Results:"},{"location":"samples/MathExtensionSample/","text":"Purpose: \u00b6 This function returns the smallest (closest to negative infinity) double value that is greater than or equal to the p1 argument, and is equal to a mathematical integer. This function wraps thejava.lang.Math.ceil() method. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App MathExtensionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MathExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : MathExtensionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: chocolate cake * amount: 50.50 4. Enter following values in the fields and send * name: coffee cake * amount: 50.30 Viewing the Results: \u00b6 Messages similar to the following would be shown on the console.\\ INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381581963, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null}\\ INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381917721, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null} @App:name(\"MathExtensionSample\") @App:description('Rounds up the sweets amount') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream AmountCorrectionStream(name string, amount double); from SweetProductionStream select name, math:ceil(amount) as amount insert into AmountCorrectionStream;","title":"Rounding up Amounts via the Math Function"},{"location":"samples/MathExtensionSample/#purpose","text":"This function returns the smallest (closest to negative infinity) double value that is greater than or equal to the p1 argument, and is equal to a mathematical integer. This function wraps thejava.lang.Math.ceil() method.","title":"Purpose:"},{"location":"samples/MathExtensionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console Siddhi App MathExtensionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/MathExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console MathExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/MathExtensionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : MathExtensionSample * Stream Name : SweetProductionStream 3. Enter following values in the fields and send * name: chocolate cake * amount: 50.50 4. Enter following values in the fields and send * name: coffee cake * amount: 50.30","title":"Testing the Sample:"},{"location":"samples/MathExtensionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console.\\ INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381581963, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null}\\ INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381917721, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null} @App:name(\"MathExtensionSample\") @App:description('Rounds up the sweets amount') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream AmountCorrectionStream(name string, amount double); from SweetProductionStream select name, math:ceil(amount) as amount insert into AmountCorrectionStream;","title":"Viewing the Results:"},{"location":"samples/PatternMatching/","text":"Purpose: \u00b6 This application demonstrates how to detect patterns with Siddhi pattern concept. In the sample we capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes Prerequisites: \u00b6 Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PatternMatching.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. Testing the Sample: \u00b6 Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'PatternMatching' as 'Siddhi App Name' -> Select 'RoomTemperatureStream' as 'Stream Name' -> Provide attribute values -> Send To generate an alert, send one event, followed by another event (within 2 mins) where the temperature of the second event shows an increment by 5 degrees or more (eg. Temperature of event 1 = 17.0, Temperature of event 2 = 30.0). Note that these two events may or may not be consecutive. Viewing the Results: \u00b6 See the output on the console. Output will be shown if the second temperature input is incremented by 5 degrees or more. Note: \u00b6 Stop this Siddhi application, once you are done with the execution @App:name(\"PatternMatching\") @App:description('Identify event patterns based on the order of event arrival') define stream RoomTemperatureStream(roomNo string, temp double); @sink(type='log') define stream RoomTemperatureAlertStream(roomNo string, initialTemp double, finalTemp double); --Capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes @info(name='query1') from every( e1 = RoomTemperatureStream ) -> e2 = RoomTemperatureStream [e1.roomNo == roomNo and (e1.temp + 5.0) <= temp] within 2 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into RoomTemperatureAlertStream;","title":"Identifying Event Patterns Based On Order of Event Arrival"},{"location":"samples/PatternMatching/#purpose","text":"This application demonstrates how to detect patterns with Siddhi pattern concept. In the sample we capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes","title":"Purpose:"},{"location":"samples/PatternMatching/#prerequisites","text":"Save this sample","title":"Prerequisites:"},{"location":"samples/PatternMatching/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PatternMatching.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PatternMatching/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start.","title":"Notes:"},{"location":"samples/PatternMatching/#testing-the-sample","text":"Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'PatternMatching' as 'Siddhi App Name' -> Select 'RoomTemperatureStream' as 'Stream Name' -> Provide attribute values -> Send To generate an alert, send one event, followed by another event (within 2 mins) where the temperature of the second event shows an increment by 5 degrees or more (eg. Temperature of event 1 = 17.0, Temperature of event 2 = 30.0). Note that these two events may or may not be consecutive.","title":"Testing the Sample:"},{"location":"samples/PatternMatching/#viewing-the-results","text":"See the output on the console. Output will be shown if the second temperature input is incremented by 5 degrees or more.","title":"Viewing the Results:"},{"location":"samples/PatternMatching/#note","text":"Stop this Siddhi application, once you are done with the execution @App:name(\"PatternMatching\") @App:description('Identify event patterns based on the order of event arrival') define stream RoomTemperatureStream(roomNo string, temp double); @sink(type='log') define stream RoomTemperatureAlertStream(roomNo string, initialTemp double, finalTemp double); --Capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes @info(name='query1') from every( e1 = RoomTemperatureStream ) -> e2 = RoomTemperatureStream [e1.roomNo == roomNo and (e1.temp + 5.0) <= temp] within 2 min select e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp insert into RoomTemperatureAlertStream;","title":"Note:"},{"location":"samples/PmmlModelProcessor/","text":"Purpose: \u00b6 This application demonstrates how to configure a pretrained PMML model to predict required raw materials from sweet production events and view the output on the console. Prerequisites: \u00b6 Download siddhi-gpl-execution-pmml-x.x.x.jar from the following link and copy the jar to {WSO2SIHome}/lib http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/pmml/siddhi-gpl-execution-pmml/ Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PmmlModelProcessor successfully deployed. Executing the Sample: \u00b6 Replace with the SweetProductionRatePrediction.pmml file's absolute path - {WSO2SIHome}/samples/artifacts/PMMLModelProcessorSample/SweetProductionRatePrediction.pmml Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. PmmlModelProcessor.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Start the application and check whether the specified events from the jms provider appear on the console. Testing the Sample: \u00b6 Send events through one or more of the following methods. * You may send events to SweetProductionStream, via event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PmmlModelProcessor * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'candy', 20, 23 respectively and then click Send to send the event. 4. Send more events as desired. Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PmmlModelProcessor\",\"data\": ['candy', 20, 23]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PmmlModelProcessor\",\"data\": ['candy', 20, 23]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Notes: \u00b6 Accepted values for the 'name' field should be either candy, caramel-bar, peanut-butter-cup or truffle for the specific pretrained PMML model. Viewing the Results: \u00b6 See the prediction as output on the in the console. Note: \u00b6 Stop this Siddhi application, once you are done with the execution. -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with SP editor. @App:name(\"PmmlModelProcessor\") @App:description('Use a pretrained PMML model to predict required raw materials. View the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='binary')) define stream SweetProductionStream (name string, currentHourAmount double, previousHourAmount double ); @sink(type='log') define stream PredictionStream (name string, currentHourAmount double, previousHourAmount double, Predicted_nextHourAmount string); from SweetProductionStream#pmml:predict('<SweetProductionRatePrediction_model_path>') select * insert into PredictionStream;","title":"Making Predictions via PMML Model"},{"location":"samples/PmmlModelProcessor/#purpose","text":"This application demonstrates how to configure a pretrained PMML model to predict required raw materials from sweet production events and view the output on the console.","title":"Purpose:"},{"location":"samples/PmmlModelProcessor/#prerequisites","text":"Download siddhi-gpl-execution-pmml-x.x.x.jar from the following link and copy the jar to {WSO2SIHome}/lib http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/pmml/siddhi-gpl-execution-pmml/ Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PmmlModelProcessor successfully deployed.","title":"Prerequisites:"},{"location":"samples/PmmlModelProcessor/#executing-the-sample","text":"Replace with the SweetProductionRatePrediction.pmml file's absolute path - {WSO2SIHome}/samples/artifacts/PMMLModelProcessorSample/SweetProductionRatePrediction.pmml Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. PmmlModelProcessor.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PmmlModelProcessor/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Start the application and check whether the specified events from the jms provider appear on the console.","title":"Notes:"},{"location":"samples/PmmlModelProcessor/#testing-the-sample","text":"Send events through one or more of the following methods. * You may send events to SweetProductionStream, via event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PmmlModelProcessor * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'candy', 20, 23 respectively and then click Send to send the event. 4. Send more events as desired. Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PmmlModelProcessor\",\"data\": ['candy', 20, 23]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PmmlModelProcessor\",\"data\": ['candy', 20, 23]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PmmlModelProcessor/#notes_1","text":"Accepted values for the 'name' field should be either candy, caramel-bar, peanut-butter-cup or truffle for the specific pretrained PMML model.","title":"Notes:"},{"location":"samples/PmmlModelProcessor/#viewing-the-results","text":"See the prediction as output on the in the console.","title":"Viewing the Results:"},{"location":"samples/PmmlModelProcessor/#note","text":"Stop this Siddhi application, once you are done with the execution. -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with SP editor. @App:name(\"PmmlModelProcessor\") @App:description('Use a pretrained PMML model to predict required raw materials. View the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='binary')) define stream SweetProductionStream (name string, currentHourAmount double, previousHourAmount double ); @sink(type='log') define stream PredictionStream (name string, currentHourAmount double, previousHourAmount double, Predicted_nextHourAmount string); from SweetProductionStream#pmml:predict('<SweetProductionRatePrediction_model_path>') select * insert into PredictionStream;","title":"Note:"},{"location":"samples/PriorityExtensionSample/","text":"Purpose: \u00b6 This application demonstrates Siddhi events priority concept. In the sample we keep the track of events in UserWallPostStream base on their priority value. After every 1 sec, priority of every events will be reduced by 1 and the updated priority will be sent out with the last known attributes of those events. Prerequisites: \u00b6 Download siddhi-execution-priority-5.0.1.jar and copy the jar to {WSO2SIHome}/lib https://maven.wso2.org/nexus/service/local/repositories/releases/content/org/wso2/extension/siddhi/execution/priority/siddhi-execution-priority/5.0.1/siddhi-execution-priority-5.0.1.jar Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PriorityExtensionSample.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. Testing the Sample: \u00b6 You may send events to UserWallPostStream, via event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : PriorityExtensionSample * Stream Name : UserWallPostStream 3. Enter following values in the fields and send * userId: \"Mohan\", * ext: \"Hello World!\", * priority: \"1\" 4. Enter following values in the fields and send * userId: \"Nuwan\", * ext: \"Good Morning!\", * priority: \"3\" Viewing the Results: \u00b6 Messages similar to the following would be shown on the editor console. INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620904666, data=[Mohan, Hello World!, 1, Mohan, 1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620905670, data=[Mohan, Hello World!, 1, Mohan, 0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620924365, data=[Nuwan, Good Morning, 3, Nuwan, 3], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620925370, data=[Nuwan, Good Morning, 3, Nuwan, 2], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620926373, data=[Nuwan, Good Morning, 3, Nuwan, 1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620927378, data=[Nuwan, Good Morning, 3, Nuwan, 0], isExpired=false} You will notice that every 1 sec priority of the userId \"Mohan\" and \"Sajith\" reduced by 1 and it will continue until their priority reduced to 0. @APP:name('PriorityExtensionSample') @App:description('Keeps track of the priority of events in a stream.') define stream UserWallPostStream (userId string, text string, priority long); @sink(type='log') define stream PrioritizedUserWallPostStream (userId string, text string, priority long, priorityKey string, currentPriority long); from UserWallPostStream#priority:time(userId, priority, 1 sec) select * insert all events into PrioritizedUserWallPostStream;","title":"Observing the Priority of Events in a Stream"},{"location":"samples/PriorityExtensionSample/#purpose","text":"This application demonstrates Siddhi events priority concept. In the sample we keep the track of events in UserWallPostStream base on their priority value. After every 1 sec, priority of every events will be reduced by 1 and the updated priority will be sent out with the last known attributes of those events.","title":"Purpose:"},{"location":"samples/PriorityExtensionSample/#prerequisites","text":"Download siddhi-execution-priority-5.0.1.jar and copy the jar to {WSO2SIHome}/lib https://maven.wso2.org/nexus/service/local/repositories/releases/content/org/wso2/extension/siddhi/execution/priority/siddhi-execution-priority/5.0.1/siddhi-execution-priority-5.0.1.jar Save this sample","title":"Prerequisites:"},{"location":"samples/PriorityExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PriorityExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PriorityExtensionSample/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start.","title":"Notes:"},{"location":"samples/PriorityExtensionSample/#testing-the-sample","text":"You may send events to UserWallPostStream, via event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : PriorityExtensionSample * Stream Name : UserWallPostStream 3. Enter following values in the fields and send * userId: \"Mohan\", * ext: \"Hello World!\", * priority: \"1\" 4. Enter following values in the fields and send * userId: \"Nuwan\", * ext: \"Good Morning!\", * priority: \"3\"","title":"Testing the Sample:"},{"location":"samples/PriorityExtensionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the editor console. INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620904666, data=[Mohan, Hello World!, 1, Mohan, 1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620905670, data=[Mohan, Hello World!, 1, Mohan, 0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620924365, data=[Nuwan, Good Morning, 3, Nuwan, 3], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620925370, data=[Nuwan, Good Morning, 3, Nuwan, 2], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620926373, data=[Nuwan, Good Morning, 3, Nuwan, 1], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - PriorityExtensionSample : PrioritizedUserWallPostStream : Event{timestamp=1513620927378, data=[Nuwan, Good Morning, 3, Nuwan, 0], isExpired=false} You will notice that every 1 sec priority of the userId \"Mohan\" and \"Sajith\" reduced by 1 and it will continue until their priority reduced to 0. @APP:name('PriorityExtensionSample') @App:description('Keeps track of the priority of events in a stream.') define stream UserWallPostStream (userId string, text string, priority long); @sink(type='log') define stream PrioritizedUserWallPostStream (userId string, text string, priority long, priorityKey string, currentPriority long); from UserWallPostStream#priority:time(userId, priority, 1 sec) select * insert all events into PrioritizedUserWallPostStream;","title":"Viewing the Results:"},{"location":"samples/PublishEmailInTextFormat/","text":"Purpose: \u00b6 This application demonstrates how to use siddhi-io-email for publishing events to files. Prerequisites: \u00b6 Replace the Sink configuration values for following options. username: senders username (Ex:- 'example123') address: senders address (Ex:- 'example123@wso2.com') password: senders password to: receivers address (Ex:- 'example987@wso2.com') subject: subject of the email you need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PublishEmailInTextFormat.siddhi - Started Successfully!. Testing the Sample: \u00b6 Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEmailInTextFormat as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Provide attribute values name: chocolate cake amount: 10.10 Click on the start button (Arrow symbol) next to the newly created simulator Viewing the Results: \u00b6 Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert similar to the following. Subject: Test Siddhi-io-email-{{name}} Content: name:\"chocolate cake\", hourlyTotal:10.1, currentHour: @App:name(\"PublishEmailInTextFormat\") @APP:description(\"Demonstrates how to use siddhi-io-email for publishing events to files.\") define stream SweetProductionStream (name string, amount double); @sink(type='email', @map(type='text') , username ='<senders email user name>', address ='<senders email address>', password= '<senders email password>', subject='Test Siddhi-io-email-{{name}}', to='<receivers email address>', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true') define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour double); @sink(type='log') define stream EmailLogStream(name string, hourlyTotal double, currentHour double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream insert into EmailLogStream;","title":"Publishing Text Events via Email"},{"location":"samples/PublishEmailInTextFormat/#purpose","text":"This application demonstrates how to use siddhi-io-email for publishing events to files.","title":"Purpose:"},{"location":"samples/PublishEmailInTextFormat/#prerequisites","text":"Replace the Sink configuration values for following options. username: senders username (Ex:- 'example123') address: senders address (Ex:- 'example123@wso2.com') password: senders password to: receivers address (Ex:- 'example987@wso2.com') subject: subject of the email you need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link Save this sample","title":"Prerequisites:"},{"location":"samples/PublishEmailInTextFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PublishEmailInTextFormat.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/PublishEmailInTextFormat/#testing-the-sample","text":"Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEmailInTextFormat as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Provide attribute values name: chocolate cake amount: 10.10 Click on the start button (Arrow symbol) next to the newly created simulator","title":"Testing the Sample:"},{"location":"samples/PublishEmailInTextFormat/#viewing-the-results","text":"Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert similar to the following. Subject: Test Siddhi-io-email-{{name}} Content: name:\"chocolate cake\", hourlyTotal:10.1, currentHour: @App:name(\"PublishEmailInTextFormat\") @APP:description(\"Demonstrates how to use siddhi-io-email for publishing events to files.\") define stream SweetProductionStream (name string, amount double); @sink(type='email', @map(type='text') , username ='<senders email user name>', address ='<senders email address>', password= '<senders email password>', subject='Test Siddhi-io-email-{{name}}', to='<receivers email address>', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true') define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour double); @sink(type='log') define stream EmailLogStream(name string, hourlyTotal double, currentHour double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream insert into EmailLogStream;","title":"Viewing the Results:"},{"location":"samples/PublishEmailInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to use siddhi-io-email for publishing events to files. Prerequisites: \u00b6 Replace the Sink configuration values for following options. username: senders username (Ex:- 'example123') address: senders address (Ex:- 'example123@wso2.com') password: senders password to: receivers address (Ex:- 'example987@wso2.com') subject: subject of the email You need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PublishEmailInXmlFormat.siddhi - Started Successfully!. Testing the Sample: \u00b6 Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEmailInXmlFormat as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Provide attribute values name: chocolate cake amount: 10.10 Click on the start button (Arrow symbol) next to the newly created simulator Viewing the Results: \u00b6 Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert as follows. Subject: <subject of the email> Content: <events><event><name>chocolate cake</name><hourlyTotal>10.1</hourlyTotal><currentHour>0.0</currentHour></event></events> Note: \u00b6 current hour depends on the system's timestamp @App:name(\"PublishEmailInXmlFormat\") define stream SweetProductionStream (name string, amount double); @sink(type='email', @map(type='xml') , username ='<senders email user name>', address ='<senders email address>', password= '<senders email password>', subject='Test Siddhi-io-email-{{name}}', to='<receivers email address>', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true') define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour double); @sink(type='log') define stream EmailLogStream(name string, hourlyTotal double, currentHour double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream insert into EmailLogStream;","title":"Publishing Emails in XML Format"},{"location":"samples/PublishEmailInXmlFormat/#purpose","text":"This application demonstrates how to use siddhi-io-email for publishing events to files.","title":"Purpose:"},{"location":"samples/PublishEmailInXmlFormat/#prerequisites","text":"Replace the Sink configuration values for following options. username: senders username (Ex:- 'example123') address: senders address (Ex:- 'example123@wso2.com') password: senders password to: receivers address (Ex:- 'example987@wso2.com') subject: subject of the email You need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link Save this sample","title":"Prerequisites:"},{"location":"samples/PublishEmailInXmlFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console PublishEmailInXmlFormat.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/PublishEmailInXmlFormat/#testing-the-sample","text":"Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEmailInXmlFormat as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Provide attribute values name: chocolate cake amount: 10.10 Click on the start button (Arrow symbol) next to the newly created simulator","title":"Testing the Sample:"},{"location":"samples/PublishEmailInXmlFormat/#viewing-the-results","text":"Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert as follows. Subject: <subject of the email> Content: <events><event><name>chocolate cake</name><hourlyTotal>10.1</hourlyTotal><currentHour>0.0</currentHour></event></events>","title":"Viewing the Results:"},{"location":"samples/PublishEmailInXmlFormat/#note","text":"current hour depends on the system's timestamp @App:name(\"PublishEmailInXmlFormat\") define stream SweetProductionStream (name string, amount double); @sink(type='email', @map(type='xml') , username ='<senders email user name>', address ='<senders email address>', password= '<senders email password>', subject='Test Siddhi-io-email-{{name}}', to='<receivers email address>', port = '465', host = 'smtp.gmail.com', ssl.enable = 'true', auth = 'true') define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour double); @sink(type='log') define stream EmailLogStream(name string, hourlyTotal double, currentHour double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream insert into EmailLogStream;","title":"Note:"},{"location":"samples/PublishEventsToFile/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish data events in to files in Json format. Prerequisites: \u00b6 Edit the uri '{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{name}}.txt' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location. Save this sample. If there is no syntax error, the following messages would be shown on the console: Siddhi App PublishEventsToFile successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishEventsToFile.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data events to the file through the event simulator: Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEventsToFile as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Click on the start button (Arrow symbol) next to the newly created simulator Provide attribute values name: toffees amount: 66.71 Send event Provide attribute values name: toffees amount: 200 Send event Viewing the Results: \u00b6 Navigate to the path defined by file.uri ({WSO2SIHome}/samples/artifacts/0038/files/sink), where you can see a .txt file named after the event (e.g., toffees.txt) and open it. You can see the data events that you sent: {\"event\":{\"name\":\"toffees\",\"amount\":66.71}} {\"event\":{\"name\":\"toffees\",\"amount\":200.0}} @App:name(\"PublishEventsToFile\") @App:description('Publish data events processed within Siddhi to files in Json format.') define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='json'), file.uri='{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{name}}.txt') define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing JSON Events to Files"},{"location":"samples/PublishEventsToFile/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish data events in to files in Json format.","title":"Purpose:"},{"location":"samples/PublishEventsToFile/#prerequisites","text":"Edit the uri '{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{name}}.txt' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location. Save this sample. If there is no syntax error, the following messages would be shown on the console: Siddhi App PublishEventsToFile successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishEventsToFile/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishEventsToFile.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishEventsToFile/#testing-the-sample","text":"You can publish data events to the file through the event simulator: Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select PublishEventsToFile as 'Siddhi App Name' Select SweetProductionStream as 'StreamName' Click on the start button (Arrow symbol) next to the newly created simulator Provide attribute values name: toffees amount: 66.71 Send event Provide attribute values name: toffees amount: 200 Send event","title":"Testing the Sample:"},{"location":"samples/PublishEventsToFile/#viewing-the-results","text":"Navigate to the path defined by file.uri ({WSO2SIHome}/samples/artifacts/0038/files/sink), where you can see a .txt file named after the event (e.g., toffees.txt) and open it. You can see the data events that you sent: {\"event\":{\"name\":\"toffees\",\"amount\":66.71}} {\"event\":{\"name\":\"toffees\",\"amount\":200.0}} @App:name(\"PublishEventsToFile\") @App:description('Publish data events processed within Siddhi to files in Json format.') define stream SweetProductionStream (name string, amount double); @sink(type='file', @map(type='json'), file.uri='{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{name}}.txt') define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub sink in Siddhi to publish events. Events which are in TEXT format are published to a googlepubsub topic. Prerequisites: \u00b6 Create a Google Cloud Platform account. Sign in to Google Account and set up a GCP Console project and enable the API. Create a service account and download a private key as JSON. Place your json file in any system property. Save the sample. If there is no syntax error, the following message is shown on the console: Siddhi App SendGooglePubSubMessage successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SendGooglePubSubMessage.siddhi - Started Successfully! Testing the Sample: \u00b6 Send events through one or more of the following methods. You may send events to googlepubsub sink, via event simulator Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : SendGooglePubSubMessage Stream Name : FooStream In the message field, enter the following and then click Send to send the event. message: Hello Send some more events. Viewing the Results: \u00b6 See the output on the terminal: 2019-03-14_12-50-21_966] INFO {io.siddhi.core.stream.output.sink.LogSink} - SendEvent : BarStream : Event{timestamp=1552548021825, data=[Hello], isExpired=false} Notes: \u00b6 Make sure the the credential file is correct and user have write access to make api calls. Stop this Siddhi application. @App:name(\"SendGooglePubSubMessage\") @App:description('Send events to a Google Pub/Sub Topic') @sink(type='googlepubsub', topic.id = 'topic75', credential.path = '/../sp.json', project.id = 'sp-path-1547649404768', @map(type='text')) define stream FooStream (message string); @sink(type = 'log') define stream BarStream(message string); @info(name = 'query1') from FooStream select message insert into BarStream;","title":"Publishing Events to a Google Pub/Sub Topic"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub sink in Siddhi to publish events. Events which are in TEXT format are published to a googlepubsub topic.","title":"Purpose:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#prerequisites","text":"Create a Google Cloud Platform account. Sign in to Google Account and set up a GCP Console project and enable the API. Create a service account and download a private key as JSON. Place your json file in any system property. Save the sample. If there is no syntax error, the following message is shown on the console: Siddhi App SendGooglePubSubMessage successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SendGooglePubSubMessage.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#testing-the-sample","text":"Send events through one or more of the following methods. You may send events to googlepubsub sink, via event simulator Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : SendGooglePubSubMessage Stream Name : FooStream In the message field, enter the following and then click Send to send the event. message: Hello Send some more events.","title":"Testing the Sample:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#viewing-the-results","text":"See the output on the terminal: 2019-03-14_12-50-21_966] INFO {io.siddhi.core.stream.output.sink.LogSink} - SendEvent : BarStream : Event{timestamp=1552548021825, data=[Hello], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#notes","text":"Make sure the the credential file is correct and user have write access to make api calls. Stop this Siddhi application. @App:name(\"SendGooglePubSubMessage\") @App:description('Send events to a Google Pub/Sub Topic') @sink(type='googlepubsub', topic.id = 'topic75', credential.path = '/../sp.json', project.id = 'sp-path-1547649404768', @map(type='text')) define stream FooStream (message string); @sink(type = 'log') define stream BarStream(message string); @info(name = 'query1') from FooStream select message insert into BarStream;","title":"Notes:"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/","text":"Sending Custom JSON Events via HTTP \u00b6 Purpose \u00b6 This application demonstrates how to configure the Streaming Integrator to send sweet production events via HTTP transport in JSON format using custom mapping. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed. Executing the Sample \u00b6 Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following messages are shown on the console. PublishHTTPInJsonFormatWithCustomMapping.siddhi - Started Successfully! 'http' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc Testing the Sample \u00b6 Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. To send events, follow one or more of the following methods. Send events with http server through the event simulator: a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. b. In the Single Simulation tab of the panel, specify the values as follows. - Siddhi App Name`: `PublishHttpInJsonFormatWithCustomMapping - Stream Name`: `SweetProductionStream c. In the id and amount fields, enter toffees and 123.5 respectively and then click Send to send the event. d. Send more events as required. Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command: a. Open a new terminal and issue the following command curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 123.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' b. If there is no error, the following message appears in the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: a. Install the 'Postman' application from Chrome web store. b. Launch the application. c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows. {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 75.6]} d. Click Send . If there is no error, the following messages appear in the terminal. * \"status\": \"OK\" * \"message\": \"Single Event simulation started successfully\" Viewing the Results \u00b6 See the output on the terminal: [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"id\":\"toffees\",\"amount\":123.5}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"id\":\"toffees\",\"amount\":123.5}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [43]] Note If the message \"'HTTP' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc\" does not appear, it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change the port from 8080 to an unused port. 3. Start the application and check whether the specified messages appear on the console. Click here to view the complete sample Siddhi application @App:name(\"PublishHTTPInJsonFormatWithCustomMapping\") @App:description('Send events via HTTP transport in JSON format with custom mapping.') define stream SweetProductionStream (id string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='json' , @payload( \"{'name': {{id}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream (id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Sending Custom JSON Events via HTTP"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#sending-custom-json-events-via-http","text":"","title":"Sending Custom JSON Events via HTTP"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure the Streaming Integrator to send sweet production events via HTTP transport in JSON format using custom mapping. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.","title":"Purpose"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#executing-the-sample","text":"Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following messages are shown on the console. PublishHTTPInJsonFormatWithCustomMapping.siddhi - Started Successfully! 'http' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc","title":"Executing the Sample"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#testing-the-sample","text":"Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. To send events, follow one or more of the following methods. Send events with http server through the event simulator: a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. b. In the Single Simulation tab of the panel, specify the values as follows. - Siddhi App Name`: `PublishHttpInJsonFormatWithCustomMapping - Stream Name`: `SweetProductionStream c. In the id and amount fields, enter toffees and 123.5 respectively and then click Send to send the event. d. Send more events as required. Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command: a. Open a new terminal and issue the following command curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 123.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' b. If there is no error, the following message appears in the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: a. Install the 'Postman' application from Chrome web store. b. Launch the application. c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows. {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 75.6]} d. Click Send . If there is no error, the following messages appear in the terminal. * \"status\": \"OK\" * \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#viewing-the-results","text":"See the output on the terminal: [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"id\":\"toffees\",\"amount\":123.5}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"id\":\"toffees\",\"amount\":123.5}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [43]] Note If the message \"'HTTP' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc\" does not appear, it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change the port from 8080 to an unused port. 3. Start the application and check whether the specified messages appear on the console. Click here to view the complete sample Siddhi application @App:name(\"PublishHTTPInJsonFormatWithCustomMapping\") @App:description('Send events via HTTP transport in JSON format with custom mapping.') define stream SweetProductionStream (id string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='json' , @payload( \"{'name': {{id}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream (id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results"},{"location":"samples/PublishHl7InER7Format/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in ER7 format via MLLP protocol and log the events in hl7Stream and the acknowledgement message to the output console. Prerequisites: \u00b6 Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHl7InER7Format successfully deployed. Executing the Sample: \u00b6 In the HAPI testpanel create a receiving connection with port that provided in the siddhi app. Start the listener. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHl7InER7Format.siddhi - Started Successfully! 'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'. Executing HL7Sender: HOST: localhost, PORT: 4000 for stream PublishHl7InER7Format:hl7Stream. Testing the Sample: \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishHl7InER7Format Stream Name : er7Stream In the payload, enter 'MSH| ~\\&|||||20190211145413.131+0530||ADT A01|10601|T|2.3' and then click Send to send the event. Send more events as desired. @App:name('PublishHl7InER7Format') @App:description('This publishes the HL7 messages in ER7 format, receives and logs the acknowledgement message in the console using MLLP protocol and custom text mapping.') define stream er7Stream (payload string); @sink(type = 'hl7', uri='localhost:4000', tls.enabled='false', hl7.encoding='er7', @map(type='text', @payload('{{payload}}'))) define stream hl7Stream (payload String); @info(name='query1') from er7Stream select payload insert into hl7Stream;","title":"Publishing ER7 Events via HL7"},{"location":"samples/PublishHl7InER7Format/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in ER7 format via MLLP protocol and log the events in hl7Stream and the acknowledgement message to the output console.","title":"Purpose:"},{"location":"samples/PublishHl7InER7Format/#prerequisites","text":"Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHl7InER7Format successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHl7InER7Format/#executing-the-sample","text":"In the HAPI testpanel create a receiving connection with port that provided in the siddhi app. Start the listener. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHl7InER7Format.siddhi - Started Successfully! 'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'. Executing HL7Sender: HOST: localhost, PORT: 4000 for stream PublishHl7InER7Format:hl7Stream.","title":"Executing the Sample:"},{"location":"samples/PublishHl7InER7Format/#testing-the-sample","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishHl7InER7Format Stream Name : er7Stream In the payload, enter 'MSH| ~\\&|||||20190211145413.131+0530||ADT A01|10601|T|2.3' and then click Send to send the event. Send more events as desired. @App:name('PublishHl7InER7Format') @App:description('This publishes the HL7 messages in ER7 format, receives and logs the acknowledgement message in the console using MLLP protocol and custom text mapping.') define stream er7Stream (payload string); @sink(type = 'hl7', uri='localhost:4000', tls.enabled='false', hl7.encoding='er7', @map(type='text', @payload('{{payload}}'))) define stream hl7Stream (payload String); @info(name='query1') from er7Stream select payload insert into hl7Stream;","title":"Testing the Sample:"},{"location":"samples/PublishHl7InXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in XML format via MLLP protocol and log the events in hl7Stream and the acknowledgment message to the output console. Prerequisites: \u00b6 Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHl7InXmlFormat successfully deployed. Executing the Sample: \u00b6 In the HAPI testpanel create a receiving connection with port that provided in the siddhi app. Start the listener. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHl7InXmlFormat.siddhi - Started Successfully! 'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'. Testing the Sample: \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specifiy the values as follows: Siddhi App Name : PublishHl7InXmlFormat Stream Name : xmlStream In the MSH1, MSH2, MSH3HD1,MSH4HD1, MSH5HD1, MSH6HD1, MSH7, MSH8, CM_MSG1, CM_MSG2, MSH10, MSH11, MSH12 fields enter '|', '^~\\&', 'sendingSystemA', 'senderFacilityA', 'receivingSystemB' , 'receivingFacilityB', '20080925161613', ' ', 'ADT', 'A01', 'S123456789', 'P', '2.3' respectively and then click Send to send the event. Send more events as desired. @App:name('PublishHl7InXmlFormat') @App:description('This publishes the HL7 messages in XML format, receives and logs the acknowledgement message in the console using MLLP protocol and custom xml mapping.') define stream xmlStream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string); @sink(type = 'hl7', uri = 'localhost:4000', hl7.encoding = 'xml', @map(type = 'xml', enclosing.element=\"<ADT_A01 xmlns='urn:hl7-org:v2xml'>\", @payload('<MSH><MSH.1>{{MSH1}}</MSH.1><MSH.2>{{MSH2}}</MSH.2><MSH.3><HD.1>{{MSH3HD1}}</HD.1></MSH.3><MSH.4><HD.1>{{MSH4HD1}}</HD.1></MSH.4><MSH.5><HD.1>{{MSH5HD1}}</HD.1></MSH.5><MSH.6><HD.1>{{MSH6HD1}}</HD.1></MSH.6><MSH.7>{{MSH7}}</MSH.7><MSH.8>{{MSH8}}</MSH.8><MSH.9><CM_MSG.1>{{CM_MSG1}}</CM_MSG.1><CM_MSG.2>{{CM_MSG2}}</CM_MSG.2></MSH.9><MSH.10>{{MSH10}}</MSH.10><MSH.11>{{MSH11}}</MSH.11><MSH.12>{{MSH12}}</MSH.12></MSH>'))) define stream hl7Stream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string); @info(name='query1') from xmlStream select * insert into hl7Stream;","title":"Publishing XML messages via HL7"},{"location":"samples/PublishHl7InXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in XML format via MLLP protocol and log the events in hl7Stream and the acknowledgment message to the output console.","title":"Purpose:"},{"location":"samples/PublishHl7InXmlFormat/#prerequisites","text":"Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHl7InXmlFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHl7InXmlFormat/#executing-the-sample","text":"In the HAPI testpanel create a receiving connection with port that provided in the siddhi app. Start the listener. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHl7InXmlFormat.siddhi - Started Successfully! 'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'.","title":"Executing the Sample:"},{"location":"samples/PublishHl7InXmlFormat/#testing-the-sample","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specifiy the values as follows: Siddhi App Name : PublishHl7InXmlFormat Stream Name : xmlStream In the MSH1, MSH2, MSH3HD1,MSH4HD1, MSH5HD1, MSH6HD1, MSH7, MSH8, CM_MSG1, CM_MSG2, MSH10, MSH11, MSH12 fields enter '|', '^~\\&', 'sendingSystemA', 'senderFacilityA', 'receivingSystemB' , 'receivingFacilityB', '20080925161613', ' ', 'ADT', 'A01', 'S123456789', 'P', '2.3' respectively and then click Send to send the event. Send more events as desired. @App:name('PublishHl7InXmlFormat') @App:description('This publishes the HL7 messages in XML format, receives and logs the acknowledgement message in the console using MLLP protocol and custom xml mapping.') define stream xmlStream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string); @sink(type = 'hl7', uri = 'localhost:4000', hl7.encoding = 'xml', @map(type = 'xml', enclosing.element=\"<ADT_A01 xmlns='urn:hl7-org:v2xml'>\", @payload('<MSH><MSH.1>{{MSH1}}</MSH.1><MSH.2>{{MSH2}}</MSH.2><MSH.3><HD.1>{{MSH3HD1}}</HD.1></MSH.3><MSH.4><HD.1>{{MSH4HD1}}</HD.1></MSH.4><MSH.5><HD.1>{{MSH5HD1}}</HD.1></MSH.5><MSH.6><HD.1>{{MSH6HD1}}</HD.1></MSH.6><MSH.7>{{MSH7}}</MSH.7><MSH.8>{{MSH8}}</MSH.8><MSH.9><CM_MSG.1>{{CM_MSG1}}</CM_MSG.1><CM_MSG.2>{{CM_MSG2}}</CM_MSG.2></MSH.9><MSH.10>{{MSH10}}</MSH.10><MSH.11>{{MSH11}}</MSH.11><MSH.12>{{MSH12}}</MSH.12></MSH>'))) define stream hl7Stream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string); @info(name='query1') from xmlStream select * insert into hl7Stream;","title":"Testing the Sample:"},{"location":"samples/PublishHttpInJsonFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpInJsonFormat successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpInJsonFormat.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'. Testing the Sample: \u00b6 Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run the \"ant\" command without any arguments. Send events through one or more of the following methods: Send events with http server through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishHttpInJsonFormat Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInJsonFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInJsonFormat\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Events: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set: [Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set: [[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"PublishHttpInJsonFormat\") @App:description('Send events via HTTP transport using JSON format') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing JSON Events via HTTP"},{"location":"samples/PublishHttpInJsonFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/PublishHttpInJsonFormat/#prerequisites","text":"Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpInJsonFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpInJsonFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpInJsonFormat.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpInJsonFormat/#testing-the-sample","text":"Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run the \"ant\" command without any arguments. Send events through one or more of the following methods: Send events with http server through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishHttpInJsonFormat Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInJsonFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInJsonFormat\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpInJsonFormat/#viewing-the-results","text":"See the output on the terminal: [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Events: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set: [Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set: [[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpInJsonFormat/#notes","text":"If you get the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"PublishHttpInJsonFormat\") @App:description('Send events via HTTP transport using JSON format') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishHttpInXmlFormat/","text":"Publishing XML Events via HTTP \u00b6 Purpose \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in XML default format and log the events in LowProductionAlertStream to the output console. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed. Executing the Sample \u00b6 Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following messages are shown on the console. PublishHttpInXmlFormat.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'. Testing the Sample \u00b6 Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. Send events using one or more of the following methods: Send events with http server through the event simulator: a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. b. In the Single Simulation tab of the panel, specify the values as follows: - Siddhi App Name: PublishHttpInXmlFormat - Stream Name: SweetProductionStream c. In the name and amount fields, enter toffees and 75.6 respectively and then click Send to send the event. d. Send more events as required. Send events to the HTTP endpoint defined by publish.url in the Sink configuration using the curl command: a. Open a new terminal and issue the following command. curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' b. If there is no error, the following message appears in the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: a. Install the 'Postman' application from Chrome web store. b. Launch the application. c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to text/plain and set the request body in text as follows. {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]} d. Click Send . If there is no error, the following messages appear in the console. - \"status\": \"OK\", - \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal. Note If the message LowProductionAlertStream stream could not connect to localhost:8080 , it could be because port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears in the console. Click here to view the complete sample Siddhi application @App:name(\"PublishHttpInXmlFormat\") @App:description('Send events via HTTP transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing XML Events via HTTP"},{"location":"samples/PublishHttpInXmlFormat/#publishing-xml-events-via-http","text":"","title":"Publishing XML Events via HTTP"},{"location":"samples/PublishHttpInXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in XML default format and log the events in LowProductionAlertStream to the output console. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.","title":"Purpose"},{"location":"samples/PublishHttpInXmlFormat/#executing-the-sample","text":"Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following messages are shown on the console. PublishHttpInXmlFormat.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.","title":"Executing the Sample"},{"location":"samples/PublishHttpInXmlFormat/#testing-the-sample","text":"Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. Send events using one or more of the following methods: Send events with http server through the event simulator: a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. b. In the Single Simulation tab of the panel, specify the values as follows: - Siddhi App Name: PublishHttpInXmlFormat - Stream Name: SweetProductionStream c. In the name and amount fields, enter toffees and 75.6 respectively and then click Send to send the event. d. Send more events as required. Send events to the HTTP endpoint defined by publish.url in the Sink configuration using the curl command: a. Open a new terminal and issue the following command. curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' b. If there is no error, the following message appears in the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: a. Install the 'Postman' application from Chrome web store. b. Launch the application. c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to text/plain and set the request body in text as follows. {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]} d. Click Send . If there is no error, the following messages appear in the console. - \"status\": \"OK\", - \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample"},{"location":"samples/PublishHttpInXmlFormat/#viewing-the-results","text":"See the output on the terminal. Note If the message LowProductionAlertStream stream could not connect to localhost:8080 , it could be because port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears in the console. Click here to view the complete sample Siddhi application @App:name(\"PublishHttpInXmlFormat\") @App:description('Send events via HTTP transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/","text":"Publishing Custom XML Events via HTTP \u00b6 Purpose \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to send sweet production events via HTTP transport in XML format using custom mapping. Map the input events($.item.id) to stream events(name) and log the events in the LowProducitonAlertStream stream on the output console. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed. Executing the Sample \u00b6 Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully! Note If you edit this application while it's running, save the application and then click Run => Run . If the PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully! message does not appear, it could be due to port 8080 which is defined in the Siddhi application already being used by a different program. To resolve this issue, do the following: * Stop this Siddhi application (click 'Run' on the menu bar -> 'Stop').. * Change the port 8080 to an unused port in this Siddhi application's source configuration.. * Start the application and check whether the specified messages appear on the console. Testing the Sample \u00b6 Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. Send events using one or more of the following methods: Send events with the HTTP server through the event simulator: a. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. b. In the Single Simulation tab of the panel, select values as follows: - Siddhi App Name : PublishHttpInXmlFormatWithCustomMapping - Stream Name : SweetProductionStream c. In the name field and amount fields, enter toffee and 50.0 respectively. Then click Send to send the event. d. Send more events. **Send events to the HTTP endpoint defined via the publish.url in the Sink configuration by issuing the following CURL command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormatWithCustomMapping\",\"data\": ['toffee', 67.43]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following message appears in the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Viewing the Results \u00b6 If you send events through the event simulator, the following output is logged. [java] [org.wso2.si.http.server.HttpServerListener]: Event Name Arrived: <events><event><name>toffee</name><amount>50.0</amount></event></events> [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Names:<events><event><name>toffee</name><amount>50.0</amount></event></events> , [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers key set:[Http_method, Transfer-encoding, Content-type] [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers value set:[[POST], [chunked], [application/xml]] If you send events through event CURL commands, the following output is logged: [java] [org.wso2.si.http.server.HttpServerListener]: Event Name Arrived: <events><event><name>toffee</name><amount>67.43</amount></event></events> [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Names:<events><event><name>toffee</name><amount>67.43</amount></event></events> , [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers key set:[Http_method, Transfer-encoding, Content-type] [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers value set:[[POST], [chunked], [application/xml]] Note Stop this Siddhi application once you are done with the execution. Click here to view the complete sample Siddhi application @App:name(\"PublishHttpInXmlFormatWithCustomMapping\") @App:description('Send events via HTTP transport using xml formatwith custom mapping') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='xml', @payload( \"<StockData><Symbol>{{name}}</Symbol><Price>{{amount}}</Price></StockData>\"))) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Custom XML Events via HTTP"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#publishing-custom-xml-events-via-http","text":"","title":"Publishing Custom XML Events via HTTP"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to send sweet production events via HTTP transport in XML format using custom mapping. Map the input events($.item.id) to stream events(name) and log the events in the LowProducitonAlertStream stream on the output console. Before executing the sample: Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. - Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.","title":"Purpose"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#executing-the-sample","text":"Start the Siddhi application by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully! Note If you edit this application while it's running, save the application and then click Run => Run . If the PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully! message does not appear, it could be due to port 8080 which is defined in the Siddhi application already being used by a different program. To resolve this issue, do the following: * Stop this Siddhi application (click 'Run' on the menu bar -> 'Stop').. * Change the port 8080 to an unused port in this Siddhi application's source configuration.. * Start the application and check whether the specified messages appear on the console.","title":"Executing the Sample"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#testing-the-sample","text":"Open a terminal and navigate to <SI_HOME>/samples/sample-clients/http-server . Then run the ant command without any arguments. Send events using one or more of the following methods: Send events with the HTTP server through the event simulator: a. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. b. In the Single Simulation tab of the panel, select values as follows: - Siddhi App Name : PublishHttpInXmlFormatWithCustomMapping - Stream Name : SweetProductionStream c. In the name field and amount fields, enter toffee and 50.0 respectively. Then click Send to send the event. d. Send more events. **Send events to the HTTP endpoint defined via the publish.url in the Sink configuration by issuing the following CURL command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormatWithCustomMapping\",\"data\": ['toffee', 67.43]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following message appears in the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Testing the Sample"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#viewing-the-results","text":"If you send events through the event simulator, the following output is logged. [java] [org.wso2.si.http.server.HttpServerListener]: Event Name Arrived: <events><event><name>toffee</name><amount>50.0</amount></event></events> [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Names:<events><event><name>toffee</name><amount>50.0</amount></event></events> , [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers key set:[Http_method, Transfer-encoding, Content-type] [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers value set:[[POST], [chunked], [application/xml]] If you send events through event CURL commands, the following output is logged: [java] [org.wso2.si.http.server.HttpServerListener]: Event Name Arrived: <events><event><name>toffee</name><amount>67.43</amount></event></events> [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Names:<events><event><name>toffee</name><amount>67.43</amount></event></events> , [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers key set:[Http_method, Transfer-encoding, Content-type] [java] [org.wso2.si.http.server.HttpServerMain]: Received Event Headers value set:[[POST], [chunked], [application/xml]] Note Stop this Siddhi application once you are done with the execution. Click here to view the complete sample Siddhi application @App:name(\"PublishHttpInXmlFormatWithCustomMapping\") @App:description('Send events via HTTP transport using xml formatwith custom mapping') define stream SweetProductionStream (name string, amount double); @sink(type='http', publisher.url='http://localhost:8080/abc', @map(type='xml', @payload( \"<StockData><Symbol>{{name}}</Symbol><Price>{{amount}}</Price></StockData>\"))) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results"},{"location":"samples/PublishHttpOAuthRequest/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Prerequisites: \u00b6 Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the API endpoint request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequest successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequest.siddhi - Started Successfully! 'Http' sink at LowProductionAlertStream stream successfully connected to 'https://localhost:8005/abc'. Testing the Sample: \u00b6 Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequest * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequest\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequest\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequest\") @App:description(\"Send HTTP events to an OAuth-protected endpoint\") define stream SweetProductionStream (name string, amount double); @sink(type='http', method=\"xxxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token',@map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing HTTP Events to to an OAuth-protected Endpoint"},{"location":"samples/PublishHttpOAuthRequest/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint.","title":"Purpose:"},{"location":"samples/PublishHttpOAuthRequest/#prerequisites","text":"Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the API endpoint request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequest successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpOAuthRequest/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequest.siddhi - Started Successfully! 'Http' sink at LowProductionAlertStream stream successfully connected to 'https://localhost:8005/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpOAuthRequest/#testing-the-sample","text":"Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequest * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequest\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequest\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpOAuthRequest/#viewing-the-results","text":"See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpOAuthRequest/#notes","text":"If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequest\") @App:description(\"Send HTTP events to an OAuth-protected endpoint\") define stream SweetProductionStream (name string, amount double); @sink(type='http', method=\"xxxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token',@map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishHttpOAuthRequestResponse/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to sending an HTTP event via an OAuth protected endpoint and catching its response. Prerequisites: \u00b6 Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:-'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values ) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestResponse successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestResponse.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'. Testing the Sample: \u00b6 Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestResponse * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestResponse\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestResponse\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestResponse\") @App:description(\"sending an HTTP event via an OAuth protected endpoint and catching its response.\") define stream SweetProductionStream (name string, amount double); @sink(type='http-request', sink.id='abc', publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxxxxxx'\", method=\"xxxxx\", consumer.key=\"xxxxxx\", consumer.secret=\"xxxxxxx\",token.url='https://localhost:8005/token', @map(type='json'), @payload( \"{'name': {{name}}, 'amount': {{amount}}}\")) define stream LowProductionAlertStream (name String, amount double); @source(type='http-response' , sink.id='abc', http.status.code='200', @map(type='text', regex.A='((.| )*)', @attributes(message='A[1]'))) define stream ResponseLowProductionAlertStream(message string); @sink(type='log') define stream loggerStream(message string); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream; @info(name='query2') from ResponseLowProductionAlertStream select * insert into loggerStream;","title":"Publishing HTTP Events via an OAuth-protected Endpoint"},{"location":"samples/PublishHttpOAuthRequestResponse/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to sending an HTTP event via an OAuth protected endpoint and catching its response.","title":"Purpose:"},{"location":"samples/PublishHttpOAuthRequestResponse/#prerequisites","text":"Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:-'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values ) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestResponse successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpOAuthRequestResponse/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestResponse.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpOAuthRequestResponse/#testing-the-sample","text":"Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestResponse * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestResponse\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestResponse\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpOAuthRequestResponse/#viewing-the-results","text":"See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpOAuthRequestResponse/#notes","text":"If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestResponse\") @App:description(\"sending an HTTP event via an OAuth protected endpoint and catching its response.\") define stream SweetProductionStream (name string, amount double); @sink(type='http-request', sink.id='abc', publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxxxxxx'\", method=\"xxxxx\", consumer.key=\"xxxxxx\", consumer.secret=\"xxxxxxx\",token.url='https://localhost:8005/token', @map(type='json'), @payload( \"{'name': {{name}}, 'amount': {{amount}}}\")) define stream LowProductionAlertStream (name String, amount double); @source(type='http-response' , sink.id='abc', http.status.code='200', @map(type='text', regex.A='((.| )*)', @attributes(message='A[1]'))) define stream ResponseLowProductionAlertStream(message string); @sink(type='log') define stream loggerStream(message string); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream; @info(name='query2') from ResponseLowProductionAlertStream select * insert into loggerStream;","title":"Notes:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use password grant type. Prerequisites: \u00b6 Replace the Sink configuration values for following options. oauth.username: sender's oauth username (Ex:- 'username123') oauth.password : sender's oauth password (Ex:- 'password123') publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithOAuthUser successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithOAuthUser.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'. Testing the Sample: \u00b6 Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithOAuthUser * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithOAuthUser\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithOAuthUser\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithOAuthUser\") @App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use password grant type.\") define stream SweetProductionStream (name string, amount double); @sink(type='http', oauth.username=\"xxxx\", oauth.password=\"xxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', method=\"xxxxxx\", @map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream ( name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing HTTP Events to an OAuth-protected Endpoint"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use password grant type.","title":"Purpose:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#prerequisites","text":"Replace the Sink configuration values for following options. oauth.username: sender's oauth username (Ex:- 'username123') oauth.password : sender's oauth password (Ex:- 'password123') publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithOAuthUser successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithOAuthUser.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#testing-the-sample","text":"Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithOAuthUser * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithOAuthUser\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithOAuthUser\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#viewing-the-results","text":"See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#notes","text":"If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithOAuthUser\") @App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use password grant type.\") define stream SweetProductionStream (name string, amount double); @sink(type='http', oauth.username=\"xxxx\", oauth.password=\"xxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', method=\"xxxxxx\", @map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream ( name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type. Prerequisites: \u00b6 Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') refresh.token : Refresh token for the http request (Ex:- 'abcdef') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithRefreshToken successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithRefreshToken.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'. Testing the Sample: \u00b6 Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithRefreshToken * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithRefreshToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithRefreshToken\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithRefreshToken\") @App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type.\") define stream SweetProductionStream (name string, amount double); @sink(type='http', method=\"xxxxx\", refresh.token=\"refreshToken\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', @map(type='json'), @payload( \"{'name': {{name}}, 'amount': {{amount}}}\")) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing HTTP Events to an OAuth-protected Endpoint while Using a Refresh Token Grant Type"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type.","title":"Purpose:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#prerequisites","text":"Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') refresh.token : Refresh token for the http request (Ex:- 'abcdef') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithRefreshToken successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithRefreshToken.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#testing-the-sample","text":"Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithRefreshToken * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithRefreshToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithRefreshToken\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#viewing-the-results","text":"See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#notes","text":"If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithRefreshToken\") @App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type.\") define stream SweetProductionStream (name string, amount double); @sink(type='http', method=\"xxxxx\", refresh.token=\"refreshToken\", publisher.url='https://localhost:8005/abc', headers=\"'Authorization: Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', @map(type='json'), @payload( \"{'name': {{name}}, 'amount': {{amount}}}\")) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send a HTTP events to an OAuth-protected endpoint without access token Prerequisites: \u00b6 Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithoutAccessToken successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithoutAccessToken.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'. Testing the Sample: \u00b6 Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithoutAccessToken * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithoutAccessToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithoutAccessToken\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]] Notes: \u00b6 If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithoutAccessToken\") @App:description(\"Send a HTTP events to an OAuth-protected endpoint without access token\") define stream SweetProductionStream (name string, amount double); @sink(type='http',method=\"xxxxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Content-Type: xxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', @map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream ( name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing HTTP Events to to an OAuth-protected Endpoint without an Access Token"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send a HTTP events to an OAuth-protected endpoint without access token","title":"Purpose:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#prerequisites","text":"Replace the Sink configuration values for following options. publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc') consumer.key : consumer key for the http request (Ex:- 'abcdef') consumer.secret: consumer secret for the http request (Ex:- 'abcdef') token.url : URL of the token end point (Ex:- 'https://localhost:8005/token') method : method type (Eg:- POST) optional (You can fill this if it is different from default values) - header (Authorization header) : access token for the http request (Ex:- 'abcdef') <= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') <= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password : API trust store password (wso2carbon) <= by default it set as wso2carbon Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishHttpOAuthRequestWithoutAccessToken successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishHttpOAuthRequestWithoutAccessToken.siddhi - Started Successfully! 'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.","title":"Executing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#testing-the-sample","text":"Send events through one or more of the following methods: * Send events with http server through the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishHttpOAuthRequestWithoutAccessToken * Stream Name : SweetProductionStream 3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event. 4. Send some more events as desired. Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command: Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithoutAccessToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: Install the 'Postman' application from the Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithoutAccessToken\",\"data\": ['toffees', 75.6]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#viewing-the-results","text":"See the output on the terminal: Siddhi App test successfully deployed. INFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc [java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} , [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length] [java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]","title":"Viewing the Results:"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#notes","text":"If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application and check whether the expected output appears on the console. If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console. If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Start the application again and check whether the expected output appears on the console. @App:name(\"PublishHttpOAuthRequestWithoutAccessToken\") @App:description(\"Send a HTTP events to an OAuth-protected endpoint without access token\") define stream SweetProductionStream (name string, amount double); @sink(type='http',method=\"xxxxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Content-Type: xxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token', @map(type='json', @payload( \"{'name': {{name}}, 'amount': {{amount}}}\"))) define stream LowProductionAlertStream ( name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishJmsInKeyvalueFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Keyvalue format. Prerequisites: \u00b6 Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip) ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishJmsInKeyvalueFormat successfully deployed. Note: \u00b6 To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: - For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> - For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal. - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib . Executing the Sample: \u00b6 Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishJmsInKeyvalueFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/jms-consumer and run the following comman. ant -Dtype='keyvalue' Send events through one or more of the following methods. Option 1: Send events to jms sink, via event simulator \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: PublishJmsInKeyvalueFormat Stream Name: SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. name: chocolate cake amount: 50.50 Send some more events. Option 2: Publish events with Curl command to the simulator http endpoint \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3: Publish events with Postman to the simulator http endpoint \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": ['chocolate cake', 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal of {WSO2SIHome}/samples/sample-clients/jms-consumer : [java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false} @App:name(\"PublishJmsInKeyvalueFormat\") @App:description('Send events via JMS transport using Keyvalue format') define stream SweetProductionStream (name string, amount double); @sink(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='keyvalue')) define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Key-value events via JMS"},{"location":"samples/PublishJmsInKeyvalueFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Keyvalue format.","title":"Purpose:"},{"location":"samples/PublishJmsInKeyvalueFormat/#prerequisites","text":"Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip) ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishJmsInKeyvalueFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishJmsInKeyvalueFormat/#note","text":"To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: - For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> - For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal. - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib .","title":"Note:"},{"location":"samples/PublishJmsInKeyvalueFormat/#executing-the-sample","text":"Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishJmsInKeyvalueFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishJmsInKeyvalueFormat/#testing-the-sample","text":"Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/jms-consumer and run the following comman. ant -Dtype='keyvalue' Send events through one or more of the following methods.","title":"Testing the Sample:"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-1-send-events-to-jms-sink-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: PublishJmsInKeyvalueFormat Stream Name: SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. name: chocolate cake amount: 50.50 Send some more events.","title":"Option 1: Send events to jms sink, via event simulator"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-2-publish-events-with-curl-command-to-the-simulator-http-endpoint","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Option 2: Publish events with Curl command to the simulator http endpoint"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": ['chocolate cake', 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Option 3: Publish events with Postman to the simulator http endpoint"},{"location":"samples/PublishJmsInKeyvalueFormat/#viewing-the-results","text":"See the output on the terminal of {WSO2SIHome}/samples/sample-clients/jms-consumer : [java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false} @App:name(\"PublishJmsInKeyvalueFormat\") @App:description('Send events via JMS transport using Keyvalue format') define stream SweetProductionStream (name string, amount double); @sink(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='keyvalue')) define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/PublishJmsInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Xml format. Prerequisites: \u00b6 Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip). ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishJmsInXmlFormat successfully deployed. Note: \u00b6 To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: - For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> - For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib . Executing the Sample: \u00b6 Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishJmsInXmlFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/jms-consumer and run ant command without arguments. Send events through one or more of the following methods. Option 1: Send events to jms sink, via event simulator \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishJmsInXmlFormat Stream Name : SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. name: chocolate cake amount: 50.50 Send some more events. Option 2: Publish events with Curl command to the simulator http endpoint \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3: Publish events with Postman to the simulator http endpoint \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": ['chocolate cake', 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal of {WSO2SIHome}/samples/sample-clients/jms-consumer : [java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false} @App:name(\"PublishJmsInXmlFormat\") @App:description('Send events via JMS transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='xml')) define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing XML Events via JMS"},{"location":"samples/PublishJmsInXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Xml format.","title":"Purpose:"},{"location":"samples/PublishJmsInXmlFormat/#prerequisites","text":"Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip). ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishJmsInXmlFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishJmsInXmlFormat/#note","text":"To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: - For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> - For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib .","title":"Note:"},{"location":"samples/PublishJmsInXmlFormat/#executing-the-sample","text":"Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishJmsInXmlFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishJmsInXmlFormat/#testing-the-sample","text":"Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/jms-consumer and run ant command without arguments. Send events through one or more of the following methods.","title":"Testing the Sample:"},{"location":"samples/PublishJmsInXmlFormat/#option-1-send-events-to-jms-sink-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishJmsInXmlFormat Stream Name : SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. name: chocolate cake amount: 50.50 Send some more events.","title":"Option 1: Send events to jms sink, via event simulator"},{"location":"samples/PublishJmsInXmlFormat/#option-2-publish-events-with-curl-command-to-the-simulator-http-endpoint","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Option 2: Publish events with Curl command to the simulator http endpoint"},{"location":"samples/PublishJmsInXmlFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": ['chocolate cake', 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Option 3: Publish events with Postman to the simulator http endpoint"},{"location":"samples/PublishJmsInXmlFormat/#viewing-the-results","text":"See the output on the terminal of {WSO2SIHome}/samples/sample-clients/jms-consumer : [java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false} @App:name(\"PublishJmsInXmlFormat\") @App:description('Send events via JMS transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='xml')) define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Avro format Using confluent Schema Registry. Prerequisites: \u00b6 Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi. Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11_2.2.0.jar kafka_clients_2.2.0.jar metrics-core-2.2.0.jar scala_library_2.11.12.jar zkclient_0.11.jar zookeeper_3.4.13.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh For Windows: ./jartobundle.bat If converted successfully, the following messages are shown on the terminal for each lib file: INFO: Created the OSGi bundle .jar for JAR file /kafka/ .jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib. Download confluent-5.2.1 from https://www.confluent.io/download/ and unzip. Save this sample. If there is no syntax error, the following message is shown on the console: * -Siddhi App PublishKafkaInAvroFormatUsingSchemaRegistry successfully deployed. Executing the Sample: \u00b6 Navigate to {KafkaHome} and start the zookeeper node using, bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using, bin/kafka-server-start.sh config/server.properties Navigate to {ConfluentHome} and start the schema registry node using, bin/schema-registry-start ./etc/schema-registry/schema-registry.properties Post the avro schema to schema registry using, curl -X POST -H \"Content-Type: application/json\" --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}' http://localhost:8081/subjects/sweet-production/versions Navigate to {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer and run the 'ant' command without arguments. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInAvroFormatUsingSchemaRegistry.siddhi - Started Successfully! - Kafka version : 2.2.0 - Kafka commitId : 05fcfde8f69b0349 - Kafka producer created. Testing the Sample: \u00b6 Send events through one or more of the following methods. Option 1 - Send events to the kafka sink via the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishKafkaInAvroFormatUsingSchemaRegistry * Stream Name : SweetProductionStream 3. In the name and amount fields, enter the following values and then click Send to send the event. * name: chocolate cake * amount: 50.50 4. Send some more events. Option 2 - Publish events with Curl to the simulator HTTP endpoint: 1. Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' 2. If there is no error, the following messages are shown on the terminal: * {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3 - Publish events with Postman to the simulator HTTP endpoint: 1. Install the 'Postman' application from the Chrome web store. 2. Launch the Postman application. 3. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": ['chocolate cake', 50.50]} 4. Click 'send'. If there is no error, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer: [java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 2, key: null, topic: kafka_result_topic, partition: 0 [java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973831995, data=[chocolate cake, 50.5], isExpired=false} Notes: \u00b6 If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092\" does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInAvroFormatUsingSchemaRegistry\") @App:description('Send events via Kafka transport using Avro format') define stream SweetProductionStream (name string, amount double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', is.binary.message='true', @map(type='avro',schema.registry='http://localhost:8081',schema.id='1')) define stream LowProductionAlertStream (name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Avro Events via Kafka"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Avro format Using confluent Schema Registry.","title":"Purpose:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#prerequisites","text":"Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi. Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11_2.2.0.jar kafka_clients_2.2.0.jar metrics-core-2.2.0.jar scala_library_2.11.12.jar zkclient_0.11.jar zookeeper_3.4.13.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh For Windows: ./jartobundle.bat If converted successfully, the following messages are shown on the terminal for each lib file: INFO: Created the OSGi bundle .jar for JAR file /kafka/ .jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib. Download confluent-5.2.1 from https://www.confluent.io/download/ and unzip. Save this sample. If there is no syntax error, the following message is shown on the console: * -Siddhi App PublishKafkaInAvroFormatUsingSchemaRegistry successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#executing-the-sample","text":"Navigate to {KafkaHome} and start the zookeeper node using, bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using, bin/kafka-server-start.sh config/server.properties Navigate to {ConfluentHome} and start the schema registry node using, bin/schema-registry-start ./etc/schema-registry/schema-registry.properties Post the avro schema to schema registry using, curl -X POST -H \"Content-Type: application/json\" --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}' http://localhost:8081/subjects/sweet-production/versions Navigate to {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer and run the 'ant' command without arguments. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInAvroFormatUsingSchemaRegistry.siddhi - Started Successfully! - Kafka version : 2.2.0 - Kafka commitId : 05fcfde8f69b0349 - Kafka producer created.","title":"Executing the Sample:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#testing-the-sample","text":"Send events through one or more of the following methods. Option 1 - Send events to the kafka sink via the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishKafkaInAvroFormatUsingSchemaRegistry * Stream Name : SweetProductionStream 3. In the name and amount fields, enter the following values and then click Send to send the event. * name: chocolate cake * amount: 50.50 4. Send some more events. Option 2 - Publish events with Curl to the simulator HTTP endpoint: 1. Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' 2. If there is no error, the following messages are shown on the terminal: * {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3 - Publish events with Postman to the simulator HTTP endpoint: 1. Install the 'Postman' application from the Chrome web store. 2. Launch the Postman application. 3. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": ['chocolate cake', 50.50]} 4. Click 'send'. If there is no error, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#viewing-the-results","text":"See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer: [java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 2, key: null, topic: kafka_result_topic, partition: 0 [java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973831995, data=[chocolate cake, 50.5], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#notes","text":"If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092\" does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInAvroFormatUsingSchemaRegistry\") @App:description('Send events via Kafka transport using Avro format') define stream SweetProductionStream (name string, amount double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', is.binary.message='true', @map(type='avro',schema.registry='http://localhost:8081',schema.id='1')) define stream LowProductionAlertStream (name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishKafkaInBinaryFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Binary format. Prerequisites: \u00b6 Setup Kafka. Kafka libs to be added and converted to OSGI from {KafkaHome}/libs are as follows. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Add the OSGI converted kafka libs to {WSO2SIHome}/lib . Add the kafka libs to {WSO2SIHome}/samples/sample-clients/lib . Save this sample. If there is no syntax error, the following messages would be shown on the console. Siddhi App PublishKafkaInBinaryFormat successfully deployed. Note: \u00b6 To convert Kafka libs to OSGI, 1. Create a folder (Eg: Kafka) and copy Kafka libs to be added from {KafkaHome}/libs . 2. Create another folder(Eg: Kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to {WSO2SIHome}/bin and issue the follwing command. * For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> * For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> 4. If converted successfully then for each lib, following messages would be shown on the terminal. - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to {WSO2SIHome}/lib . Executing the Sample: \u00b6 Navigate to {KafkaHome} and start zookeeper node using following command. bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start kafka server node using following command. bin/kafka-server-start.sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run ant command with following arguments. ant -DisBinaryMessage=true -DtopicList=kafka_result_topic -Dtype=binary Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. - PublishKafkaInBinaryFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created. Testing the Sample: \u00b6 Send events with kafka server, through event simulator: \u00b6 To open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishKafkaInBinaryFormat Stream Name: SweetProductionStream In the batchNumber field and lowTotal fields, enter '1', '85.5' respectively and then click Send to send the event. Send some more events. Publish events with curl command: \u00b6 Open a new terminal and issue the following command. curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\", \"data\": [1, 85.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in json format as follows, {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\",\"data\": [1, 85.5]} Click 'send'. If there is no error, the following messages would be shown on the console. \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 It will print the results in binary format. Notes: \u00b6 If the message \"'Kafka' sink at 'LowProducitonAlertStream' has successfully connected to 'http://localhost:9092' does not appear, it could be due to port 9092, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9092 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInBinaryFormat\") @App:description('Send events via Kafka transport using Binary format') define stream SweetProductionStream (batchNumber long, lowTotal double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', is.binary.message='true', @map(type='binary')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Binary Events via Kafka"},{"location":"samples/PublishKafkaInBinaryFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Binary format.","title":"Purpose:"},{"location":"samples/PublishKafkaInBinaryFormat/#prerequisites","text":"Setup Kafka. Kafka libs to be added and converted to OSGI from {KafkaHome}/libs are as follows. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Add the OSGI converted kafka libs to {WSO2SIHome}/lib . Add the kafka libs to {WSO2SIHome}/samples/sample-clients/lib . Save this sample. If there is no syntax error, the following messages would be shown on the console. Siddhi App PublishKafkaInBinaryFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishKafkaInBinaryFormat/#note","text":"To convert Kafka libs to OSGI, 1. Create a folder (Eg: Kafka) and copy Kafka libs to be added from {KafkaHome}/libs . 2. Create another folder(Eg: Kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to {WSO2SIHome}/bin and issue the follwing command. * For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> * For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> 4. If converted successfully then for each lib, following messages would be shown on the terminal. - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to {WSO2SIHome}/lib .","title":"Note:"},{"location":"samples/PublishKafkaInBinaryFormat/#executing-the-sample","text":"Navigate to {KafkaHome} and start zookeeper node using following command. bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start kafka server node using following command. bin/kafka-server-start.sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run ant command with following arguments. ant -DisBinaryMessage=true -DtopicList=kafka_result_topic -Dtype=binary Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. - PublishKafkaInBinaryFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created.","title":"Executing the Sample:"},{"location":"samples/PublishKafkaInBinaryFormat/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/PublishKafkaInBinaryFormat/#send-events-with-kafka-server-through-event-simulator","text":"To open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishKafkaInBinaryFormat Stream Name: SweetProductionStream In the batchNumber field and lowTotal fields, enter '1', '85.5' respectively and then click Send to send the event. Send some more events.","title":"Send events with kafka server, through event simulator:"},{"location":"samples/PublishKafkaInBinaryFormat/#publish-events-with-curl-command","text":"Open a new terminal and issue the following command. curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\", \"data\": [1, 85.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'","title":"Publish events with curl command:"},{"location":"samples/PublishKafkaInBinaryFormat/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in json format as follows, {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\",\"data\": [1, 85.5]} Click 'send'. If there is no error, the following messages would be shown on the console. \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/PublishKafkaInBinaryFormat/#viewing-the-results","text":"It will print the results in binary format.","title":"Viewing the Results:"},{"location":"samples/PublishKafkaInBinaryFormat/#notes","text":"If the message \"'Kafka' sink at 'LowProducitonAlertStream' has successfully connected to 'http://localhost:9092' does not appear, it could be due to port 9092, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9092 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInBinaryFormat\") @App:description('Send events via Kafka transport using Binary format') define stream SweetProductionStream (batchNumber long, lowTotal double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', is.binary.message='true', @map(type='binary')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishKafkaInCustomAvroFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Avro format with custom mapping Prerequisites: \u00b6 Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi. Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh For Windows: ./jartobundle.bat If converted successfully, the following messages are shown on the terminal for each lib file: INFO: Created the OSGi bundle .jar for JAR file /kafka/ .jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib. Save this sample. If there is no syntax error, the following message is shown on the console: -Siddhi App PublishKafkaInAVroFormat successfully deployed. Executing the Sample: \u00b6 Navigate to {KafkaHome} and start the zookeeper node using bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using bin/kafka-server-start. Navigate to {ConfluentHome} and start the schema registry node using, bin/schema-registry-start ./etc/schema-registry/schema-registry.properties Post the avro schema to schema registry using, curl -X POST -H \"Content-Type: application/json\" --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"Amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}' http://localhost:8081/subjects/sweet-production/versions sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer and run the 'ant' command as follows: ant -Dtype=avro -DisBinaryMessage=true Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInCustomAvroFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created. Testing the Sample: \u00b6 Send events through one or more of the following methods. Option 1 - Send events to the kafka sink via the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishKafkaInCustomAvroFormat * Stream Name : SweetProductionStream 3. In the name and amount fields, enter the following values and then click Send to send the event. * name: chocolate cake * amount: 50.50 4. Send some more events. Option 2 - Publish events with Curl to the simulator HTTP endpoint: 1. Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' 2. If there is no error, the following messages are shown on the terminal: * {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3 - Publish events with Postman to the simulator HTTP endpoint: 1. Install the 'Postman' application from the Chrome web store. 2. Launch the Postman application. 3. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": ['chocolate cake', 50.50]} 4. Click 'send'. If there is no error, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer: [java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 0, key: null, topic: kafka_result_topic, partition: 0 [java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973319457, data=[chocolate cake, 50.5], isExpired=false} Notes: \u00b6 If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092\" does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInCustomAvroFormat\") @App:description('Send events via Kafka transport using Custom Avro format') define stream SweetProductionStream (sweetName string, sweetAmount double); @sink(type='kafka', topic='kafka_result_topic', is.binary.message='true', bootstrap.servers='localhost:9092', @map(type='avro',schema.def=\"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}\"\"\", @payload(\"\"\"{\"name\": \"{{sweetName}}\", \"amount\": {{sweetAmount}}}\"\"\"))) define stream LowProductionAlertStream (sweetName string, sweetAmount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Custom Avro Events via Kafka"},{"location":"samples/PublishKafkaInCustomAvroFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Avro format with custom mapping","title":"Purpose:"},{"location":"samples/PublishKafkaInCustomAvroFormat/#prerequisites","text":"Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi. Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh For Windows: ./jartobundle.bat If converted successfully, the following messages are shown on the terminal for each lib file: INFO: Created the OSGi bundle .jar for JAR file /kafka/ .jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib. Save this sample. If there is no syntax error, the following message is shown on the console: -Siddhi App PublishKafkaInAVroFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishKafkaInCustomAvroFormat/#executing-the-sample","text":"Navigate to {KafkaHome} and start the zookeeper node using bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using bin/kafka-server-start. Navigate to {ConfluentHome} and start the schema registry node using, bin/schema-registry-start ./etc/schema-registry/schema-registry.properties Post the avro schema to schema registry using, curl -X POST -H \"Content-Type: application/json\" --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"Amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}' http://localhost:8081/subjects/sweet-production/versions sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer and run the 'ant' command as follows: ant -Dtype=avro -DisBinaryMessage=true Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInCustomAvroFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created.","title":"Executing the Sample:"},{"location":"samples/PublishKafkaInCustomAvroFormat/#testing-the-sample","text":"Send events through one or more of the following methods. Option 1 - Send events to the kafka sink via the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows: * Siddhi App Name : PublishKafkaInCustomAvroFormat * Stream Name : SweetProductionStream 3. In the name and amount fields, enter the following values and then click Send to send the event. * name: chocolate cake * amount: 50.50 4. Send some more events. Option 2 - Publish events with Curl to the simulator HTTP endpoint: 1. Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' 2. If there is no error, the following messages are shown on the terminal: * {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3 - Publish events with Postman to the simulator HTTP endpoint: 1. Install the 'Postman' application from the Chrome web store. 2. Launch the Postman application. 3. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": ['chocolate cake', 50.50]} 4. Click 'send'. If there is no error, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\"","title":"Testing the Sample:"},{"location":"samples/PublishKafkaInCustomAvroFormat/#viewing-the-results","text":"See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer: [java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 0, key: null, topic: kafka_result_topic, partition: 0 [java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973319457, data=[chocolate cake, 50.5], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/PublishKafkaInCustomAvroFormat/#notes","text":"If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092\" does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInCustomAvroFormat\") @App:description('Send events via Kafka transport using Custom Avro format') define stream SweetProductionStream (sweetName string, sweetAmount double); @sink(type='kafka', topic='kafka_result_topic', is.binary.message='true', bootstrap.servers='localhost:9092', @map(type='avro',schema.def=\"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}\"\"\", @payload(\"\"\"{\"name\": \"{{sweetName}}\", \"amount\": {{sweetAmount}}}\"\"\"))) define stream LowProductionAlertStream (sweetName string, sweetAmount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishKafkaInJsonFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in JSON format. Prerequisites: \u00b6 Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi . Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> If converted successfully, the following messages are shown on the terminal for each lib file: - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishKafkaInJsonFormat successfully deployed. Executing the Sample: \u00b6 Navigate to {KafkaHome} and start the zookeeper node using following command. bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using following command. bin/kafka-server-start.sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run the ant command without arguments. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInJsonFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created. Testing the Sample: \u00b6 Send events through one or more of the following methods. Option 1: Send events to the kafka sink via the event simulator: \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: PublishKafkaInJsonFormat Stream Name: SweetProductionStream In the batchNumber and lowTotal fields, enter the following values and then click Send to send the event. batchNumber: 1 lowTotal: 50.50 Send some more events. Option 2: Publish events with Curl to the simulator HTTP endpoint: \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Option 3: Publish events with Postman to the simulator HTTP endpoint: \u00b6 Install the 'Postman' application from the Chrome web store. Launch the Postman application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-consumer : [java] [org.wso2.si.sample.kafka.consumer.KafkaReceiver] : Event received in Kafka Event Adaptor: {\"event\":{\"name\":\"chocolate cake\",\"amount\":50.50}}, offSet: 0, key: null, topic: kafka_result_topic, partition: 0 [java] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] : Committed offset 1 for partition kafka_result_topic-0 Notes: \u00b6 If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092' does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInJsonFormat\") @App:description('Send events via Kafka transport using JSON format') define stream SweetProductionStream (batchNumber long, lowTotal double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', @map(type='json')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing JSON Events via Kafka"},{"location":"samples/PublishKafkaInJsonFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in JSON format.","title":"Purpose:"},{"location":"samples/PublishKafkaInJsonFormat/#prerequisites","text":"Set up Kafka as follows: Create a folder called kafka and another folder called kafka-osgi . Copy the following files from {KafkaHome}/libs to the kafka folder you just created: kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder. Navigate to {WSO2SIHome}/bin and issue the following command: For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> If converted successfully, the following messages are shown on the terminal for each lib file: - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishKafkaInJsonFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishKafkaInJsonFormat/#executing-the-sample","text":"Navigate to {KafkaHome} and start the zookeeper node using following command. bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start the kafka server node using following command. bin/kafka-server-start.sh config/server.properties Navigate to {WSO2SIHome}/samples/sample-clients/kafka-consumer and run the ant command without arguments. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - PublishKafkaInJsonFormat.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : 23c69d62a0cabf06 - Kafka producer created.","title":"Executing the Sample:"},{"location":"samples/PublishKafkaInJsonFormat/#testing-the-sample","text":"Send events through one or more of the following methods.","title":"Testing the Sample:"},{"location":"samples/PublishKafkaInJsonFormat/#option-1-send-events-to-the-kafka-sink-via-the-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: PublishKafkaInJsonFormat Stream Name: SweetProductionStream In the batchNumber and lowTotal fields, enter the following values and then click Send to send the event. batchNumber: 1 lowTotal: 50.50 Send some more events.","title":"Option 1: Send events to the kafka sink via the event simulator:"},{"location":"samples/PublishKafkaInJsonFormat/#option-2-publish-events-with-curl-to-the-simulator-http-endpoint","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Option 2: Publish events with Curl to the simulator HTTP endpoint:"},{"location":"samples/PublishKafkaInJsonFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","text":"Install the 'Postman' application from the Chrome web store. Launch the Postman application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Option 3: Publish events with Postman to the simulator HTTP endpoint:"},{"location":"samples/PublishKafkaInJsonFormat/#viewing-the-results","text":"See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-consumer : [java] [org.wso2.si.sample.kafka.consumer.KafkaReceiver] : Event received in Kafka Event Adaptor: {\"event\":{\"name\":\"chocolate cake\",\"amount\":50.50}}, offSet: 0, key: null, topic: kafka_result_topic, partition: 0 [java] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] : Committed offset 1 for partition kafka_result_topic-0","title":"Viewing the Results:"},{"location":"samples/PublishKafkaInJsonFormat/#notes","text":"If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092' does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console. @App:name(\"PublishKafkaInJsonFormat\") @App:description('Send events via Kafka transport using JSON format') define stream SweetProductionStream (batchNumber long, lowTotal double); @sink(type='kafka', topic='kafka_result_topic', bootstrap.servers='localhost:9092', @map(type='json')) define stream LowProductionAlertStream (batchNumber long, lowTotal double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishMqttInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via MQTT transport in XML format and view the output on the mqtt-consumer. Prerequisites: \u00b6 Save this sample. The following message appears on the console. Siddhi App PublishMqttInXmlFormat successfully deployed. Setting up MQTT Mosquitto broker in Ubuntu Linux. Add the mosquitto repository using the following commands. sudo apt-add-repository ppa:mosquitto-dev/mosquitto-ppa sudo apt-get update Execute the following command to install the Mosquitto broker package. ```bash sudo apt-get install mosquitto ```` Install Mosquitto developer libraries to develop MQTT clients. sudo apt-get install libmosquitto-dev Execute the following command to install Mosquitto client packages. sudo apt-get install mosquitto-clients Ensure that the Mosquitto broker is running. sudo service mosquitto status Executing the Sample: \u00b6 Open a terminal, navigate to the {WSO2SIHome}/samples/sample-clients/mqtt-consumer directory and run the ant command. If you use the default topic mqtt_topic and URL tcp://localhost:1883 , in your program use the ant command without any arguments. However, if you use a different topic, run the ant command with appropriate argument. e.g. ant -Dtopic=mqttTest Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console PublishMqttInXmlFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishMqttInXmlFormat Stream Name: SweetProductionStream In the name field and amount fields, enter 'toffee' and '45.24' respectively. Then click Send to send the event. Send some more events. Viewing the Results: \u00b6 See the output in the terminal of {WSO2SIHome}/samples/sample-clients/mqtt-consumer . You will get the output as follows (example for 3 events): [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462478777, data=[chocolate, 78.34], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462520264, data=[sweets, 34.57], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462534053, data=[coffee, 12.7], isExpired=false} Notes: \u00b6 If you need to edit this application while it is running, stop the application -> Save -> Start. If the message \"LowProductionAlertStream' stream could not connect to 'localhost:1883\", it could be due to port 1883, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on the menu bar -> 'Stop'). * Change the port 1883 to an unused port. To do this you need to add a new listener in mosquitto.conf (e.g., listener port 1884,listener port 1885). * Start the application and check whether the expected output appears on the console. @App:name(\"PublishMqttInXmlFormat\") define stream SweetProductionStream (name string, amount double); @sink(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing XML Events via MQTT"},{"location":"samples/PublishMqttInXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via MQTT transport in XML format and view the output on the mqtt-consumer.","title":"Purpose:"},{"location":"samples/PublishMqttInXmlFormat/#prerequisites","text":"Save this sample. The following message appears on the console. Siddhi App PublishMqttInXmlFormat successfully deployed. Setting up MQTT Mosquitto broker in Ubuntu Linux. Add the mosquitto repository using the following commands. sudo apt-add-repository ppa:mosquitto-dev/mosquitto-ppa sudo apt-get update Execute the following command to install the Mosquitto broker package. ```bash sudo apt-get install mosquitto ```` Install Mosquitto developer libraries to develop MQTT clients. sudo apt-get install libmosquitto-dev Execute the following command to install Mosquitto client packages. sudo apt-get install mosquitto-clients Ensure that the Mosquitto broker is running. sudo service mosquitto status","title":"Prerequisites:"},{"location":"samples/PublishMqttInXmlFormat/#executing-the-sample","text":"Open a terminal, navigate to the {WSO2SIHome}/samples/sample-clients/mqtt-consumer directory and run the ant command. If you use the default topic mqtt_topic and URL tcp://localhost:1883 , in your program use the ant command without any arguments. However, if you use a different topic, run the ant command with appropriate argument. e.g. ant -Dtopic=mqttTest Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console PublishMqttInXmlFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishMqttInXmlFormat/#testing-the-sample","text":"Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishMqttInXmlFormat Stream Name: SweetProductionStream In the name field and amount fields, enter 'toffee' and '45.24' respectively. Then click Send to send the event. Send some more events.","title":"Testing the Sample:"},{"location":"samples/PublishMqttInXmlFormat/#viewing-the-results","text":"See the output in the terminal of {WSO2SIHome}/samples/sample-clients/mqtt-consumer . You will get the output as follows (example for 3 events): [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462478777, data=[chocolate, 78.34], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462520264, data=[sweets, 34.57], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462534053, data=[coffee, 12.7], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/PublishMqttInXmlFormat/#notes","text":"If you need to edit this application while it is running, stop the application -> Save -> Start. If the message \"LowProductionAlertStream' stream could not connect to 'localhost:1883\", it could be due to port 1883, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on the menu bar -> 'Stop'). * Change the port 1883 to an unused port. To do this you need to add a new listener in mosquitto.conf (e.g., listener port 1884,listener port 1885). * Start the application and check whether the expected output appears on the console. @App:name(\"PublishMqttInXmlFormat\") define stream SweetProductionStream (name string, amount double); @sink(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishPrometheusMetricsHTTPServer/","text":"Purpose: \u00b6 This application demonstrates how to use siddhi-io-prometheus for publishing events through http server. Prerequisites: \u00b6 The following steps must be executed to enable WSO2 SP to publish events to Prometheus Download Prometheus server 'prometheus-2.5.0' from here : https://prometheus.io/download/#prometheus And extract the file in a preferred location. Open the prometheus.yml file from {prometheus} directory and add the following text under \"scrape_configs:\" \" - job_name: 'server' honor_labels: true static_configs: - targets: ['localhost:9080']\" you can replace values for 'localhost:9080' in targets according to your preferred host and port. In that case, the 'server.url' option must be included in the Sink definition with the same host and port values. Download and copy the prometheus client jars to the {WSO2SIHome}/lib directory as follows. Download the following jars from https://mvnrepository.com/artifact/io.prometheus and copy them to {WSO2SIHome}/lib directory. simpleclient_pushgateway-0.5.0.jar simpleclient_common-0.5.0.jar simpleclient-0.5.0.jar simpleclient_httpserver-0.5.0.jar Navigate to {prometheus} and start prometheus server using ' ./prometheus --config.file=\"prometheus.yml\" ' command Start the editor WSO2 SP by giving this command in the terminal : sh editor.sh Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console, PublishPrometheusMetricHTTPServer.siddhi - Started Successfully! SweetProductionStream has successfully connected at http://localhost:9080 Testing the Sample: \u00b6 Send events through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishPrometheusMetricHTTPServer Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '55.4' respectively and then click Send to send the event. Send some more events as follows, ['gingerbread', 72.3], ['toffees', 32.4], ['lollipop', 23.8]. Viewing the Results: \u00b6 See the output events through one or more of the following methods: Open the url \"http://localhost:9080/metrics\" in your browser. The folllowing output will be displayed, \"# HELP SweetProductionStream help for counter SweetProductionStream\" \"# TYPE SweetProductionStream counter\" \"SweetProductionStream{Name=\"gingerbread\",} 72.3\" \"SweetProductionStream{Name=\"lollipop\",} 23.8\" \"SweetProductionStream{Name=\"toffees\",} 87.8\"\" Send http request to the endpoint \"http://localhost:9090\" to the running prometheus server using the curl command: Open a new terminal and issue the following command: curl -X GET http://localhost:9090/ If there is no error, the result will be shown on the terminal in JSON Format similar to the following : {\"status\":\"success\", \"data\":{ \"resultType\":\"vector\", \"result\":[ { \"metric\":{\"Name\":\"gingerbread\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"72.3\"] }, { \"metric\":{\"Name\":\"lollipop\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"23.8\"] }, { \"metric\":{\"Name\":\"toffees\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"87.8\"] } ]} } @App:name(\"PublishPrometheusMetrics\") @App:description('Publish consumed events to Prometheus metrics and expose them via http server.') define stream FooStream(Name string, amount double); @sink(type='log', priority='WARN', prefix ='received sweet products') define stream LogStream(Name string, amount double); @sink(type='prometheus' , job='SweetProduction', publish.mode='server', metric.type='counter', value.attribute = \"amount\", @map(type='keyvalue')) define stream SweetProductionStream(Name string, amount double); @info(name='Add-events') from FooStream select * insert into SweetProductionStream; @info(name='Log-events') from FooStream select * insert into LogStream;","title":"Publishing Consumed Events to Prometheus Metrics and Exposing then via HTTP"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#purpose","text":"This application demonstrates how to use siddhi-io-prometheus for publishing events through http server.","title":"Purpose:"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#prerequisites","text":"The following steps must be executed to enable WSO2 SP to publish events to Prometheus Download Prometheus server 'prometheus-2.5.0' from here : https://prometheus.io/download/#prometheus And extract the file in a preferred location. Open the prometheus.yml file from {prometheus} directory and add the following text under \"scrape_configs:\" \" - job_name: 'server' honor_labels: true static_configs: - targets: ['localhost:9080']\" you can replace values for 'localhost:9080' in targets according to your preferred host and port. In that case, the 'server.url' option must be included in the Sink definition with the same host and port values. Download and copy the prometheus client jars to the {WSO2SIHome}/lib directory as follows. Download the following jars from https://mvnrepository.com/artifact/io.prometheus and copy them to {WSO2SIHome}/lib directory. simpleclient_pushgateway-0.5.0.jar simpleclient_common-0.5.0.jar simpleclient-0.5.0.jar simpleclient_httpserver-0.5.0.jar Navigate to {prometheus} and start prometheus server using ' ./prometheus --config.file=\"prometheus.yml\" ' command Start the editor WSO2 SP by giving this command in the terminal : sh editor.sh Save this sample","title":"Prerequisites:"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console, PublishPrometheusMetricHTTPServer.siddhi - Started Successfully! SweetProductionStream has successfully connected at http://localhost:9080","title":"Executing the Sample:"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#testing-the-sample","text":"Send events through the event simulator: Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : PublishPrometheusMetricHTTPServer Stream Name : SweetProductionStream In the name and amount fields, enter 'toffees' and '55.4' respectively and then click Send to send the event. Send some more events as follows, ['gingerbread', 72.3], ['toffees', 32.4], ['lollipop', 23.8].","title":"Testing the Sample:"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#viewing-the-results","text":"See the output events through one or more of the following methods: Open the url \"http://localhost:9080/metrics\" in your browser. The folllowing output will be displayed, \"# HELP SweetProductionStream help for counter SweetProductionStream\" \"# TYPE SweetProductionStream counter\" \"SweetProductionStream{Name=\"gingerbread\",} 72.3\" \"SweetProductionStream{Name=\"lollipop\",} 23.8\" \"SweetProductionStream{Name=\"toffees\",} 87.8\"\" Send http request to the endpoint \"http://localhost:9090\" to the running prometheus server using the curl command: Open a new terminal and issue the following command: curl -X GET http://localhost:9090/ If there is no error, the result will be shown on the terminal in JSON Format similar to the following : {\"status\":\"success\", \"data\":{ \"resultType\":\"vector\", \"result\":[ { \"metric\":{\"Name\":\"gingerbread\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"72.3\"] }, { \"metric\":{\"Name\":\"lollipop\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"23.8\"] }, { \"metric\":{\"Name\":\"toffees\", \"__name__\":\"SweetProductionStream\", \"instance\":\"localhost:9080\", \"job\":\"server\"}, \"value\":[1*********.***,\"87.8\"] } ]} } @App:name(\"PublishPrometheusMetrics\") @App:description('Publish consumed events to Prometheus metrics and expose them via http server.') define stream FooStream(Name string, amount double); @sink(type='log', priority='WARN', prefix ='received sweet products') define stream LogStream(Name string, amount double); @sink(type='prometheus' , job='SweetProduction', publish.mode='server', metric.type='counter', value.attribute = \"amount\", @map(type='keyvalue')) define stream SweetProductionStream(Name string, amount double); @info(name='Add-events') from FooStream select * insert into SweetProductionStream; @info(name='Log-events') from FooStream select * insert into LogStream;","title":"Viewing the Results:"},{"location":"samples/PublishRabbitmqInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer. Prerequisites: \u00b6 Save this sample. The following message appears on the console. Siddhi App PublishRabbitmqInXmlFormat successfully deployed. Setting up RabbitMQ broker in Ubuntu Linux. To download and install RabbitMQ, run the following commands: sudo apt-get update sudo apt-get install rabbitmq-server To manage the maximum amount of connections upon launch, open and edit the following configuration file using a nano command: sudo nano /etc/default/rabbitmq-server To enable RabbitMQ Management Console, run the following command: sudo rabbitmq-plugins enable rabbitmq_management To start the service, issue the following command: invoke-rc.d rabbitmq-server start Executing the Sample: \u00b6 Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/rabbitmq-consumer directory and run the ant command. If you use the default exchange RABBITMQSAMPLE and URI amqp://guest:guest@localhost:5672 in your program use ant command without any arguments. However, if you use different exchange names or URIs, run the ant command with appropriate arguments. e.g., ant -Dexchange=rabbitMqtest Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. PublishRabbitmqInXmlFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Open the event simulator by clicking the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishRabbitmqInXmlFormat Stream Name: SweetProductiontream In the name field and amount fields, enter 'toffee' and '45.24' respectively and then click Send to send the event. Send some more events. Viewing the Results: \u00b6 See the output in the terminal of {WSO2SIHome}/samples/sample-clients/rabbitmq-consumer . You will get the output as follows (sample output for 4 events): [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448790922, data=[toffee, 9.78], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448813975, data=[lollipop, 12.6], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448830488, data=[Pop, 72.6], isExpired=false} Notes: \u00b6 If you need to edit this application while it is running, stop the application -> Save -> Start. If the message 'LowProducitonAlertStream' stream could not connect to 'localhost:5672' , it could be due to port 5672, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 5672 to an unused port. * Start the application and check whether the expected output appears on the console. RabbitMq consumer was written according to this example (go through the RabbitMQReceiver application). @App:name(\"PublishRabbitmqInXmlFormat\") @APP:description(\"demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer\") define stream SweetProductionStream (name string, amount double); @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'RABBITMQSAMPLE', @map(type='xml')) define stream LowProducitonAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProducitonAlertStream;","title":"Publishing XML Events via RabbitMQ"},{"location":"samples/PublishRabbitmqInXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer.","title":"Purpose:"},{"location":"samples/PublishRabbitmqInXmlFormat/#prerequisites","text":"Save this sample. The following message appears on the console. Siddhi App PublishRabbitmqInXmlFormat successfully deployed. Setting up RabbitMQ broker in Ubuntu Linux. To download and install RabbitMQ, run the following commands: sudo apt-get update sudo apt-get install rabbitmq-server To manage the maximum amount of connections upon launch, open and edit the following configuration file using a nano command: sudo nano /etc/default/rabbitmq-server To enable RabbitMQ Management Console, run the following command: sudo rabbitmq-plugins enable rabbitmq_management To start the service, issue the following command: invoke-rc.d rabbitmq-server start","title":"Prerequisites:"},{"location":"samples/PublishRabbitmqInXmlFormat/#executing-the-sample","text":"Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/rabbitmq-consumer directory and run the ant command. If you use the default exchange RABBITMQSAMPLE and URI amqp://guest:guest@localhost:5672 in your program use ant command without any arguments. However, if you use different exchange names or URIs, run the ant command with appropriate arguments. e.g., ant -Dexchange=rabbitMqtest Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. PublishRabbitmqInXmlFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/PublishRabbitmqInXmlFormat/#testing-the-sample","text":"Open the event simulator by clicking the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishRabbitmqInXmlFormat Stream Name: SweetProductiontream In the name field and amount fields, enter 'toffee' and '45.24' respectively and then click Send to send the event. Send some more events.","title":"Testing the Sample:"},{"location":"samples/PublishRabbitmqInXmlFormat/#viewing-the-results","text":"See the output in the terminal of {WSO2SIHome}/samples/sample-clients/rabbitmq-consumer . You will get the output as follows (sample output for 4 events): [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448790922, data=[toffee, 9.78], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448813975, data=[lollipop, 12.6], isExpired=false} [java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper [java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448830488, data=[Pop, 72.6], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/PublishRabbitmqInXmlFormat/#notes","text":"If you need to edit this application while it is running, stop the application -> Save -> Start. If the message 'LowProducitonAlertStream' stream could not connect to 'localhost:5672' , it could be due to port 5672, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 5672 to an unused port. * Start the application and check whether the expected output appears on the console. RabbitMq consumer was written according to this example (go through the RabbitMQReceiver application). @App:name(\"PublishRabbitmqInXmlFormat\") @APP:description(\"demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer\") define stream SweetProductionStream (name string, amount double); @sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'RABBITMQSAMPLE', @map(type='xml')) define stream LowProducitonAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProducitonAlertStream;","title":"Notes:"},{"location":"samples/PublishTcpInBinaryFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in binary format and view the output on the console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory. Run the command: ant -Dcontext=LowProductionAlertStream Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInBinaryFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'. Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInBinaryFormat Stream Name: SweetProductionStream In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event. Send some more events. Check the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server. You will see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false} Notes: \u00b6 If you need to edit this application while it is running, stop the application -> Save -> Start. If the message 'LowProducitonAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInBinaryFormat\") @App:description('Send events via TCP transport using binary format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='binary')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Binary Events via TCP"},{"location":"samples/PublishTcpInBinaryFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in binary format and view the output on the console.","title":"Purpose:"},{"location":"samples/PublishTcpInBinaryFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/PublishTcpInBinaryFormat/#executing-the-sample","text":"Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory. Run the command: ant -Dcontext=LowProductionAlertStream Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInBinaryFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'. Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInBinaryFormat Stream Name: SweetProductionStream In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event. Send some more events. Check the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server. You will see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}","title":"Executing the Sample:"},{"location":"samples/PublishTcpInBinaryFormat/#notes","text":"If you need to edit this application while it is running, stop the application -> Save -> Start. If the message 'LowProducitonAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInBinaryFormat\") @App:description('Send events via TCP transport using binary format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='binary')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishTcpInJsonFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in JSON format, and view the output on the console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory and run the following command: ant -Dtype=json -Dcontext=LowProductionAlertStream Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInJsonFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProducitonAlertStream' stream successfully connected to 'localhost:9892'. Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInJsonFormat Stream Name: SweetProductionStream In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event. Send some more events. See the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server . You can see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false} Notes: \u00b6 If you need to edit this application while it is running, stop the application -> Save -> Start. If you see the message 'LowProducitonAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInJsonFormat\") @App:description('Send events via TCP transport using json format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing JSON Events via TCP"},{"location":"samples/PublishTcpInJsonFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in JSON format, and view the output on the console.","title":"Purpose:"},{"location":"samples/PublishTcpInJsonFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/PublishTcpInJsonFormat/#executing-the-sample","text":"Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory and run the following command: ant -Dtype=json -Dcontext=LowProductionAlertStream Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInJsonFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProducitonAlertStream' stream successfully connected to 'localhost:9892'. Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInJsonFormat Stream Name: SweetProductionStream In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event. Send some more events. See the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server . You can see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}","title":"Executing the Sample:"},{"location":"samples/PublishTcpInJsonFormat/#notes","text":"If you need to edit this application while it is running, stop the application -> Save -> Start. If you see the message 'LowProducitonAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInJsonFormat\") @App:description('Send events via TCP transport using json format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='json')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishTcpInTextFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in text format, and view the output on the server terminal. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory. Run the ant command with argument -Dtype=text . Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInTextFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInTextFormat Stream Name: SweetProductionStream In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event. Send some more events. See the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server . You can see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false} Notes: \u00b6 If you need to edit this application while it is running, stop the application -> Save -> Start. If you see the message 'LowProductionAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9893 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInTextFormat\") @App:description('Send events via TCP transport using text format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='text')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing Text Events via TCP"},{"location":"samples/PublishTcpInTextFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in text format, and view the output on the server terminal.","title":"Purpose:"},{"location":"samples/PublishTcpInTextFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/PublishTcpInTextFormat/#executing-the-sample","text":"Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/tcp-server directory. Run the ant command with argument -Dtype=text . Start the Siddhi application by clicking 'Run'. If the Siddhi application starts successfully, the following messages appear on the console. * PublishTcpInTextFormat.siddhi - Started Successfully! * 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishTcpInTextFormat Stream Name: SweetProductionStream In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event. Send some more events. See the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server . You can see output similar to the following: [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false} [java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}","title":"Executing the Sample:"},{"location":"samples/PublishTcpInTextFormat/#notes","text":"If you need to edit this application while it is running, stop the application -> Save -> Start. If you see the message 'LowProductionAlertStream' stream could not connect to 'localhost:9892' , it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -> 'Stop'). * Change the port 9893 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console. @App:name(\"PublishTcpInTextFormat\") @App:description('Send events via TCP transport using text format') define stream SweetProductionStream (name string, amount double); @sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream', @map(type='text')) define stream LowProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/PublishWebSocketInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via WebSocket transport in XML default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishWebSocketInXmlFormat successfully deployed. Executing the Sample: \u00b6 Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/websocket-receiver directory and run the ant command. If you use the default host 'localhost' and port '8025' in your program use ant command without any arguments. However, if you use different host or port, run the ant command with appropriate arguments. e.g., ant -Dport=9025 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishWebSocketInXmlFormat.siddhi - Started Successfully! Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishWebSocketInXmlFormat Stream Name: SweetProductionStream In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event. Send some more events. Check the output in the terminal of {WSO2SIHome}/samples/sample-clients/websocket-receiver . You will see output similar to the following: WebSocketSample : logStream : Event{timestamp=1517982716368, data=[toffee, 45.25], isExpired=false} WebSocketSample : logStream : Event{timestamp=1517982792293, data=[coffee, 9.78], isExpired=false} WebSocketSample : logStream : Event{timestamp=1517982828856, data=[chocolate, 78.23], isExpired=false} Viewing the Results: \u00b6 See the output on the terminal. Notes: \u00b6 If the message 'LowProductionAlertStream' stream could not connect to 'localhost:8025' , it could be due to port 8025 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8025 to an unused port in this Siddhi application's source configuration and in the websocket-receiver file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"PublishWebSocketInXmlFormat\") @App:description('Send events via WebSocket transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='websocket', url='ws://localhost:8025/abc', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Publishing XML Events via WebSocket"},{"location":"samples/PublishWebSocketInXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via WebSocket transport in XML default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/PublishWebSocketInXmlFormat/#prerequisites","text":"Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App PublishWebSocketInXmlFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/PublishWebSocketInXmlFormat/#executing-the-sample","text":"Open a terminal and navigate to the {WSO2SIHome}/samples/sample-clients/websocket-receiver directory and run the ant command. If you use the default host 'localhost' and port '8025' in your program use ant command without any arguments. However, if you use different host or port, run the ant command with appropriate arguments. e.g., ant -Dport=9025 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: PublishWebSocketInXmlFormat.siddhi - Started Successfully! Open the event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: PublishWebSocketInXmlFormat Stream Name: SweetProductionStream In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event. Send some more events. Check the output in the terminal of {WSO2SIHome}/samples/sample-clients/websocket-receiver . You will see output similar to the following: WebSocketSample : logStream : Event{timestamp=1517982716368, data=[toffee, 45.25], isExpired=false} WebSocketSample : logStream : Event{timestamp=1517982792293, data=[coffee, 9.78], isExpired=false} WebSocketSample : logStream : Event{timestamp=1517982828856, data=[chocolate, 78.23], isExpired=false}","title":"Executing the Sample:"},{"location":"samples/PublishWebSocketInXmlFormat/#viewing-the-results","text":"See the output on the terminal.","title":"Viewing the Results:"},{"location":"samples/PublishWebSocketInXmlFormat/#notes","text":"If the message 'LowProductionAlertStream' stream could not connect to 'localhost:8025' , it could be due to port 8025 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). 2. Change the port from 8025 to an unused port in this Siddhi application's source configuration and in the websocket-receiver file. 3. Start the application and check whether the expected output appears on the console. @App:name(\"PublishWebSocketInXmlFormat\") @App:description('Send events via WebSocket transport using XML format') define stream SweetProductionStream (name string, amount double); @sink(type='websocket', url='ws://localhost:8025/abc', @map(type='xml')) define stream LowProductionAlertStream (name string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Notes:"},{"location":"samples/RScriptSample/","text":"Purpose: \u00b6 This application demonstrates how to configure a Siddhi app to process events with a R script. Prerequisites: \u00b6 Install R. See notes for more information see 'How to use' in https://github.com/wso2-extensions/siddhi-gpl-execution-r/blob/master/README.md. Download siddhi-gpl-execution-r-x.x.x.jar from http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/r/siddhi-gpl-execution-r/ and copy the jar to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App RScriptSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * RScriptSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You may send events to weather stream, via event simulator. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : RScriptSample Stream Name : weather Use the following values in the time and temp fields and send each value pair one after the other. time: 10, temp:55.6 time: 20, temp:65.6 time: 30, temp:75.6 Viewing the Results: \u00b6 See the processed output for each event on the console. @App:name(\"RScriptSample\") @App:description('Use a R script to process events and produce aggregated outputs based on the provided input variable parameters and expected output attributes.') define stream weather (time long, temp double); @sink(type='log') define stream dataOut (time long, temp double, c long, m double ); @info(name = 'query') from weather#window.lengthBatch(2)#r:eval(\"c <- sum(time); m <- sum(temp); \", \"c long, m double\",time, temp) select * insert into dataOut;","title":"Processing Events and Producing Aggregated Outputs Using R Script"},{"location":"samples/RScriptSample/#purpose","text":"This application demonstrates how to configure a Siddhi app to process events with a R script.","title":"Purpose:"},{"location":"samples/RScriptSample/#prerequisites","text":"Install R. See notes for more information see 'How to use' in https://github.com/wso2-extensions/siddhi-gpl-execution-r/blob/master/README.md. Download siddhi-gpl-execution-r-x.x.x.jar from http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/r/siddhi-gpl-execution-r/ and copy the jar to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App RScriptSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/RScriptSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * RScriptSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/RScriptSample/#testing-the-sample","text":"You may send events to weather stream, via event simulator. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name : RScriptSample Stream Name : weather Use the following values in the time and temp fields and send each value pair one after the other. time: 10, temp:55.6 time: 20, temp:65.6 time: 30, temp:75.6","title":"Testing the Sample:"},{"location":"samples/RScriptSample/#viewing-the-results","text":"See the processed output for each event on the console. @App:name(\"RScriptSample\") @App:description('Use a R script to process events and produce aggregated outputs based on the provided input variable parameters and expected output attributes.') define stream weather (time long, temp double); @sink(type='log') define stream dataOut (time long, temp double, c long, m double ); @info(name = 'query') from weather#window.lengthBatch(2)#r:eval(\"c <- sum(time); m <- sum(temp); \", \"c long, m double\",time, temp) select * insert into dataOut;","title":"Viewing the Results:"},{"location":"samples/ReceiveAndCount/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events via HTTP transport and view the output on the console. The count of all events arriving to the stream is calculated. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveAndCount.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Testing the Sample: \u00b6 Publish events to the http endpoint defined by receiver.url in Source configuration. \u00b6 Publish events with the client: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal. If you want to publish custom number of events, you need to run \"ant\" command as follows. ant -DnoOfEventsToSend=5 Publish events with curl command: \u00b6 Publish few events in json format to the http endpoint as follows (The values for name, age and country attributes can be changed as desired). curl -X POST -d \"{\\\"event\\\":{\\\"name\\\":\\\"Cake\\\",\\\"amount\\\":20.12}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows. { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } } Viewing the Results: \u00b6 See the output on the console. Note how the count increments with every event you send. @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream;","title":"Receiving Events via HTTP Transport"},{"location":"samples/ReceiveAndCount/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events via HTTP transport and view the output on the console. The count of all events arriving to the stream is calculated.","title":"Purpose:"},{"location":"samples/ReceiveAndCount/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveAndCount/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveAndCount.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveAndCount/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveAndCount/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveAndCount/#publish-events-to-the-http-endpoint-defined-by-receiverurl-in-source-configuration","text":"","title":"Publish events to the http endpoint defined by receiver.url in Source configuration."},{"location":"samples/ReceiveAndCount/#publish-events-with-the-client","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal. If you want to publish custom number of events, you need to run \"ant\" command as follows. ant -DnoOfEventsToSend=5","title":"Publish events with the client:"},{"location":"samples/ReceiveAndCount/#publish-events-with-curl-command","text":"Publish few events in json format to the http endpoint as follows (The values for name, age and country attributes can be changed as desired). curl -X POST -d \"{\\\"event\\\":{\\\"name\\\":\\\"Cake\\\",\\\"amount\\\":20.12}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\"","title":"Publish events with curl command:"},{"location":"samples/ReceiveAndCount/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows. { \"event\": { \"name\": \"Cake\", \"amount\": 20.12 } }","title":"Publish events with Postman:"},{"location":"samples/ReceiveAndCount/#viewing-the-results","text":"See the output on the console. Note how the count increments with every event you send. @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveEmailInXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to use siddhi-io-email for receiving events from emails. Prerequisites: \u00b6 Add relevant siddhi-io-email and siddhi-map-xml jars to the {WSO2Home}/lib folder if not exist. Make sure you have provide less secure access to the sender's email account. eg: For gmail this can be done by visiting https://myaccount.google.com/lesssecureapps. Edit the siddhi app by providing following details. receiver_email_username receiver_email_password Give the subject of the email to . For further information search.terms refer https://wso2-extensions.github.io/siddhi-io-email/api/1.0.9/. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. ReceiveEmailInXmlFormat.siddhi - Started Successfully! Send an email with the body in following format to receiver's email address. <events> <event> <name>WSO2</name> <amount>34.56</amount> </event> </events> The message should be logged in the console. @App:name(\"ReceiveEmailInXmlFormat\") @source(type='email', @map(type='xml'), username='<receiver_username>', password='<receiver_email_password>', store = 'imap' , host = 'imap.gmail.com', folder = 'INBOX', ssl.enable = 'true' , polling.interval = '30' , search.term = 'subject:<subject_of_mail>' , content.type = 'text/plain') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, hourlyTotal double, currentHour double); from SweetProductionStream select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream;","title":"Receiving XML Events via Email"},{"location":"samples/ReceiveEmailInXmlFormat/#purpose","text":"This application demonstrates how to use siddhi-io-email for receiving events from emails.","title":"Purpose:"},{"location":"samples/ReceiveEmailInXmlFormat/#prerequisites","text":"Add relevant siddhi-io-email and siddhi-map-xml jars to the {WSO2Home}/lib folder if not exist. Make sure you have provide less secure access to the sender's email account. eg: For gmail this can be done by visiting https://myaccount.google.com/lesssecureapps. Edit the siddhi app by providing following details. receiver_email_username receiver_email_password Give the subject of the email to . For further information search.terms refer https://wso2-extensions.github.io/siddhi-io-email/api/1.0.9/. Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveEmailInXmlFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. ReceiveEmailInXmlFormat.siddhi - Started Successfully! Send an email with the body in following format to receiver's email address. <events> <event> <name>WSO2</name> <amount>34.56</amount> </event> </events> The message should be logged in the console. @App:name(\"ReceiveEmailInXmlFormat\") @source(type='email', @map(type='xml'), username='<receiver_username>', password='<receiver_email_password>', store = 'imap' , host = 'imap.gmail.com', folder = 'INBOX', ssl.enable = 'true' , polling.interval = '30' , search.term = 'subject:<subject_of_mail>' , content.type = 'text/plain') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, hourlyTotal double, currentHour double); from SweetProductionStream select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream;","title":"Executing the Sample:"},{"location":"samples/ReceiveEventsFromFile/","text":"Purpose: \u00b6 This application demonstrates how to use siddhi-io-file for receiving. Prerequisites: \u00b6 Edit this sample file by replacing {WSO2SIHome} with the absolute path of your WSO2SI home directory. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. ReceiveEventsFromFile.siddhi - Started Successfully! Check the directories {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed and new . All the files which were in the directory new should have been moved to consumed directory. Note: \u00b6 If the sample is not running and producing output, do the following first. * Move all the files in {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed directory to {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new . * Delete all the files in consumed and sink directories. Viewing the Results: \u00b6 Processed output events will be logged in the console as follows: INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847875990, beforeWindowData=null, onAfterWindowData=null, outputData=[apache, 80.0, 2.0], type=CURRENT, next=null} INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847876004, beforeWindowData=null, onAfterWindowData=null, outputData=[cloudbees, 134.4, 2.0], type=CURRENT, next=null} @App:name('ReceiveEventsFromFile') @source(type='file', mode='text.full', dir.uri='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new', action.after.process='move', tailing='false', move.after.process='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed', @map(type='json')) define stream SweetProductionStream (name string, amount double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream#log('event') insert into LogStream;","title":"Receiving Events via File"},{"location":"samples/ReceiveEventsFromFile/#purpose","text":"This application demonstrates how to use siddhi-io-file for receiving.","title":"Purpose:"},{"location":"samples/ReceiveEventsFromFile/#prerequisites","text":"Edit this sample file by replacing {WSO2SIHome} with the absolute path of your WSO2SI home directory. Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveEventsFromFile/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. ReceiveEventsFromFile.siddhi - Started Successfully! Check the directories {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed and new . All the files which were in the directory new should have been moved to consumed directory.","title":"Executing the Sample:"},{"location":"samples/ReceiveEventsFromFile/#note","text":"If the sample is not running and producing output, do the following first. * Move all the files in {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed directory to {WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new . * Delete all the files in consumed and sink directories.","title":"Note:"},{"location":"samples/ReceiveEventsFromFile/#viewing-the-results","text":"Processed output events will be logged in the console as follows: INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847875990, beforeWindowData=null, onAfterWindowData=null, outputData=[apache, 80.0, 2.0], type=CURRENT, next=null} INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847876004, beforeWindowData=null, onAfterWindowData=null, outputData=[cloudbees, 134.4, 2.0], type=CURRENT, next=null} @App:name('ReceiveEventsFromFile') @source(type='file', mode='text.full', dir.uri='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new', action.after.process='move', tailing='false', move.after.process='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed', @map(type='json')) define stream SweetProductionStream (name string, amount double); from SweetProductionStream#window.time(1 min) select name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour insert into LowProductionAlertStream; from LowProductionAlertStream#log('event') insert into LogStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub source in Siddhi to consume events. Events which are in TEXT format are consumed from a googlepubsub topic. Prerequisites: \u00b6 Create a Google Cloud Platform account. Sign in to Google Account and set up a GCP Console project and enable the API. Create a service account and download a private key as JSON. Place your json file in any system folder and provide the path for the credential.path. Save the sample. If there is no syntax error, the following message is shown on the console: * -Siddhi App ReceiveGooglePubSubMesssage successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console, ReceiveGooglePubSubMesssage.siddhi - Started Successfully! Testing the Sample: \u00b6 Receive events through the following, You may listen to the events coming to a topic after the subscription is made. Viewing the Results: \u00b6 See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEvent : BarStream : Event{timestamp=1552548124599, data=[message:\"Hello\"], isExpired=false} (Encoded) Notes: \u00b6 Make sure the the credential file is correct and user have write access to make api calls. Stop this Siddhi application @App:name(\"ReceiveGooglePubSubMesssage\") @App:description('Consume messages from a googlepubsub Topic.') @App:name(\"ReceiveEvent\") @App:description(\"Listen for messages received for a topic in Google Pub Sub Server.\") @source(type ='googlepubsub', project.id = 'sp-path-1547649404768', topic.id = 'topic75', credential.path = '/../sp.json', subscription.id = 'sub75', @map(type = 'text')) define stream FooStream (message string); @sink(type='log') define stream BarStream(message string); @info(name='EventsPassthroughQuery') from FooStream select message insert into BarStream;","title":"Receiving Messages from a Google Pub/Sub Topic"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub source in Siddhi to consume events. Events which are in TEXT format are consumed from a googlepubsub topic.","title":"Purpose:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#prerequisites","text":"Create a Google Cloud Platform account. Sign in to Google Account and set up a GCP Console project and enable the API. Create a service account and download a private key as JSON. Place your json file in any system folder and provide the path for the credential.path. Save the sample. If there is no syntax error, the following message is shown on the console: * -Siddhi App ReceiveGooglePubSubMesssage successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console, ReceiveGooglePubSubMesssage.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#testing-the-sample","text":"Receive events through the following, You may listen to the events coming to a topic after the subscription is made.","title":"Testing the Sample:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#viewing-the-results","text":"See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEvent : BarStream : Event{timestamp=1552548124599, data=[message:\"Hello\"], isExpired=false} (Encoded)","title":"Viewing the Results:"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#notes","text":"Make sure the the credential file is correct and user have write access to make api calls. Stop this Siddhi application @App:name(\"ReceiveGooglePubSubMesssage\") @App:description('Consume messages from a googlepubsub Topic.') @App:name(\"ReceiveEvent\") @App:description(\"Listen for messages received for a topic in Google Pub Sub Server.\") @source(type ='googlepubsub', project.id = 'sp-path-1547649404768', topic.id = 'topic75', credential.path = '/../sp.json', subscription.id = 'sub75', @map(type = 'text')) define stream FooStream (message string); @sink(type='log') define stream BarStream(message string); @info(name='EventsPassthroughQuery') from FooStream select message insert into BarStream;","title":"Notes:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in JSON format using custom mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInJsonFormatWithDefaultMapping.siddhi - Started Successfully! Testing the Sample: \u00b6 Option 1: Publish events with http sample client: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -DcustomMapping=true If you want to publish custom number of events, you need to run \"ant\" command as follows: ant -DcustomMapping=true -DnoOfEventsToSend=5 Option 2: Publish events with curl command: \u00b6 Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d \"{\\\"item\\\": {\\\"id\\\":\\\"Sugar\\\",\\\"amount\\\": 300.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Option 3: Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows. { \"item\": { \"id\": \"sugar\", \"amount\": 20.0 } } Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Viewing the Results: \u00b6 See the output. Following message would be shown on the console. ReceiveHTTPInJsonFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPInJsonFormatWithCustomMapping\") @App:description('Receive events via HTTP transport in JSON format with custom mapping and view the output on the console') @source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',@map(type='json', @attributes( name = '$.item.id', amount = '$.item.amount'))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Custom JSON Events via HTTP"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in JSON format using custom mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInJsonFormatWithDefaultMapping.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-1-publish-events-with-http-sample-client","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -DcustomMapping=true If you want to publish custom number of events, you need to run \"ant\" command as follows: ant -DcustomMapping=true -DnoOfEventsToSend=5","title":"Option 1: Publish events with http sample client:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-2-publish-events-with-curl-command","text":"Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d \"{\\\"item\\\": {\\\"id\\\":\\\"Sugar\\\",\\\"amount\\\": 300.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\"","title":"Option 2: Publish events with curl command:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-3-publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows. { \"item\": { \"id\": \"sugar\", \"amount\": 20.0 } }","title":"Option 3: Publish events with Postman:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#viewing-the-results","text":"See the output. Following message would be shown on the console. ReceiveHTTPInJsonFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPInJsonFormatWithCustomMapping\") @App:description('Receive events via HTTP transport in JSON format with custom mapping and view the output on the console') @source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',@map(type='json', @attributes( name = '$.item.id', amount = '$.item.amount'))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in JSON format using default mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully! Testing the Sample: \u00b6 Option 1: Publish events with http sample client: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run \"ant\" command as follows: Run ant command in the terminal. If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5 Option 2: Publish events with curl command: \u00b6 Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\" Option 3: Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"sugar\", \"amount\": 20.0 } } Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console Viewing the Results: \u00b6 See the output. Following message would be shown on the console. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProductionAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name('ReceiveHTTPInJsonFormatWithDefaultMapping') @App:description('Receive events via HTTP transport in JSON format with default mapping and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving JSON Events via HTTP"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in JSON format using default mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-1-publish-events-with-http-sample-client","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run \"ant\" command as follows: Run ant command in the terminal. If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5","title":"Option 1: Publish events with http sample client:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-2-publish-events-with-curl-command","text":"Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\"","title":"Option 2: Publish events with curl command:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-3-publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, { \"event\": { \"name\": \"sugar\", \"amount\": 20.0 } }","title":"Option 3: Publish events with Postman:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console","title":"Notes:"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#viewing-the-results","text":"See the output. Following message would be shown on the console. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProductionAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name('ReceiveHTTPInJsonFormatWithDefaultMapping') @App:description('Receive events via HTTP transport in JSON format with default mapping and view the output on the console') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in XML default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 1) Start the Siddhi application by clicking on 'Run'. 2) If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully! Testing the Sample: \u00b6 Option 1: Publish events with http sample client: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=xml If you want to publish custom number of events, you need to run ant command as follows: ant -Dtype=xml -DnoOfEventsToSend=5 Option 2: Publish events with curl command: \u00b6 Publish few events in xml format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d '<events><event><name>sugar</name><amount>300</amount></event></events>' http://localhost:8006/productionStream --header \"Content-Type:application/xml\" Option 3: Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <events> <event> <name>sugar</name> <amount>200</amount> </event> </events> Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console Viewing the Results: \u00b6 See the output. Following message would be shown on the console if you use option 2 or 3 to publish events. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPInXMLFormatWithDefaultMapping\") @App:description('Receive events via HTTP transport in XML format with default mapping and view the output on the console.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving XML Events via HTTP"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in XML default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#executing-the-sample","text":"1) Start the Siddhi application by clicking on 'Run'. 2) If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-1-publish-events-with-http-sample-client","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=xml If you want to publish custom number of events, you need to run ant command as follows: ant -Dtype=xml -DnoOfEventsToSend=5","title":"Option 1: Publish events with http sample client:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-2-publish-events-with-curl-command","text":"Publish few events in xml format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). curl -X POST -d '<events><event><name>sugar</name><amount>300</amount></event></events>' http://localhost:8006/productionStream --header \"Content-Type:application/xml\"","title":"Option 2: Publish events with curl command:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-3-publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <events> <event> <name>sugar</name> <amount>200</amount> </event> </events>","title":"Option 3: Publish events with Postman:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console","title":"Notes:"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#viewing-the-results","text":"See the output. Following message would be shown on the console if you use option 2 or 3 to publish events. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPInXMLFormatWithDefaultMapping\") @App:description('Receive events via HTTP transport in XML format with default mapping and view the output on the console.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/","text":"Purpose \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in XML custom format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithCustomMapping.siddhi - Started Successfully! Testing the Sample: \u00b6 Option 1: Publish events with http sample client: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=xml -DcustomMapping=true If you want to publish custom number of events, you need to run \"ant\" command as follows: ant -Dtype=xml -DcustomMapping=true -DnoOfEventsToSend=5 Option 2: Publish events with curl command: \u00b6 curl -X POST -d '<events><item><id>sugar</id><amount>300</amount></item></events>' http://localhost:8006/productionStream --header \"Content-Type:application/xml\" Option 3: Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <events> <item> <id>sugar</id> <amount>200</amount> </item> </events> Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console Viewing the Results: \u00b6 See the output. Following message would be shown on the console. ReceiveHTTPinXMLFormatWithCustomMapping : LowProducitonAlertStream : Event{timestamp=1511939868628, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPinXMLFormatWithCustomMapping\") @App:description('Receive events via HTTP transport in XML format with custom mapping and view the output on the console.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml',@attributes( name = \"item/id\", amount = \"item/amount\"))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Custom XML Events via HTTP"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via HTTP transport in XML custom format and log the events in LowProductionAlertStream to the output console.","title":"Purpose"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Source Listener has created for url http://localhost:8006/productionStream * ReceiveHTTPInXMLFormatWithCustomMapping.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-1-publish-events-with-http-sample-client","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=xml -DcustomMapping=true If you want to publish custom number of events, you need to run \"ant\" command as follows: ant -Dtype=xml -DcustomMapping=true -DnoOfEventsToSend=5","title":"Option 1: Publish events with http sample client:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-2-publish-events-with-curl-command","text":"curl -X POST -d '<events><item><id>sugar</id><amount>300</amount></item></events>' http://localhost:8006/productionStream --header \"Content-Type:application/xml\"","title":"Option 2: Publish events with curl command:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-3-publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <events> <item> <id>sugar</id> <amount>200</amount> </item> </events>","title":"Option 3: Publish events with Postman:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console","title":"Notes:"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#viewing-the-results","text":"See the output. Following message would be shown on the console. ReceiveHTTPinXMLFormatWithCustomMapping : LowProducitonAlertStream : Event{timestamp=1511939868628, data=[sugar, 300.0], isExpired=false} @App:name(\"ReceiveHTTPinXMLFormatWithCustomMapping\") @App:description('Receive events via HTTP transport in XML format with custom mapping and view the output on the console.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml',@attributes( name = \"item/id\", amount = \"item/amount\"))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveHl7InER7Format/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in ER7 format to the hl7Stream via MLLP protocol and log the events in er7Stream to the output console. Prerequisites: \u00b6 Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveHl7InER7Format successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. Starting SimpleServer running on port 4000 ReceiveHl7InER7Format.siddhi - Started Successfully! Testing the Sample: \u00b6 In the HAPI testpanel create a sending connection with port that provided in the siddhi app. Send this message 'MSH| ~\\&|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADT A01|589888ADT30502184808|P|2.3' from the testpanel Viewing the Results: \u00b6 See the output. Following message would be shown on the console if you publish events. ReceiveHl7InER7Format : er7Stream : Event{timestamp=1552530948958, data=[MSH| ~\\&|||||20190211145413.131+0530||ADT A01|10601|T|2.3 ], isExpired=false} @App:name('ReceiveHl7InER7Format') @App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and text mapping.') @source(type = 'hl7', port = '4000', hl7.encoding = 'er7', @map(type = 'text')) define stream hl7stream(payload string); @sink(type='log') define stream er7Stream (payload string); @info(name='query1') from hl7stream select * insert into er7Stream;","title":"Receiving ER7 Events via HL7"},{"location":"samples/ReceiveHl7InER7Format/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in ER7 format to the hl7Stream via MLLP protocol and log the events in er7Stream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveHl7InER7Format/#prerequisites","text":"Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveHl7InER7Format successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveHl7InER7Format/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. Starting SimpleServer running on port 4000 ReceiveHl7InER7Format.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHl7InER7Format/#testing-the-sample","text":"In the HAPI testpanel create a sending connection with port that provided in the siddhi app. Send this message 'MSH| ~\\&|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADT A01|589888ADT30502184808|P|2.3' from the testpanel","title":"Testing the Sample:"},{"location":"samples/ReceiveHl7InER7Format/#viewing-the-results","text":"See the output. Following message would be shown on the console if you publish events. ReceiveHl7InER7Format : er7Stream : Event{timestamp=1552530948958, data=[MSH| ~\\&|||||20190211145413.131+0530||ADT A01|10601|T|2.3 ], isExpired=false} @App:name('ReceiveHl7InER7Format') @App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and text mapping.') @source(type = 'hl7', port = '4000', hl7.encoding = 'er7', @map(type = 'text')) define stream hl7stream(payload string); @sink(type='log') define stream er7Stream (payload string); @info(name='query1') from hl7stream select * insert into er7Stream;","title":"Viewing the Results:"},{"location":"samples/ReceiveHl7InXmlFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in XML format to the hl7Stream via MLLP protocol and log the events in xmlStream to the output console. Prerequisites: \u00b6 Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveHl7InXmlFormat successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. Starting SimpleServer running on port 4000 ReceiveHl7InXmlFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 In the HAPI testpanel create a sending connection with port that provided in the siddhi app. Send this message 'MSH| ~\\&|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADT A01|589888ADT30502184808|P|2.3' from the testpanel Viewing the Results: \u00b6 See the output. Following message would be shown on the console if you publish events. ReceiveHl7InXmlFormat : er7Stream : Event{timestamp=1552532452870, data=[589888ADT30502184808, sendingSystemA], isExpired=false} @App:name('ReceiveHl7InXmlFormat') @App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and custom xml mapping.') @source(type = 'hl7', port = '4000', hl7.encoding = 'xml', @map(type = 'xml', namespaces = 'ns=urn:hl7-org:v2xml', @attributes(MSH10 = \"ns:MSH/ns:MSH.10\", MSH3HD1 = \"ns:MSH/ns:MSH.3/ns:HD.1\"))) define stream hl7stream (MSH10 string, MSH3HD1 string); @sink(type='log') define stream xmlStream (MSH10 string, MSH3HD1 string); @info(name='query1') from hl7stream select * insert into xmlStream;","title":"Receiving Custom XML Messages via HL7"},{"location":"samples/ReceiveHl7InXmlFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in XML format to the hl7Stream via MLLP protocol and log the events in xmlStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveHl7InXmlFormat/#prerequisites","text":"Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html) Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveHl7InXmlFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveHl7InXmlFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. Starting SimpleServer running on port 4000 ReceiveHl7InXmlFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveHl7InXmlFormat/#testing-the-sample","text":"In the HAPI testpanel create a sending connection with port that provided in the siddhi app. Send this message 'MSH| ~\\&|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADT A01|589888ADT30502184808|P|2.3' from the testpanel","title":"Testing the Sample:"},{"location":"samples/ReceiveHl7InXmlFormat/#viewing-the-results","text":"See the output. Following message would be shown on the console if you publish events. ReceiveHl7InXmlFormat : er7Stream : Event{timestamp=1552532452870, data=[589888ADT30502184808, sendingSystemA], isExpired=false} @App:name('ReceiveHl7InXmlFormat') @App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and custom xml mapping.') @source(type = 'hl7', port = '4000', hl7.encoding = 'xml', @map(type = 'xml', namespaces = 'ns=urn:hl7-org:v2xml', @attributes(MSH10 = \"ns:MSH/ns:MSH.10\", MSH3HD1 = \"ns:MSH/ns:MSH.3/ns:HD.1\"))) define stream hl7stream (MSH10 string, MSH3HD1 string); @sink(type='log') define stream xmlStream (MSH10 string, MSH3HD1 string); @info(name='query1') from hl7stream select * insert into xmlStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveJMSInJsonFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Jms transport in Json format using default mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar) Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip) ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveJMSInJsonFormat successfully deployed. Note: \u00b6 To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: * For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> * For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create 'activemq-client-5.x.x' directory in the with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal. - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3) You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib . Executing the Sample: \u00b6 Navigate to {apache-activemq-5.x.x} unzipped directory. Provide required permissions by executing, chmod +x bin/activemq Create system wide configuration defaults, by executing, sudo bin/activemq setup /etc/default/activemq Start ActiveMQ server node using 'bin/activemq start'. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: ReceiveJMSInJsonFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/jms-producer and run ant command without arguments. Viewing the Results: \u00b6 Messages similar to the following would be shown on the editor console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInJsonFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false} @App:name('ReceiveJMSInJsonFormat') @App:description('Receive events via JMS provider in JSON format with default mapping and view the output on the console.') @source(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', transport.jms.SubscriptionDurable='true', transport.jms.DurableSubscriberClientID='wso2SPclient1', @map(type='json')) define stream SweetProductionStream(name string, amount double); @sink(type='log') define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving JSON Events via JMS"},{"location":"samples/ReceiveJMSInJsonFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Jms transport in Json format using default mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveJMSInJsonFormat/#prerequisites","text":"Setup ActiveMQ Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar) Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip) ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveJMSInJsonFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveJMSInJsonFormat/#note","text":"To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: * For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> * For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) . * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create 'activemq-client-5.x.x' directory in the with OSGi converted and original jars: - activemq-client-5.x.x.jar (Original Jar) - activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal. - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3) You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib .","title":"Note:"},{"location":"samples/ReceiveJMSInJsonFormat/#executing-the-sample","text":"Navigate to {apache-activemq-5.x.x} unzipped directory. Provide required permissions by executing, chmod +x bin/activemq Create system wide configuration defaults, by executing, sudo bin/activemq setup /etc/default/activemq Start ActiveMQ server node using 'bin/activemq start'. Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: ReceiveJMSInJsonFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveJMSInJsonFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/jms-producer and run ant command without arguments.","title":"Testing the Sample:"},{"location":"samples/ReceiveJMSInJsonFormat/#viewing-the-results","text":"Messages similar to the following would be shown on the editor console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInJsonFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false} @App:name('ReceiveJMSInJsonFormat') @App:description('Receive events via JMS provider in JSON format with default mapping and view the output on the console.') @source(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', transport.jms.SubscriptionDurable='true', transport.jms.DurableSubscriberClientID='wso2SPclient1', @map(type='json')) define stream SweetProductionStream(name string, amount double); @sink(type='log') define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Jms transport in Keyvalue and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Setup ActiveMQ. Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip). ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveJMSInKeyvalueFormat successfully deployed. Note: \u00b6 To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: * For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> * For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: * activemq-client-5.x.x.jar (Original Jar) * activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib . Executing the Sample: \u00b6 Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq start . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: ReceiveJMSInKeyvalueFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/jms-producer and run the following command. ant -Dtype='keyvalue' Viewing the Results: \u00b6 Messages similar to the following would be shown on the editor console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInKeyvalueFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false} @App:name('ReceiveJMSInKeyvalueFormat') @App:description('Receive events via JMS provider in Keyvalue format with default mapping and view the output on the console.') @source(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='keyvalue')) define stream SweetProductionStream(name string, amount double); @sink(type='log') define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Key Value Events via JMS"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Jms transport in Keyvalue and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#prerequisites","text":"Setup ActiveMQ. Download activemq-client-5.x.x.jar (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar). Download apache-activemq-5.x.x-bin.zip (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip). ActiveMQ activemq-client-5.x.x.jar lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI). Unzip the apache-activemq-5.x.x-bin.zip and copy the following ActiveMQ libs in apache-activemq-5.x.x/lib to {WSO2SIHome}/samples/sample-clients/lib and {WSO2SIHome}/lib . hawtbuf-1.9.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Save this sample. If there is no syntax error, the following message is shown on the console: Siddhi App ReceiveJMSInKeyvalueFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#note","text":"To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command: * For Linux: ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>/activemq-client-5.x.x.jar <Output Jar Path> * For Windows: ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory <Downloaded Jar Path>\\activemq-client-5.x.x.jar <Output Jar Path> * Provide privileges if necessary using chmod +x icf-provider.(sh|bat) * Also, this will register the InitialContextFactory implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create activemq-client-5.x.x directory in the <Output Jar Path> with OSGi converted and original jars: * activemq-client-5.x.x.jar (Original Jar) * activemq-client-5.x.x_1.0.0.jar (OSGi converted Jar) Also, following messages would be shown on the terminal - INFO: Executing 'jar uf <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar -C <absolute_path>/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file <absolute_path>/activemq-client-5.x.x/activemq-client-5.x.x.jar 3. You can find the osgi converted libs in activemq-client-5.x.x folder. You can copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar to {WSO2SIHome}/lib and activemq-client-5.x.x/activemq-client-5.x.x.jar to {WSO2SIHome}/samples/sample-clients/lib .","title":"Note:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#executing-the-sample","text":"Navigate to {apache-activemq-5.x.x} unzipped directory and start ActiveMQ server node using bin/activemq start . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: ReceiveJMSInKeyvalueFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/jms-producer and run the following command. ant -Dtype='keyvalue'","title":"Testing the Sample:"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#viewing-the-results","text":"Messages similar to the following would be shown on the editor console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInKeyvalueFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false} @App:name('ReceiveJMSInKeyvalueFormat') @App:description('Receive events via JMS provider in Keyvalue format with default mapping and view the output on the console.') @source(type='jms', factory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory', provider.url='tcp://localhost:61616', destination='jms_result_topic', connection.factory.type='topic', connection.factory.jndi.name='TopicConnectionFactory', @map(type='keyvalue')) define stream SweetProductionStream(name string, amount double); @sink(type='log') define stream LowProductionAlertStream(name string, amount double); @info(name='EventsPassthroughQuery') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveKafkaInBinaryFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Kafka transport in Binary Format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 The following steps must be executed to enable WSO2 SI to receive events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding. Download the Kafka broker from here: https://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz. Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {WSO2SIHome}/libs directory as follows. Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Create another directory named {Destination} in a preferred location in your machine. To convert all the Kafka jars you copied into the {Source} directory, issue the following command, For Windows: {WSO2SIHome}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path> For Linux: sh {WSO2SIHome}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path> Add the OSGI converted kafka libs from {Destination} directory to {WSO2SIHome}/lib . Add the original Kafka libs from {Source} to {WSO2SIHome}/samples/sample-clients/lib . Navigate to {KafkaHome} and start zookeeper node using following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start Kafka server node using following command. sh bin/kafka-server-start.sh config/server.properties Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveKafkaInBinaryFormat.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run \"ant\" command as follows: ant -DnoOfEventsToSend=5 -DtopicName=kafka_sample_topic -DisBinaryMessage=true Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInBinaryFormat : LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false} Note: \u00b6 Stop this Siddhi application, once you are done with the execution. Stop Kafka server and Zookeeper server individually by executing Ctrl+C. @App:name(\"ReceiveKafkaInBinaryFormat\") @App:description('Receive events via Kafka transport in Binary format and view the output on the console') @source(type='kafka', topic.list='kafka_sample_topic', partition.no.list='0', threading.option='single.thread', group.id='group', is.binary.message='true', bootstrap.servers='localhost:9092', @map(type='binary')) define stream SweetProductionStream(id string, amount double); @sink(type='log') define stream LowProductionAlertStream(id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Binary Events via Kafka"},{"location":"samples/ReceiveKafkaInBinaryFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Kafka transport in Binary Format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#prerequisites","text":"The following steps must be executed to enable WSO2 SI to receive events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding. Download the Kafka broker from here: https://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz. Convert and copy the Kafka client jars from the {KafkaHome}/libs directory to the {WSO2SIHome}/libs directory as follows. Create a directory named {Source} in a preferred location in your machine and copy the following JARs to it from the {KafkaHome}/libs directory. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Create another directory named {Destination} in a preferred location in your machine. To convert all the Kafka jars you copied into the {Source} directory, issue the following command, For Windows: {WSO2SIHome}/bin/jartobundle.bat <{Source} Directory Path> <{Destination} Directory Path> For Linux: sh {WSO2SIHome}/bin/jartobundle.sh <{Source} Directory Path> <{Destination} Directory Path> Add the OSGI converted kafka libs from {Destination} directory to {WSO2SIHome}/lib . Add the original Kafka libs from {Source} to {WSO2SIHome}/samples/sample-clients/lib . Navigate to {KafkaHome} and start zookeeper node using following command. sh bin/zookeeper-server-start.sh config/zookeeper.properties Navigate to {KafkaHome} and start Kafka server node using following command. sh bin/kafka-server-start.sh config/server.properties Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveKafkaInBinaryFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start.","title":"Notes:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run \"ant\" command as follows: ant -DnoOfEventsToSend=5 -DtopicName=kafka_sample_topic -DisBinaryMessage=true","title":"Testing the Sample:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInBinaryFormat : LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/ReceiveKafkaInBinaryFormat/#note","text":"Stop this Siddhi application, once you are done with the execution. Stop Kafka server and Zookeeper server individually by executing Ctrl+C. @App:name(\"ReceiveKafkaInBinaryFormat\") @App:description('Receive events via Kafka transport in Binary format and view the output on the console') @source(type='kafka', topic.list='kafka_sample_topic', partition.no.list='0', threading.option='single.thread', group.id='group', is.binary.message='true', bootstrap.servers='localhost:9092', @map(type='binary')) define stream SweetProductionStream(id string, amount double); @sink(type='log') define stream LowProductionAlertStream(id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Note:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via Kafka transport in Text format using custom mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Setup Kafka. Kafka libs to be added and converted to OSGI from {KafkaHome}/libs are as follows. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Add the OSGI converted kafka libs to {WSO2SIHome}/lib . Add the kafka libs to {WSO2SIHome}/samples/sample-clients/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App PublishKafkaInJsonFormat successfully deployed. Note: \u00b6 To convert Kafka libs to OSGI, 1. Create a folder (eg: kafka) and copy Kafka libs to be added from {KafkaHome}/libs . 2. Create another folder (eg: kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to {WSO2SIHome}/bin and issue the following command. - For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> - For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> 4. If converted successfully then for each lib, following messages would be shown on the terminal. - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to {WSO2SIHome}/lib . Executing the Sample: \u00b6 Navigate to {KafkaHome} and start zookeeper node using bin/zookeeper-server-start.sh config/zookeeper.properties . Navigate to {KafkaHome} and start kafka server node using bin/kafka-server-start.sh config/server.properties . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - ReceiveKafkaInTextFormatWithCustomMapping.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : b8642491e78c5a13 - Adding partition 0 for topic: kafka_sample_topic - Adding partitions [0] for topic: kafka_sample_topic - Subscribed for topics: [kafka_sample_topic] - Kafka Consumer thread starting to listen on topic/s: [kafka_sample_topic] with partition/s: [0] - Discovered coordinator 10.100.7.56:9092 (id: 2147483647 rack: null) for group group Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run ant command as follows: ant -Dtype=text -DcustomMapping=true Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInTextFormatWithCustomMapping: LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false} Note: \u00b6 Stop this Siddhi application, once you are done with the execution. Stop Kafka server and Zookeeper server individually by executing Ctrl+C. @App:name(\"ReceiveKafkaInTextFormatWithCustomMapping\") @App:description('Receive events via Kafka transport in Text format with custom mapping and view the output on the console') @source(type='kafka', topic.list='kafka_sample_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='localhost:9092', @map(type='text',fail.on.missing.attribute='true', regex.A='(id):(.*)', regex.B='(amount):([-.0-9]+)', @attributes(id = 'A[2]', amount = 'B[2]'))) define stream SweetProductionStream(id string, amount double); @sink(type='log') define stream LowProductionAlertStream(id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Custom Text Events via Kafka"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via Kafka transport in Text format using custom mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#prerequisites","text":"Setup Kafka. Kafka libs to be added and converted to OSGI from {KafkaHome}/libs are as follows. kafka_2.11-0.10.0.0.jar kafka-clients-0.10.0.0.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar zkclient-0.8.jar zookeeper-3.4.6.jar Add the OSGI converted kafka libs to {WSO2SIHome}/lib . Add the kafka libs to {WSO2SIHome}/samples/sample-clients/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App PublishKafkaInJsonFormat successfully deployed.","title":"Prerequisites:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#note","text":"To convert Kafka libs to OSGI, 1. Create a folder (eg: kafka) and copy Kafka libs to be added from {KafkaHome}/libs . 2. Create another folder (eg: kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to {WSO2SIHome}/bin and issue the following command. - For Linux: ./jartobundle.sh <path/kafka> <path/kafka-osgi> - For Windows: ./jartobundle.bat <path/kafka> <path/kafka-osgi> 4. If converted successfully then for each lib, following messages would be shown on the terminal. - INFO: Created the OSGi bundle <kafka-lib-name>.jar for JAR file <absolute_path>/kafka/<kafka-lib-name>.jar 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to {WSO2SIHome}/lib .","title":"Note:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#executing-the-sample","text":"Navigate to {KafkaHome} and start zookeeper node using bin/zookeeper-server-start.sh config/zookeeper.properties . Navigate to {KafkaHome} and start kafka server node using bin/kafka-server-start.sh config/server.properties . Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: - ReceiveKafkaInTextFormatWithCustomMapping.siddhi - Started Successfully! - Kafka version : 0.10.0.0 - Kafka commitId : b8642491e78c5a13 - Adding partition 0 for topic: kafka_sample_topic - Adding partitions [0] for topic: kafka_sample_topic - Subscribed for topics: [kafka_sample_topic] - Kafka Consumer thread starting to listen on topic/s: [kafka_sample_topic] with partition/s: [0] - Discovered coordinator 10.100.7.56:9092 (id: 2147483647 rack: null) for group group","title":"Executing the Sample:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/kafka-producer and run ant command as follows: ant -Dtype=text -DcustomMapping=true","title":"Testing the Sample:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#viewing-the-results","text":"Messages similar to the following would be shown on the console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInTextFormatWithCustomMapping: LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#note_1","text":"Stop this Siddhi application, once you are done with the execution. Stop Kafka server and Zookeeper server individually by executing Ctrl+C. @App:name(\"ReceiveKafkaInTextFormatWithCustomMapping\") @App:description('Receive events via Kafka transport in Text format with custom mapping and view the output on the console') @source(type='kafka', topic.list='kafka_sample_topic', partition.no.list='0', threading.option='single.thread', group.id=\"group\", bootstrap.servers='localhost:9092', @map(type='text',fail.on.missing.attribute='true', regex.A='(id):(.*)', regex.B='(amount):([-.0-9]+)', @attributes(id = 'A[2]', amount = 'B[2]'))) define stream SweetProductionStream(id string, amount double); @sink(type='log') define stream LowProductionAlertStream(id string, amount double); @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Note:"},{"location":"samples/ReceiveMQTTInXMLFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via MQTT transport in XML format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. \" Siddhi App ReceiveMQTTInXMLFormat successfully deployed \" message would be shown in the console. Before running this MQTT sample, set up mosquitto server which supports MQTT. This can be done by the following commands: sudo apt-get update sudo apt-get install mosquitto Install mosquitto client packages by executing following command. sudo apt-get install mosquitto-clients After the set up ,start the mosquitto server by running the following command. sudo service mosquitto start Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run', the following messages would be shown on the console. ReceiveMQTTInXMLFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Option 1: Publish events with the command line publisher: \u00b6 Open a terminal and publish events using following command. (The values for name and amount attributes can be changed as desired). mosquitto_pub -t 'mqtt_topic_input' -m '<events><event><name>sugar</name><amount>300.0</amount></event></events>' Option 2: Publish events with mqtt sample client: \u00b6 Open a terminal and navigate to <WSO2_SI_HOME>/samples/sample-clients/mqtt-client . Run the following command in the terminal: ant If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5 Viewing the Results: \u00b6 See the output. Following message would be shown on the console. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false} Note: \u00b6 Stop this Siddhi application. Stop the mosquitto server using following command once you are done with the execution. sudo service mosquitto stop @App:name(\"ReceiveMQTTInXMLFormat\") @App:description('Receive events via MQTT transport in XML format and view the output on the console.') @source(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic_input', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving XML events via MQTT"},{"location":"samples/ReceiveMQTTInXMLFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via MQTT transport in XML format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveMQTTInXMLFormat/#prerequisites","text":"Save this sample. \" Siddhi App ReceiveMQTTInXMLFormat successfully deployed \" message would be shown in the console. Before running this MQTT sample, set up mosquitto server which supports MQTT. This can be done by the following commands: sudo apt-get update sudo apt-get install mosquitto Install mosquitto client packages by executing following command. sudo apt-get install mosquitto-clients After the set up ,start the mosquitto server by running the following command. sudo service mosquitto start","title":"Prerequisites:"},{"location":"samples/ReceiveMQTTInXMLFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run', the following messages would be shown on the console. ReceiveMQTTInXMLFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveMQTTInXMLFormat/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveMQTTInXMLFormat/#option-1-publish-events-with-the-command-line-publisher","text":"Open a terminal and publish events using following command. (The values for name and amount attributes can be changed as desired). mosquitto_pub -t 'mqtt_topic_input' -m '<events><event><name>sugar</name><amount>300.0</amount></event></events>'","title":"Option 1: Publish events with the command line publisher:"},{"location":"samples/ReceiveMQTTInXMLFormat/#option-2-publish-events-with-mqtt-sample-client","text":"Open a terminal and navigate to <WSO2_SI_HOME>/samples/sample-clients/mqtt-client . Run the following command in the terminal: ant If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5","title":"Option 2: Publish events with mqtt sample client:"},{"location":"samples/ReceiveMQTTInXMLFormat/#viewing-the-results","text":"See the output. Following message would be shown on the console. ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/ReceiveMQTTInXMLFormat/#note","text":"Stop this Siddhi application. Stop the mosquitto server using following command once you are done with the execution. sudo service mosquitto stop @App:name(\"ReceiveMQTTInXMLFormat\") @App:description('Receive events via MQTT transport in XML format and view the output on the console.') @source(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic_input', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Note:"},{"location":"samples/ReceivePrometheusMetrics/","text":"Purpose: \u00b6 This application demonstrates how to use prometheus-source to retrieve Prometheus metrics that are exported at an HTTP endpoint. Pre-requisites: 1. The following steps must be executed to enable WSO2 SP to publish and retrieve events via Prometheus. 1. Download and copy the prometheus client jars to the {WSO2SIHome}/lib directory as follows. 1. Download the following jars from https://mvnrepository.com/artifact/io.prometheus and copy them to {WSO2SIHome}/lib directory. * simpleclient_common-0.5.0.jar * simpleclient-0.5.0.jar * simpleclient_httpserver-0.5.0.jar * simpleclient_pushgateway-0.5.0.jar 2. Start the editor WSO2 SP by giving this command in the terminal : sh editor.sh 3. Save this sample \"Siddhi App EnergyAlertApp successfully deployed\" message would be shown in the console 4. Navigate to {WSO2SIHome}/samples/sample-clients/prometheus-client and run \"ant\" command as follows: ant Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console ReceivePrometheusMetrics.siddhi - Started Successfully! PowerConsumptionStream has successfully connected at http://localhost:9080 Note: \u00b6 If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again. Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server001, F3Room2, **, **], isExpired=false} - INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server002, F2Room2, **, **], isExpired=false} @App:name(\"EnergyAlertApp\") @App:description(\"Use siddhi-io-prometheus retrieve and analyse Prometheus metrics in Siddhi\") @source(type='prometheus' , target.url='http://localhost:9080/metrics',metric.type='counter', metric.name='total_device_power_consumption_WATTS', scrape.interval='5', @map(type='keyvalue')) define stream devicePowerStream(deviceID string, roomID string, value int); @sink(type='log', priority='WARN', prefix ='High power consumption') define stream AlertStream (deviceID string, roomID string, initialPower int, finalPower int); @sink(type='log', priority='WARN', prefix ='Logging purpose') define stream LogStream (deviceID string, roomID string, power int); @info(name='power increase pattern') from every( e1=devicePowerStream ) -> e2=devicePowerStream[ e1.deviceID == deviceID and (e1.value + 5) <= value] within 1 min select e1.deviceID, e1.roomID, e1.value as initialPower, e2.value as finalPower insert into AlertStream;","title":"Receiving Prometheus Metrics"},{"location":"samples/ReceivePrometheusMetrics/#purpose","text":"This application demonstrates how to use prometheus-source to retrieve Prometheus metrics that are exported at an HTTP endpoint. Pre-requisites: 1. The following steps must be executed to enable WSO2 SP to publish and retrieve events via Prometheus. 1. Download and copy the prometheus client jars to the {WSO2SIHome}/lib directory as follows. 1. Download the following jars from https://mvnrepository.com/artifact/io.prometheus and copy them to {WSO2SIHome}/lib directory. * simpleclient_common-0.5.0.jar * simpleclient-0.5.0.jar * simpleclient_httpserver-0.5.0.jar * simpleclient_pushgateway-0.5.0.jar 2. Start the editor WSO2 SP by giving this command in the terminal : sh editor.sh 3. Save this sample \"Siddhi App EnergyAlertApp successfully deployed\" message would be shown in the console 4. Navigate to {WSO2SIHome}/samples/sample-clients/prometheus-client and run \"ant\" command as follows: ant","title":"Purpose:"},{"location":"samples/ReceivePrometheusMetrics/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console ReceivePrometheusMetrics.siddhi - Started Successfully! PowerConsumptionStream has successfully connected at http://localhost:9080","title":"Executing the Sample:"},{"location":"samples/ReceivePrometheusMetrics/#note","text":"If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.","title":"Note:"},{"location":"samples/ReceivePrometheusMetrics/#viewing-the-results","text":"Messages similar to the following would be shown on the console. - INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server001, F3Room2, **, **], isExpired=false} - INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server002, F2Room2, **, **], isExpired=false} @App:name(\"EnergyAlertApp\") @App:description(\"Use siddhi-io-prometheus retrieve and analyse Prometheus metrics in Siddhi\") @source(type='prometheus' , target.url='http://localhost:9080/metrics',metric.type='counter', metric.name='total_device_power_consumption_WATTS', scrape.interval='5', @map(type='keyvalue')) define stream devicePowerStream(deviceID string, roomID string, value int); @sink(type='log', priority='WARN', prefix ='High power consumption') define stream AlertStream (deviceID string, roomID string, initialPower int, finalPower int); @sink(type='log', priority='WARN', prefix ='Logging purpose') define stream LogStream (deviceID string, roomID string, power int); @info(name='power increase pattern') from every( e1=devicePowerStream ) -> e2=devicePowerStream[ e1.deviceID == deviceID and (e1.value + 5) <= value] within 1 min select e1.deviceID, e1.roomID, e1.value as initialPower, e2.value as finalPower insert into AlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via rabbitmq broker in JSON format using default mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Install the rabbitmq server using the following command sudo apt-get install rabbitmq-server. To enable RabbitMQ Management Console, run the following: sudo rabbitmq-plugins enable rabbitmq_management. To start the service, issue the following command: invoke-rc.d rabbitmq-server start Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveRabbitmqInJSONFormat.siddhi - Started Successfully! Check whether the exchange 'rabbitmq_sample' is created in the rabbitmq server or not. To check that you can visit http://localhost:15672/. Testing the Sample: \u00b6 Publish events with rabbitmq sample publisher: \u00b6 Open a terminal and issue command from the {WSO2SIHome}/samples/sample-clients/rabbitmq-producer and run ant command. If you want to publish custom number of events, you need to run ant command as follows: ant -DnoOfEventsToPublish=5 Viewing the Results: \u00b6 See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233900122, data=[Lollipop, 6186.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233901122, data=[Donut, 7904.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233902124, data=[Honeycomb, 4495.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233903125, data=[Donut, 1393.0], isExpired=false} @App:name(\"ReceiveRabbitmqInJSONFormat\") @app:description(\"Receives the events from the rabbitmq broker using the AMQP protocol.\") @source(type='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'rabbitmq_sample', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving JSON Events via RabbitMQ"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via rabbitmq broker in JSON format using default mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#prerequisites","text":"Save this sample. Install the rabbitmq server using the following command sudo apt-get install rabbitmq-server. To enable RabbitMQ Management Console, run the following: sudo rabbitmq-plugins enable rabbitmq_management. To start the service, issue the following command: invoke-rc.d rabbitmq-server start","title":"Prerequisites:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveRabbitmqInJSONFormat.siddhi - Started Successfully! Check whether the exchange 'rabbitmq_sample' is created in the rabbitmq server or not. To check that you can visit http://localhost:15672/.","title":"Executing the Sample:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#testing-the-sample","text":"","title":"Testing the Sample:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#publish-events-with-rabbitmq-sample-publisher","text":"Open a terminal and issue command from the {WSO2SIHome}/samples/sample-clients/rabbitmq-producer and run ant command. If you want to publish custom number of events, you need to run ant command as follows: ant -DnoOfEventsToPublish=5","title":"Publish events with rabbitmq sample publisher:"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#viewing-the-results","text":"See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233900122, data=[Lollipop, 6186.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233901122, data=[Donut, 7904.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233902124, data=[Honeycomb, 4495.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233903125, data=[Donut, 1393.0], isExpired=false} @App:name(\"ReceiveRabbitmqInJSONFormat\") @app:description(\"Receives the events from the rabbitmq broker using the AMQP protocol.\") @source(type='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'rabbitmq_sample', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via TCP transport in Text default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPInTextFormatWithCustomMapping.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=text -DcustomMapping=true If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=text -DcustomMapping=true -DnoOfEventsToSend=5 Notes: \u00b6 If the message Source Listener has created for url tcp://localhost:9892/SweetProductionStream does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Viewing the Results: \u00b6 See the output. Following messages would be shown on the console continuousely. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971370311, data=[\"Eclair\", 132.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971371299, data=[\"Ice\", 6733.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971372300, data=[\"Lollipop\", 742.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971373300, data=[\"Marshmallow\", 8781.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971374301, data=[\"Marshmallow\", 5105.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971375301, data=[\"Froyo\", 5531.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971376301, data=[\"Gingerbread\", 2193.0], isExpired=false} @App:name(\"ReceiveTCPInTextFormatWithCustomMapping\") @App:description('Receive events via TCP transport in text format with custom mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='((?<=id:)(.*)(?= ))',regex.B='([-0-9]+)', @attributes(name ='A',amount= 'B'))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Custom Text Events via TCP"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via TCP transport in Text default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPInTextFormatWithCustomMapping.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=text -DcustomMapping=true If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=text -DcustomMapping=true -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#notes","text":"If the message Source Listener has created for url tcp://localhost:9892/SweetProductionStream does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#viewing-the-results","text":"See the output. Following messages would be shown on the console continuousely. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971370311, data=[\"Eclair\", 132.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971371299, data=[\"Ice\", 6733.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971372300, data=[\"Lollipop\", 742.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971373300, data=[\"Marshmallow\", 8781.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971374301, data=[\"Marshmallow\", 5105.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971375301, data=[\"Froyo\", 5531.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971376301, data=[\"Gingerbread\", 2193.0], isExpired=false} @App:name(\"ReceiveTCPInTextFormatWithCustomMapping\") @App:description('Receive events via TCP transport in text format with custom mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='text', fail.on.unknown.attribute = 'true', regex.A='((?<=id:)(.*)(?= ))',regex.B='([-0-9]+)', @attributes(name ='A',amount= 'B'))) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveTCPinBinaryFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinBinaryFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=binary If you want to publish custom number of events, you need to run ant command as follows: ant -Dtype=binary -DnoOfEventsToSend=5 Notes: \u00b6 If the message Source Listener has created for url tcp://localhost:9892/SweetProductionStream does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Viewing the Results: \u00b6 [2017-12-11 15:55:12,682] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987912665, data=[Cupcake, 10.550546580727683], isExpired=false} [2017-12-11 15:55:13,671] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987913669, data=[Donut, 98.45360926759642], isExpired=false} [2017-12-11 15:55:14,672] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987914670, data=[Eclair, 48.77465755572478], isExpired=false} [2017-12-11 15:55:15,672] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987915671, data=[Froyo, 28.209321491656706], isExpired=false} [2017-12-11 15:55:16,673] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987916671, data=[Gingerbread, 110.46772110205492], isExpired=false} @App:name(\"ReceiveTCPinBinaryFormat\") @App:description('Receive events via TCP transport in binary format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Binary Events via TCP"},{"location":"samples/ReceiveTCPinBinaryFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveTCPinBinaryFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveTCPinBinaryFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinBinaryFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveTCPinBinaryFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: Run ant command in the terminal as follows: ant -Dtype=binary If you want to publish custom number of events, you need to run ant command as follows: ant -Dtype=binary -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/ReceiveTCPinBinaryFormat/#notes","text":"If the message Source Listener has created for url tcp://localhost:9892/SweetProductionStream does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveTCPinBinaryFormat/#viewing-the-results","text":"[2017-12-11 15:55:12,682] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987912665, data=[Cupcake, 10.550546580727683], isExpired=false} [2017-12-11 15:55:13,671] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987913669, data=[Donut, 98.45360926759642], isExpired=false} [2017-12-11 15:55:14,672] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987914670, data=[Eclair, 48.77465755572478], isExpired=false} [2017-12-11 15:55:15,672] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987915671, data=[Froyo, 28.209321491656706], isExpired=false} [2017-12-11 15:55:16,673] INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987916671, data=[Gingerbread, 110.46772110205492], isExpired=false} @App:name(\"ReceiveTCPinBinaryFormat\") @App:description('Receive events via TCP transport in binary format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveTCPinJSONFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in JSON format using default mapping and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinJSONFormat - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: ant -Dtype=application/json If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=application/json -DnoOfEventsToSend=5 Notes: \u00b6 If the message Tcp Server started in 0.0.0.0:9892 does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Viewing the Results: \u00b6 See the output. Following messages would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049050858, data=[Gingerbread, 6664.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049051858, data=[Cream Sandwich, 6190.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049052859, data=[Gingerbread, 9725.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049053859, data=[Donut, 7777.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049054860, data=[Honeycomb, 8818.0], isExpired=false} @App:name(\"ReceiveTCPinJSONFormat\") @App:description('Receive events via TCP transport in JSON format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving JSON Events via TCP"},{"location":"samples/ReceiveTCPinJSONFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in JSON format using default mapping and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveTCPinJSONFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveTCPinJSONFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinJSONFormat - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveTCPinJSONFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: ant -Dtype=application/json If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=application/json -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/ReceiveTCPinJSONFormat/#notes","text":"If the message Tcp Server started in 0.0.0.0:9892 does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveTCPinJSONFormat/#viewing-the-results","text":"See the output. Following messages would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049050858, data=[Gingerbread, 6664.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049051858, data=[Cream Sandwich, 6190.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049052859, data=[Gingerbread, 9725.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049053859, data=[Donut, 7777.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049054860, data=[Honeycomb, 8818.0], isExpired=false} @App:name(\"ReceiveTCPinJSONFormat\") @App:description('Receive events via TCP transport in JSON format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveTCPinTextFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in text default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinTextFormat.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: ant -Dtype=text If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=text -DnoOfEventsToSend=5 Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console. Viewing the Results: \u00b6 See the output. Following messages would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990726372, data=[Eclair, 2171.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990727362, data=[Froyo, 1155.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990728363, data=[Gingerbread, 8840.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990729363, data=[Marshmallow, 7400.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990730364, data=[Cupcake, 889.0], isExpired=false} @App:name(\"ReceiveTCPinTextFormat\") @App:description('Receive events via TCP transport in text format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='text')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving Text Events via TCP"},{"location":"samples/ReceiveTCPinTextFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in text default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveTCPinTextFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveTCPinTextFormat/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * ReceiveTCPinTextFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveTCPinTextFormat/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run ant command as follows: ant -Dtype=text If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=text -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/ReceiveTCPinTextFormat/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.","title":"Notes:"},{"location":"samples/ReceiveTCPinTextFormat/#viewing-the-results","text":"See the output. Following messages would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990726372, data=[Eclair, 2171.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990727362, data=[Froyo, 1155.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990728363, data=[Gingerbread, 8840.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990729363, data=[Marshmallow, 7400.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990730364, data=[Cupcake, 889.0], isExpired=false} @App:name(\"ReceiveTCPinTextFormat\") @App:description('Receive events via TCP transport in text format with default mapping and view the output on the console.') @Source(type = 'tcp', context='SweetProductionStream', @map(type='text')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProductionAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/ReceiveWebSocketInXMLFormat/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via WebSocket transport in XML default format and log the events in LowProductionAlertStream to the output console. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/websocket-producer and run ant command as follows: Run ant command in the terminal. ant If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveWebSocketInXMLFormat.siddhi - Started Successfully! Viewing the Results: \u00b6 See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985540005, data=[Honeycomb, 2700.3555330804284], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985541009, data=[Froyo, 4195.429933118964], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985542006, data=[Donut, 9625.837679695496], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985543008, data=[Froyo, 1909.568113992198], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985544012, data=[Lollipop, 291.8985351086241], isExpired=false} @App:name(\"ReceiveWebSocketInXMLFormat\") @App:description('Receive events via WebSocket transport in XML format and view the output on the console.') @Source(type = 'websocket', url='ws://localhost:8025/abc', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Receiving XML Events via Websocket"},{"location":"samples/ReceiveWebSocketInXMLFormat/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via WebSocket transport in XML default format and log the events in LowProductionAlertStream to the output console.","title":"Purpose:"},{"location":"samples/ReceiveWebSocketInXMLFormat/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/ReceiveWebSocketInXMLFormat/#executing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/websocket-producer and run ant command as follows: Run ant command in the terminal. ant If you want to publish custom number of events, you need to run ant command as follows. ant -DnoOfEventsToSend=5 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * ReceiveWebSocketInXMLFormat.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/ReceiveWebSocketInXMLFormat/#viewing-the-results","text":"See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985540005, data=[Honeycomb, 2700.3555330804284], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985541009, data=[Froyo, 4195.429933118964], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985542006, data=[Donut, 9625.837679695496], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985543008, data=[Froyo, 1909.568113992198], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985544012, data=[Lollipop, 291.8985351086241], isExpired=false} @App:name(\"ReceiveWebSocketInXMLFormat\") @App:description('Receive events via WebSocket transport in XML format and view the output on the console.') @Source(type = 'websocket', url='ws://localhost:8025/abc', @map(type='xml')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream LowProductionAlertStream (name string, amount double); -- passthrough data in the SweetProductionStream into LowProducitonAlertStream @info(name='query1') from SweetProductionStream select * insert into LowProductionAlertStream;","title":"Viewing the Results:"},{"location":"samples/RegexExecutionSample/","text":"Purpose: \u00b6 This function attempts to find the next sub-sequence of the inputSequence that matches the regex pattern. It returns true if such a sub sequence exists, or returns false otherwise Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App RegexExecutionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * RegexExecutionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name : RegexExecutionSample Stream Name : SweetProductionStream Enter following values in the fields and send, name: chocolate cake startingIndex: 0 Enter following values in the fields and send name: coffee cake startingIndex: 0 Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759840093, data=[chocolate cake, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759907324, data=[coffee cake, false], isExpired=false} @App:name(\"RegexExecutionSample\") @App:description('Finds if a sweet is a chocolate product.') define stream SweetProductionStream (name string, startingIndex int); @sink(type='log') define stream ChocolateProductStream(name string, isAChocolateProduct bool); from SweetProductionStream select name, regex:find(\"^chocolate(\\s*[a-zA-Z]+)\", str:lower(name), startingIndex) as isAChocolateProduct insert into ChocolateProductStream;","title":"Identifying Sub-sequences in Input Sequences"},{"location":"samples/RegexExecutionSample/#purpose","text":"This function attempts to find the next sub-sequence of the inputSequence that matches the regex pattern. It returns true if such a sub sequence exists, or returns false otherwise","title":"Purpose:"},{"location":"samples/RegexExecutionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App RegexExecutionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/RegexExecutionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * RegexExecutionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/RegexExecutionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name : RegexExecutionSample Stream Name : SweetProductionStream Enter following values in the fields and send, name: chocolate cake startingIndex: 0 Enter following values in the fields and send name: coffee cake startingIndex: 0","title":"Testing the Sample:"},{"location":"samples/RegexExecutionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759840093, data=[chocolate cake, true], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759907324, data=[coffee cake, false], isExpired=false} @App:name(\"RegexExecutionSample\") @App:description('Finds if a sweet is a chocolate product.') define stream SweetProductionStream (name string, startingIndex int); @sink(type='log') define stream ChocolateProductStream(name string, isAChocolateProduct bool); from SweetProductionStream select name, regex:find(\"^chocolate(\\s*[a-zA-Z]+)\", str:lower(name), startingIndex) as isAChocolateProduct insert into ChocolateProductStream;","title":"Viewing the Results:"},{"location":"samples/SNMPGetRequestApp/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to resive snmp source via SNMP in keyvalue using custom mapping. Prerequisites: \u00b6 Save this sample. Install snmp agent on your network node. for linux can install snmpd / for windows it can be configured by 'windows features'. configure snmp agent ex:- community string, port, user access. If there is no syntax error, the following message is shown on the console: SNMP-get-request-app successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SNMP-set-request-app - Started Successfully! Viewing the Results: \u00b6 See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:33.05, mail@wso2.com], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:38.05, mail@wso2.com], isExpired=false} @App:name(\"SNMPGetRequestApp\") @App:description('listening oid status from agent') @source(type ='snmp', @map(type='keyvalue', @attributes(sysUpTime= '1.3.6.1.2.1.1.3.0', sysContact = '1.3.6.1.2.1.1.4.0') ), host ='127.0.0.1', version = 'v1', request.interval = '5000', community = 'public', agent.port = '2019', oids='1.3.6.1.2.1.1.3.0, 1.3.6.1.2.1.1.4.0') define stream inputStream(sysUpTime string, sysContact string); @sink(type='log') define stream logStream (sysUpTime string, sysContact string); -- passthrough data in the inputStream to logStream @info(name='query1') from inputStream select * insert into logStream;","title":"Receiving Custom Key Value Events via SNMP"},{"location":"samples/SNMPGetRequestApp/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to resive snmp source via SNMP in keyvalue using custom mapping.","title":"Purpose:"},{"location":"samples/SNMPGetRequestApp/#prerequisites","text":"Save this sample. Install snmp agent on your network node. for linux can install snmpd / for windows it can be configured by 'windows features'. configure snmp agent ex:- community string, port, user access. If there is no syntax error, the following message is shown on the console: SNMP-get-request-app successfully deployed.","title":"Prerequisites:"},{"location":"samples/SNMPGetRequestApp/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SNMP-set-request-app - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/SNMPGetRequestApp/#viewing-the-results","text":"See the output. Following message would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:33.05, mail@wso2.com], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:38.05, mail@wso2.com], isExpired=false} @App:name(\"SNMPGetRequestApp\") @App:description('listening oid status from agent') @source(type ='snmp', @map(type='keyvalue', @attributes(sysUpTime= '1.3.6.1.2.1.1.3.0', sysContact = '1.3.6.1.2.1.1.4.0') ), host ='127.0.0.1', version = 'v1', request.interval = '5000', community = 'public', agent.port = '2019', oids='1.3.6.1.2.1.1.3.0, 1.3.6.1.2.1.1.4.0') define stream inputStream(sysUpTime string, sysContact string); @sink(type='log') define stream logStream (sysUpTime string, sysContact string); -- passthrough data in the inputStream to logStream @info(name='query1') from inputStream select * insert into logStream;","title":"Viewing the Results:"},{"location":"samples/SNMPSetRequestApp/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send snmp set request via SNMP in keyvalue using custom mapping. Prerequisites: \u00b6 Save this sample. Install snmp agent on your network node. for linux can install snmpd / for windows it can be configured by 'windows features'. configure snmp agent ex:- community string = public, If there is no syntax error, the following message is shown on the console: SNMP-set-request-app successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SNMP-set-request-app - Started Successfully! Testing the Sample: \u00b6 Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select SNMP-set-request-app as 'Siddhi App Name' Select outputStream as 'StreamName' Provide attribute values sysLocation : asia-branch-singapore Click on the start button (Arrow symbol) next to the newly created simulator Viewing the Results: \u00b6 See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-set-request-app : logStream : Event{timestamp=1*********, data=[asia-branch-singapore], isExpired=false} Notes: \u00b6 Make sure the port number is correct and user have write access to agent @App:name(\"SNMPSetRequestApp\") @App:description('setting oids on agent') @Sink(type='snmp', @map(type='keyvalue', @payload('1.3.6.1.2.1.1.6.0' = 'sysLocation')), host = '127.0.0.1', version = 'v1', community = 'public', agent.port = '2019') define stream outputStream(sysLocation string); @sink(type='log') define stream logStream(sysLocation string); @info(name='query_name') from outputStream select sysLocation insert into logStream;","title":"Sending Custom Keyvalue Events via SNMP"},{"location":"samples/SNMPSetRequestApp/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send snmp set request via SNMP in keyvalue using custom mapping.","title":"Purpose:"},{"location":"samples/SNMPSetRequestApp/#prerequisites","text":"Save this sample. Install snmp agent on your network node. for linux can install snmpd / for windows it can be configured by 'windows features'. configure snmp agent ex:- community string = public, If there is no syntax error, the following message is shown on the console: SNMP-set-request-app successfully deployed.","title":"Prerequisites:"},{"location":"samples/SNMPSetRequestApp/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: SNMP-set-request-app - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/SNMPSetRequestApp/#testing-the-sample","text":"Click on 'Event Simulator' (double arrows on left tab) Click 'Single Simulation' (this will be already selected) Select SNMP-set-request-app as 'Siddhi App Name' Select outputStream as 'StreamName' Provide attribute values sysLocation : asia-branch-singapore Click on the start button (Arrow symbol) next to the newly created simulator","title":"Testing the Sample:"},{"location":"samples/SNMPSetRequestApp/#viewing-the-results","text":"See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-set-request-app : logStream : Event{timestamp=1*********, data=[asia-branch-singapore], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/SNMPSetRequestApp/#notes","text":"Make sure the port number is correct and user have write access to agent @App:name(\"SNMPSetRequestApp\") @App:description('setting oids on agent') @Sink(type='snmp', @map(type='keyvalue', @payload('1.3.6.1.2.1.1.6.0' = 'sysLocation')), host = '127.0.0.1', version = 'v1', community = 'public', agent.port = '2019') define stream outputStream(sysLocation string); @sink(type='log') define stream logStream(sysLocation string); @info(name='query_name') from outputStream select sysLocation insert into logStream;","title":"Notes:"},{"location":"samples/Script-js-sample/","text":"Purpose: \u00b6 This sample demonstrate how javascript functions can be used in Siddhi Applications. Prerequisites: \u00b6 Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Script-js-sample.siddhi - Started Successfully! Testing the Sample: \u00b6 To open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: Script-js-sample Stream Name: SweetProductiontream In the name field and amount fields, enter 'toffee', '45.25' respectively and then click Send to send the event. Send some more events. Viewing the Results: \u00b6 See the output on the console. Description of the raw material with the passed details are shown in the logger.You will get the output as follows: [2017-12-20_14-26-03_120] INFO {io.siddhi.core.stream.output.sink.LogSink} - Script-js-sample : logStream : Event{timestamp=1513760163112, data=[toffee, 45.25, There are 45.25kg of toffee in the store], isExpired=false} Notes: If you need to edit this application while it is running, then Save -> Start. @App:name('Script-js-sample') @App:Description('Demonstrate how javascript functions can be used in Siddhi Applications.') define stream sweetProductionStream (name string, amount double); @sink(type='log') define stream logStream (name string, amount double, itemDescription string); define function detailedMaterial[JavaScript] return string { var name = data[0]; var amount = data[1]; var res = \"There are \"+amount+\"kg of \"+name+ \" in the store\"; return res; }; from sweetProductionStream select name , amount, detailedMaterial(name,amount) as itemDescription insert into detailedProductionstream; from detailedProductionstream select * insert into logStream;","title":"Using Javascript Functions in Siddhi Applications"},{"location":"samples/Script-js-sample/#purpose","text":"This sample demonstrate how javascript functions can be used in Siddhi Applications.","title":"Purpose:"},{"location":"samples/Script-js-sample/#prerequisites","text":"Save this sample.","title":"Prerequisites:"},{"location":"samples/Script-js-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Script-js-sample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/Script-js-sample/#testing-the-sample","text":"To open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: Script-js-sample Stream Name: SweetProductiontream In the name field and amount fields, enter 'toffee', '45.25' respectively and then click Send to send the event. Send some more events.","title":"Testing the Sample:"},{"location":"samples/Script-js-sample/#viewing-the-results","text":"See the output on the console. Description of the raw material with the passed details are shown in the logger.You will get the output as follows: [2017-12-20_14-26-03_120] INFO {io.siddhi.core.stream.output.sink.LogSink} - Script-js-sample : logStream : Event{timestamp=1513760163112, data=[toffee, 45.25, There are 45.25kg of toffee in the store], isExpired=false} Notes: If you need to edit this application while it is running, then Save -> Start. @App:name('Script-js-sample') @App:Description('Demonstrate how javascript functions can be used in Siddhi Applications.') define stream sweetProductionStream (name string, amount double); @sink(type='log') define stream logStream (name string, amount double, itemDescription string); define function detailedMaterial[JavaScript] return string { var name = data[0]; var amount = data[1]; var res = \"There are \"+amount+\"kg of \"+name+ \" in the store\"; return res; }; from sweetProductionStream select name , amount, detailedMaterial(name,amount) as itemDescription insert into detailedProductionstream; from detailedProductionstream select * insert into logStream;","title":"Viewing the Results:"},{"location":"samples/Store-cassandra/","text":"Purpose: \u00b6 This application demonstrates how to perform Cassandra operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, update or insert the existing data in the store: Search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exist in the store) Prerequisites: \u00b6 Ensure that Cassandra version 3 or above is installed on your machine. Add the DataStax Java driver into {WSO2_SI_HOME}/lib as follows: Download the DataStax Java driver from: http://central.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.3.2/cassandra-driver-core-3.3.2.jar Use the jartobundle tool in {WSO2_SI_Home}/bin to extract and convert the above JARs into OSGi bundles. For Windows: <SI_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux: <SI_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> Note: The driver given in the above link is a OSGi bundled one. Please skip this step if the jar is already OSGi bunbled. Copy the converted bundles to the {WSO2_SI_Home}/lib directory. Create a keyspace named 'production' in Cassandra store: CREATE KEYSPACE \"production\" WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}; In the store configuration of this application, replace 'username' and 'password' values with your Cassandra credentials. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console. Store-cassandra.siddhi - Started Successfully! Note: \u00b6 If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again. Testing the Sample: \u00b6 Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'Store-cassandra' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'. Provide attribute values, and then click Send. Send at least one event where the name matches a name value in the data you previously inserted into the SweetProductionTable . This will satisfy the 'on' condition of the join query. Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events. Notes: \u00b6 After a change in the store, you can use the search stream to see whether the operation is successful. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"Store-cassandra\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @store(type='cassandra' , cassandra.host='localhost', username='cassandra', password='cassandra',keyspace='production', column.family='SweetProductionTable') define table SweetProductionTable (name string, amount double); /* Inserting event into the cassandra keyspace */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from cassandra keyspace */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from cassandra keyspace */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from cassandra keyspace */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in cassandra keyspace */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Receive Events via Simulator and Persist in Cassandra Store"},{"location":"samples/Store-cassandra/#purpose","text":"This application demonstrates how to perform Cassandra operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, update or insert the existing data in the store: Search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exist in the store)","title":"Purpose:"},{"location":"samples/Store-cassandra/#prerequisites","text":"Ensure that Cassandra version 3 or above is installed on your machine. Add the DataStax Java driver into {WSO2_SI_HOME}/lib as follows: Download the DataStax Java driver from: http://central.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.3.2/cassandra-driver-core-3.3.2.jar Use the jartobundle tool in {WSO2_SI_Home}/bin to extract and convert the above JARs into OSGi bundles. For Windows: <SI_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux: <SI_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> Note: The driver given in the above link is a OSGi bundled one. Please skip this step if the jar is already OSGi bunbled. Copy the converted bundles to the {WSO2_SI_Home}/lib directory. Create a keyspace named 'production' in Cassandra store: CREATE KEYSPACE \"production\" WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}; In the store configuration of this application, replace 'username' and 'password' values with your Cassandra credentials. Save this sample.","title":"Prerequisites:"},{"location":"samples/Store-cassandra/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console. Store-cassandra.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/Store-cassandra/#note","text":"If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.","title":"Note:"},{"location":"samples/Store-cassandra/#testing-the-sample","text":"Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'Store-cassandra' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'. Provide attribute values, and then click Send. Send at least one event where the name matches a name value in the data you previously inserted into the SweetProductionTable . This will satisfy the 'on' condition of the join query. Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events.","title":"Testing the Sample:"},{"location":"samples/Store-cassandra/#notes","text":"After a change in the store, you can use the search stream to see whether the operation is successful. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Notes:"},{"location":"samples/Store-cassandra/#viewing-the-results","text":"See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"Store-cassandra\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @store(type='cassandra' , cassandra.host='localhost', username='cassandra', password='cassandra',keyspace='production', column.family='SweetProductionTable') define table SweetProductionTable (name string, amount double); /* Inserting event into the cassandra keyspace */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from cassandra keyspace */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from cassandra keyspace */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from cassandra keyspace */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in cassandra keyspace */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Viewing the Results:"},{"location":"samples/Store-rdbms/","text":"Purpose: \u00b6 This application demonstrates how to perform RDBMS operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store: * search - searchSweetProductionStream * insert - insertSweetProductionStream * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exist in the store) Prerequisites: \u00b6 Ensure that MySQL is installed on your machine. Add the MySQL JDBC driver into {WSO2_SI_HOME}/lib as follows: Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz Unzip the archive. Copy mysql-connector-java-5.1.45-bin.jar to {WSO2_SI_Home}/lib directory. Create a database named production in MySQL. This database is referred to with the jdbc:mysql://localhost:3306/production url. Create a table named SweetProductionTable as follows. CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Insert some values into the table as follows. INSERT INTO SweetProductionTable VALUES ('Sugar',23.50); Note: You can also use insertSweetProductionStream for this. In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console. Store-rdbms.siddhi - Started Successfully! Note: \u00b6 If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again. Testing the Sample: \u00b6 Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'Store-rdbms' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'. Provide attribute values, and then click Send. Send at least one event where the name matches a name value in the data you previously inserted into the SweetProductionTable. This will satisfy the 'on' condition of the join query. Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events. Notes: \u00b6 After a change in the store, you can use the search stream to see whether the operation is successful. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"Store-rdbms\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2\", password=\"123\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") --@PrimaryKey(\"name\") @index(\"amount\") define table SweetProductionTable (name string, amount double); /* Inserting event into the mysql database */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from mysql database */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from mysql database */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from mysql database */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in mysql database */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Receiving Events via Simulator and Persisting in RDBMS Store"},{"location":"samples/Store-rdbms/#purpose","text":"This application demonstrates how to perform RDBMS operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store: * search - searchSweetProductionStream * insert - insertSweetProductionStream * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exist in the store)","title":"Purpose:"},{"location":"samples/Store-rdbms/#prerequisites","text":"Ensure that MySQL is installed on your machine. Add the MySQL JDBC driver into {WSO2_SI_HOME}/lib as follows: Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz Unzip the archive. Copy mysql-connector-java-5.1.45-bin.jar to {WSO2_SI_Home}/lib directory. Create a database named production in MySQL. This database is referred to with the jdbc:mysql://localhost:3306/production url. Create a table named SweetProductionTable as follows. CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2)); Insert some values into the table as follows. INSERT INTO SweetProductionTable VALUES ('Sugar',23.50); Note: You can also use insertSweetProductionStream for this. In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials. Save this sample.","title":"Prerequisites:"},{"location":"samples/Store-rdbms/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console. Store-rdbms.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/Store-rdbms/#note","text":"If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.","title":"Note:"},{"location":"samples/Store-rdbms/#testing-the-sample","text":"Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'Store-rdbms' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'. Provide attribute values, and then click Send. Send at least one event where the name matches a name value in the data you previously inserted into the SweetProductionTable. This will satisfy the 'on' condition of the join query. Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events.","title":"Testing the Sample:"},{"location":"samples/Store-rdbms/#notes","text":"After a change in the store, you can use the search stream to see whether the operation is successful. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Notes:"},{"location":"samples/Store-rdbms/#viewing-the-results","text":"See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"Store-rdbms\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\", username=\"wso2\", password=\"123\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") --@PrimaryKey(\"name\") @index(\"amount\") define table SweetProductionTable (name string, amount double); /* Inserting event into the mysql database */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from mysql database */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from mysql database */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from mysql database */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in mysql database */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Viewing the Results:"},{"location":"samples/Store-redis/","text":"Purpose: \u00b6 This application demonstrates how to perform CRUD operations using Siddhi queries in Redis stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store. Search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store). Prerequisites: \u00b6 Download the Redis from https://redis.io/. Download redis java client 'Jedis' jar (>2.7.0) from https://mvnrepository.com/artifact/redis.clients/jedis and place in <SI_HOME>/lib folder. In redis.conf , provide the requirepass as root. Start redis by redis-server . Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Store-redis.siddhi - Started Successfully! Testing the Sample: \u00b6 Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'Store-redis' as 'Siddhi App Name' -> Select 'searchSweetProductionStream' as 'Stream Name' -> Provide attribute values -> Send. Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query. Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events. After a change in the store, using the search stream the developer can see whether the operation is successful. Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for row materials on the console. Inserted, deleted, updated events can be checked by searchSweetProductionStream. @App:name(\"Store-redis\") @App:description(\"Receive events via simulator and received data are persisted in store.\") define stream insertSweetProductionStream (symbol string, price float, volume long); define stream deleteSweetProductionStream (symbol string, price float, volume long); define stream searchSweetProductionStream(symbol string); define stream updateOrInsertSweetProductionStream(symbol string, price float, volume long); define stream updateSweetProductionStream (symbol string, price float, volume long); define stream containsSweetProductionStream (symbol string, price float, volume long); @Store(type='redis', table.name='SweetProductionTable', host= 'localhost', port='6379', password=\"root\") @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); @sink(type='log') define stream OutStream(symbol string, price float, volume long); @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; @info(name = 'query2') from updateSweetProductionStream update SweetProductionTable set SweetProductionTable.price = price on SweetProductionTable.symbol == symbol; @info(name = 'query3') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.symbol==symbol ; @info(name = 'query4') from updateOrInsertSweetProductionStream#window.timeBatch(1 sec) update or insert into SweetProductionTable on SweetProductionTable.symbol==symbol; @info(name = 'query5') from containsSweetProductionStream[ (symbol==SweetProductionTable.symbol) in SweetProductionTable] insert into OutStream; @info(name = 'query6') from searchSweetProductionStream#window.length(1) join SweetProductionTable on searchSweetProductionStream.symbol==SweetProductionTable.symbol select searchSweetProductionStream.symbol as symbol, SweetProductionTable.price as price, SweetProductionTable.volume as volume insert into OutStream;","title":"Receiving Events and Persisting Them in Redis Store"},{"location":"samples/Store-redis/#purpose","text":"This application demonstrates how to perform CRUD operations using Siddhi queries in Redis stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store. Search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store).","title":"Purpose:"},{"location":"samples/Store-redis/#prerequisites","text":"Download the Redis from https://redis.io/. Download redis java client 'Jedis' jar (>2.7.0) from https://mvnrepository.com/artifact/redis.clients/jedis and place in <SI_HOME>/lib folder. In redis.conf , provide the requirepass as root. Start redis by redis-server .","title":"Prerequisites:"},{"location":"samples/Store-redis/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Store-redis.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/Store-redis/#testing-the-sample","text":"Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'Store-redis' as 'Siddhi App Name' -> Select 'searchSweetProductionStream' as 'Stream Name' -> Provide attribute values -> Send. Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query. Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events. After a change in the store, using the search stream the developer can see whether the operation is successful. Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Testing the Sample:"},{"location":"samples/Store-redis/#viewing-the-results","text":"See the output for row materials on the console. Inserted, deleted, updated events can be checked by searchSweetProductionStream. @App:name(\"Store-redis\") @App:description(\"Receive events via simulator and received data are persisted in store.\") define stream insertSweetProductionStream (symbol string, price float, volume long); define stream deleteSweetProductionStream (symbol string, price float, volume long); define stream searchSweetProductionStream(symbol string); define stream updateOrInsertSweetProductionStream(symbol string, price float, volume long); define stream updateSweetProductionStream (symbol string, price float, volume long); define stream containsSweetProductionStream (symbol string, price float, volume long); @Store(type='redis', table.name='SweetProductionTable', host= 'localhost', port='6379', password=\"root\") @primaryKey('symbol') @index('price') define table SweetProductionTable (symbol string, price float, volume long); @sink(type='log') define stream OutStream(symbol string, price float, volume long); @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; @info(name = 'query2') from updateSweetProductionStream update SweetProductionTable set SweetProductionTable.price = price on SweetProductionTable.symbol == symbol; @info(name = 'query3') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.symbol==symbol ; @info(name = 'query4') from updateOrInsertSweetProductionStream#window.timeBatch(1 sec) update or insert into SweetProductionTable on SweetProductionTable.symbol==symbol; @info(name = 'query5') from containsSweetProductionStream[ (symbol==SweetProductionTable.symbol) in SweetProductionTable] insert into OutStream; @info(name = 'query6') from searchSweetProductionStream#window.length(1) join SweetProductionTable on searchSweetProductionStream.symbol==SweetProductionTable.symbol select searchSweetProductionStream.symbol as symbol, SweetProductionTable.price as price, SweetProductionTable.volume as volume insert into OutStream;","title":"Viewing the Results:"},{"location":"samples/Store-solr/","text":"Purpose: \u00b6 This application demonstrates how to perform CRUD operations using Siddhi queries in Solr stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store. search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store). Prerequisites: \u00b6 Download the solr-6.x.x.zip distribution from https://archive.apache.org/dist/lucene/solr/. Start the Solr server in cloud mode using the command {SOLR_HOME}/bin/solr -e cloud . This will create a simple solr cloud in your local machine. When creating the cloud provide the suggested examples values for the each field. Give the collection name as gettingstarted . For the configuration provide basic_configs . Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Store-solr.siddhi - Started Successfully! Testing the Sample: \u00b6 Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'Store-solr' as 'Siddhi App Name' -> Select 'searchSweetProductionStream' as 'Stream Name' -> Provide attribute values -> Send. Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query. Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events. After a change in the store, using the search stream the developer can see whether the operation is successful. Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for row materials on the console. Inserted, deleted, updated events can be checked by searchSweetProductionStream . @App:name(\"Store-solr\") @App:description('Receive events via simulator and received data are persisted in store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type='solr', collection='SweetProductionTable', zookeeper.url='localhost:9983', shards='2', replicas='2', schema='name string stored,amount double stored') --@PrimaryKey(\"name\") @index(\"amount\") define table SweetProductionTable (name string, amount double); /* Inserting events*/ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting events*/ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating events*/ @info(name = 'query3') from updateSweetProductionStream select name,amount update SweetProductionTable set SweetProductionTable.amount = amount on SweetProductionTable.name == name ; /* Updating or inserting events */ @info(name = 'query4') from updateOrInsertSweetProductionStream select name,amount update or insert into SweetProductionTable set SweetProductionTable.amount = amount on SweetProductionTable.name == name; /* Siddhi In (Contains)*/ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Receiving Events via Simulator and Persisting in SOLR Store"},{"location":"samples/Store-solr/#purpose","text":"This application demonstrates how to perform CRUD operations using Siddhi queries in Solr stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store. search - searchSweetProductionStream insert - insertSweetProductionStream delete - deleteSweetProductionStream update - updateSweetProductionStream update or insert - updateOrInsertSweetProductionStream contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store).","title":"Purpose:"},{"location":"samples/Store-solr/#prerequisites","text":"Download the solr-6.x.x.zip distribution from https://archive.apache.org/dist/lucene/solr/. Start the Solr server in cloud mode using the command {SOLR_HOME}/bin/solr -e cloud . This will create a simple solr cloud in your local machine. When creating the cloud provide the suggested examples values for the each field. Give the collection name as gettingstarted . For the configuration provide basic_configs .","title":"Prerequisites:"},{"location":"samples/Store-solr/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Store-solr.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/Store-solr/#testing-the-sample","text":"Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -> 'Single Simulation' -> Select 'Store-solr' as 'Siddhi App Name' -> Select 'searchSweetProductionStream' as 'Stream Name' -> Provide attribute values -> Send. Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query. Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events. After a change in the store, using the search stream the developer can see whether the operation is successful. Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Testing the Sample:"},{"location":"samples/Store-solr/#viewing-the-results","text":"See the output for row materials on the console. Inserted, deleted, updated events can be checked by searchSweetProductionStream . @App:name(\"Store-solr\") @App:description('Receive events via simulator and received data are persisted in store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type='solr', collection='SweetProductionTable', zookeeper.url='localhost:9983', shards='2', replicas='2', schema='name string stored,amount double stored') --@PrimaryKey(\"name\") @index(\"amount\") define table SweetProductionTable (name string, amount double); /* Inserting events*/ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting events*/ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating events*/ @info(name = 'query3') from updateSweetProductionStream select name,amount update SweetProductionTable set SweetProductionTable.amount = amount on SweetProductionTable.name == name ; /* Updating or inserting events */ @info(name = 'query4') from updateOrInsertSweetProductionStream select name,amount update or insert into SweetProductionTable set SweetProductionTable.amount = amount on SweetProductionTable.name == name; /* Siddhi In (Contains)*/ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Viewing the Results:"},{"location":"samples/StreamingKMeansSample/","text":"Purpose: \u00b6 This sample demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App StreamingKMeansSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StreamingKMeansSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : StreamingKMeansSample * Stream Name : SweetProductionStream 3. Enter and send suitable values for the attributes of selected stream. Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - SteamingMLExtensionkmeans-incremental-sample : SweetStatePredictionStream : Event{timestamp=1513603080892, data=[12.5, 124.5, 12.5, 124.5], isExpired=false} First two values of the data array represent the coordinates of the cluster that given product belongs to. (eg: 12.5, 124.5) @App:name(\"StreamingKMeansSample\") @App:Description('Demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering.') define stream SweetProductionStream(temperature double, density double); @sink(type='log') define stream SweetStatePredictionStream(closestCentroidCoordinate1 double, closestCentroidCoordinate2 double, temperature double, density double); @info(name = 'query1') from SweetProductionStream#streamingml:kMeansIncremental(2, 0.2, temperature, density) select closestCentroidCoordinate1, closestCentroidCoordinate2, temperature, density insert into SweetStatePredictionStream;","title":"Using StreamingML Kmeans for Clustering"},{"location":"samples/StreamingKMeansSample/#purpose","text":"This sample demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering.","title":"Purpose:"},{"location":"samples/StreamingKMeansSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App StreamingKMeansSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/StreamingKMeansSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StreamingKMeansSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/StreamingKMeansSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name : StreamingKMeansSample * Stream Name : SweetProductionStream 3. Enter and send suitable values for the attributes of selected stream.","title":"Testing the Sample:"},{"location":"samples/StreamingKMeansSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - SteamingMLExtensionkmeans-incremental-sample : SweetStatePredictionStream : Event{timestamp=1513603080892, data=[12.5, 124.5, 12.5, 124.5], isExpired=false} First two values of the data array represent the coordinates of the cluster that given product belongs to. (eg: 12.5, 124.5) @App:name(\"StreamingKMeansSample\") @App:Description('Demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering.') define stream SweetProductionStream(temperature double, density double); @sink(type='log') define stream SweetStatePredictionStream(closestCentroidCoordinate1 double, closestCentroidCoordinate2 double, temperature double, density double); @info(name = 'query1') from SweetProductionStream#streamingml:kMeansIncremental(2, 0.2, temperature, density) select closestCentroidCoordinate1, closestCentroidCoordinate2, temperature, density insert into SweetStatePredictionStream;","title":"Viewing the Results:"},{"location":"samples/StreamingRegressor/","text":"Purpose: \u00b6 This application demonstrates how to train a ML model with CSV data and do predictions based on it. The sample explores a scenario where the buying/ site browsing patterns, and average monthly earnings per person are used to predict how much she can be expected to spend on buying goods from a certain store per month. Prerequisites: \u00b6 Download siddhi-gpl-execution-streamingml-x.x.x.jar from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to {WSO2SIHome}/lib . Shutdown the server and copy this jar file to {WSO2SIHome}/lib location. Re-start the server. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StreamingRegressor.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. Testing the Sample: \u00b6 Train the ML model with training data set. The {WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv file is used to create the ML model. To simulate the events in this CSV file, execute the following steps. Click on 'Event Simulator' (double arrows on left tab). Click 'Feed Simulation' -> 'Create'. Give a name, description and time interval. Select 'CSV file' as the 'Simulation Source'. Click on 'Add Simulation Source'. Select StreamingRegressor as 'Siddhi App Name'. Select 'TrainInputStream' as 'StreamName'. Click on 'Upload' button under 'CSV file' section and upload the {WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv file. Give ',' as the delimiter. Save the configuration. Click on the start button (Arrow symbol) next to the newly created simulator, and wait few seconds for the model training to complete Send events to PredictionInputStream using {WSO2SIHome}/samples/artifacts/StreamingRegressor/test.csv file, to see the predicted values per each event in test.csv. CSV simulation can be done as explained above. Viewing the Results: \u00b6 See the predicted values per each event on the console. Note: \u00b6 Stop this Siddhi application, once you are done with the execution. @App:name(\"StreamingRegressor\") @App:description('Train a machine learning regressor model with CSV data and subsequently do predictions using that model') define stream TrainInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double, AvgAmountSpentPerMonth double ); define stream PredictionInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double); @sink(type='log') define stream PredictionOutputStream(prediction double, meanSquaredError double); --Train the ML model @info(name = 'query-train') from TrainInputStream#streamingml:updateAMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth, AvgAmountSpentPerMonth) select meanSquaredError insert into TrainOutputStream; --Predict using the model1 created with query 'query-train' @info(name = 'query-predict') from PredictionInputStream#streamingml:AMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth ) select prediction, meanSquaredError insert into PredictionOutputStream;","title":"Making Predictions via a Regressor Model"},{"location":"samples/StreamingRegressor/#purpose","text":"This application demonstrates how to train a ML model with CSV data and do predictions based on it. The sample explores a scenario where the buying/ site browsing patterns, and average monthly earnings per person are used to predict how much she can be expected to spend on buying goods from a certain store per month.","title":"Purpose:"},{"location":"samples/StreamingRegressor/#prerequisites","text":"Download siddhi-gpl-execution-streamingml-x.x.x.jar from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to {WSO2SIHome}/lib . Shutdown the server and copy this jar file to {WSO2SIHome}/lib location. Re-start the server. Save this sample.","title":"Prerequisites:"},{"location":"samples/StreamingRegressor/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StreamingRegressor.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/StreamingRegressor/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start.","title":"Notes:"},{"location":"samples/StreamingRegressor/#testing-the-sample","text":"Train the ML model with training data set. The {WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv file is used to create the ML model. To simulate the events in this CSV file, execute the following steps. Click on 'Event Simulator' (double arrows on left tab). Click 'Feed Simulation' -> 'Create'. Give a name, description and time interval. Select 'CSV file' as the 'Simulation Source'. Click on 'Add Simulation Source'. Select StreamingRegressor as 'Siddhi App Name'. Select 'TrainInputStream' as 'StreamName'. Click on 'Upload' button under 'CSV file' section and upload the {WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv file. Give ',' as the delimiter. Save the configuration. Click on the start button (Arrow symbol) next to the newly created simulator, and wait few seconds for the model training to complete Send events to PredictionInputStream using {WSO2SIHome}/samples/artifacts/StreamingRegressor/test.csv file, to see the predicted values per each event in test.csv. CSV simulation can be done as explained above.","title":"Testing the Sample:"},{"location":"samples/StreamingRegressor/#viewing-the-results","text":"See the predicted values per each event on the console.","title":"Viewing the Results:"},{"location":"samples/StreamingRegressor/#note","text":"Stop this Siddhi application, once you are done with the execution. @App:name(\"StreamingRegressor\") @App:description('Train a machine learning regressor model with CSV data and subsequently do predictions using that model') define stream TrainInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double, AvgAmountSpentPerMonth double ); define stream PredictionInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double); @sink(type='log') define stream PredictionOutputStream(prediction double, meanSquaredError double); --Train the ML model @info(name = 'query-train') from TrainInputStream#streamingml:updateAMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth, AvgAmountSpentPerMonth) select meanSquaredError insert into TrainOutputStream; --Predict using the model1 created with query 'query-train' @info(name = 'query-predict') from PredictionInputStream#streamingml:AMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth ) select prediction, meanSquaredError insert into PredictionOutputStream;","title":"Note:"},{"location":"samples/StringExtensionSample/","text":"Purpose: \u00b6 This String function converts the string value to lowercase letters Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console * Siddhi App StringExtensionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StringExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name: StringExtensionSample * Stream Name: SweetProductionStream 3. Enter following values in the fields and send. name: CaKe amount: 55.6 Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - StringExtensionSample : OutputStream : Event{timestamp=1513760993921, data=[cake, 55.6], isExpired=false} @App:name(\"StringExtensionSample\") @App:description('Converts the sweet name to lowercase letters.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream(name string, amount double); from SweetProductionStream select str:lower(name) as name, amount insert into OutputStream;","title":"Converting String Values to Lowercase"},{"location":"samples/StringExtensionSample/#purpose","text":"This String function converts the string value to lowercase letters","title":"Purpose:"},{"location":"samples/StringExtensionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console * Siddhi App StringExtensionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/StringExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * StringExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/StringExtensionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows: * Siddhi App Name: StringExtensionSample * Stream Name: SweetProductionStream 3. Enter following values in the fields and send. name: CaKe amount: 55.6","title":"Testing the Sample:"},{"location":"samples/StringExtensionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - StringExtensionSample : OutputStream : Event{timestamp=1513760993921, data=[cake, 55.6], isExpired=false} @App:name(\"StringExtensionSample\") @App:description('Converts the sweet name to lowercase letters.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream OutputStream(name string, amount double); from SweetProductionStream select str:lower(name) as name, amount insert into OutputStream;","title":"Viewing the Results:"},{"location":"samples/SweetProductionDataPreprocessing/","text":"Purpose: \u00b6 This application demonstrates how to receive events via TCP transport and carryout data pre-processing with numerous Siddhi extensions (eg. string extension, time extension). For more information on Siddhi extensions please refer to \"https://wso2.github.io/siddhi/extensions/\". In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event Prerequisites: \u00b6 Ensure that MySQL is installed on your machine. Add the MySQL JDBC driver into {WSO2_SI_HOME}/lib as follows: Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz. Unzip the archive. Copy mysql-connector-java-5.1.45-bin.jar to {WSO2_SI_Home}/lib directory. Create a database named sampleDB in MySQL. This database is referred to with jdbc:mysql://localhost:3306/sampleDB url. In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * SweetProductionDataPreprocessing.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run the ant command as follows. ant -Dtype=binary If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=binary -DnoOfEventsToSend=5 Viewing the Results: \u00b6 Check the ProcessedSweetProductionTable created in sampleDB . You would be able to see the pre-processed data written to the table @App:name(\"SweetProductionDataPreprocessing\") @App:description('Collect data via TCP transport and pre-process') @source(type='tcp', context='SweetProductionStream', port='9892', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sampleDB\", username=\"root\", password=\"mysql\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"compositeID\") define table ProcessedSweetProductionTable (compositeID string, amount double, date string); --Process smart home data by concatenating the IDs and formatting the time @info(name='query1') from SweetProductionStream select str:concat(str:lower(name), \"::\", time:currentTimestamp()) as compositeID, amount, time:currentDate() as date insert into ProcessedSweetProductionTable;","title":"Receiving Data via TCP and Preprocessing"},{"location":"samples/SweetProductionDataPreprocessing/#purpose","text":"This application demonstrates how to receive events via TCP transport and carryout data pre-processing with numerous Siddhi extensions (eg. string extension, time extension). For more information on Siddhi extensions please refer to \"https://wso2.github.io/siddhi/extensions/\". In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event","title":"Purpose:"},{"location":"samples/SweetProductionDataPreprocessing/#prerequisites","text":"Ensure that MySQL is installed on your machine. Add the MySQL JDBC driver into {WSO2_SI_HOME}/lib as follows: Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz. Unzip the archive. Copy mysql-connector-java-5.1.45-bin.jar to {WSO2_SI_Home}/lib directory. Create a database named sampleDB in MySQL. This database is referred to with jdbc:mysql://localhost:3306/sampleDB url. In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials. Save this sample.","title":"Prerequisites:"},{"location":"samples/SweetProductionDataPreprocessing/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * SweetProductionDataPreprocessing.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/SweetProductionDataPreprocessing/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/tcp-client and run the ant command as follows. ant -Dtype=binary If you want to publish custom number of events, you need to run ant command as follows. ant -Dtype=binary -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/SweetProductionDataPreprocessing/#viewing-the-results","text":"Check the ProcessedSweetProductionTable created in sampleDB . You would be able to see the pre-processed data written to the table @App:name(\"SweetProductionDataPreprocessing\") @App:description('Collect data via TCP transport and pre-process') @source(type='tcp', context='SweetProductionStream', port='9892', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @Store(type=\"rdbms\", jdbc.url=\"jdbc:mysql://localhost:3306/sampleDB\", username=\"root\", password=\"mysql\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PrimaryKey(\"compositeID\") define table ProcessedSweetProductionTable (compositeID string, amount double, date string); --Process smart home data by concatenating the IDs and formatting the time @info(name='query1') from SweetProductionStream select str:concat(str:lower(name), \"::\", time:currentTimestamp()) as compositeID, amount, time:currentDate() as date insert into ProcessedSweetProductionTable;","title":"Viewing the Results:"},{"location":"samples/TensorFlowTestApp/","text":"Purpose: \u00b6 This application demonstrates how to import a pretrained Tensorflow model WSO2 Streaming Integrator to perform a regression task. Prerequisites: \u00b6 Replace {SI_HOME} with absolute path to the Streaming Integrator Tooling home directory. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TensorFlowTestApp successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: * TensorFlowTestApp.siddhi - Started Successfully! Testing the Sample: \u00b6 Send events through one or more of the following methods. Send events to ProductionInputStream , via event simulator. \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: TensorFlowTestApp Stream Name: InputStream In the x field, enter the following and then click Send to send the event. x: \"double:[1,-2]\" Send some more events. Send events to the simulator http endpoint through the curl command: \u00b6 Open a new terminal and issue the following command: curl -X POST \\ http://localhost:9390/simulation/single \\ -H 'content-type: text/plain' \\ -d '{\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": [\"double:[1,-2]\"]}' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": ['double:[1,-2]']} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal. @App:name(\"TensorFlowTestApp\") define stream InputStream (x string); @sink(type='log') define stream OutputStream (outputPoint0 double, outputPoint1 double); @info(name = 'query1') from InputStream#tensorFlow:predict('{SI_HOME}/samples/artifacts/TensorflowSample/Regression', 'inputPoint', 'outputPoint', x) select outputPoint0, outputPoint1 insert into OutputStream;","title":"Performing Regression Tasks via an Imported Tensorflow Model"},{"location":"samples/TensorFlowTestApp/#purpose","text":"This application demonstrates how to import a pretrained Tensorflow model WSO2 Streaming Integrator to perform a regression task.","title":"Purpose:"},{"location":"samples/TensorFlowTestApp/#prerequisites","text":"Replace {SI_HOME} with absolute path to the Streaming Integrator Tooling home directory. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TensorFlowTestApp successfully deployed.","title":"Prerequisites:"},{"location":"samples/TensorFlowTestApp/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console: * TensorFlowTestApp.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/TensorFlowTestApp/#testing-the-sample","text":"Send events through one or more of the following methods.","title":"Testing the Sample:"},{"location":"samples/TensorFlowTestApp/#send-events-to-productioninputstream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: TensorFlowTestApp Stream Name: InputStream In the x field, enter the following and then click Send to send the event. x: \"double:[1,-2]\" Send some more events.","title":"Send events to ProductionInputStream, via event simulator."},{"location":"samples/TensorFlowTestApp/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","text":"Open a new terminal and issue the following command: curl -X POST \\ http://localhost:9390/simulation/single \\ -H 'content-type: text/plain' \\ -d '{\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": [\"double:[1,-2]\"]}' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl command:"},{"location":"samples/TensorFlowTestApp/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": ['double:[1,-2]']} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/TensorFlowTestApp/#viewing-the-results","text":"See the output on the terminal. @App:name(\"TensorFlowTestApp\") define stream InputStream (x string); @sink(type='log') define stream OutputStream (outputPoint0 double, outputPoint1 double); @info(name = 'query1') from InputStream#tensorFlow:predict('{SI_HOME}/samples/artifacts/TensorflowSample/Regression', 'inputPoint', 'outputPoint', x) select outputPoint0, outputPoint1 insert into OutputStream;","title":"Viewing the Results:"},{"location":"samples/TextMappingWithInmemoryTransport/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary format and check the custom text mapping and the default text mapping using inMemory transport and log the events in OutputStreams accordingly to the output console. Prerequisites: \u00b6 Save this sample Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * TextMappingWithInmemoryTransport.siddhi - Started Successfully! Testing the Sample: \u00b6 In order to publish events with TCP client, 1. Go to {WSO2SIHome}/samples/sample-clients/tcp-client/ directory. 2. Run ant commant as following. ant -Dtype=binary If you want to publish custom number of events, you need to run \"ant\" command as follows. ant -Dtype=binary -DnoOfEventsToSend=5 Viewing the Results: \u00b6 INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599736271, data=[Jelly Bean, 9.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599737255, data=[Froyo, 1534.87], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599737255, data=[Froyo, 1.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599738255, data=[Jelly Bean, 3030.71], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599738256, data=[Jelly Bean, 3.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599739256, data=[Cupcake, 3212.83], isExpired=false} Notes: \u00b6 If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console @App:name(\"TextMappingWithInmemoryTransport\") @App:description('Use inmemory transport to custom text mapping and the default text mapping and view the output on the console.') @source(type='tcp', context='SweetProductionStream', port='9892', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', prefix='Default Mapper') define stream DefaultOutputStream (name string, amount double); @sink(type='log', prefix='Custom Mapper') define stream CustomOutputStream (name string, amount double); -- Default text mapping. @sink(type='inMemory', topic='home', @map(type='text')) define stream InMemoryDefaultSweetProductionInputData (name string, amount double); @source(type='inMemory', topic='home', @map(type='text')) define stream UsageStream (name string, amount double); -- Custom text mapping. @sink(type='inMemory', topic='home1', @map(type='text', @payload(\"\"\"name:{{name}}, amount:{{amount}}\"\"\"))) define stream InMemoryCustomSweetProductionInputData (name string, amount double); @source(type='inMemory', topic='home1', @map(type='text' , regex.A='((?<=name:)(.*)(?=,))',regex.B='([-0-9]+)', @attributes(name = 'A', amount = 'B'))) define stream UsageStream2 (name string, amount double); from SweetProductionStream select * insert into InMemoryDefaultSweetProductionInputData; from UsageStream select * insert into DefaultOutputStream; from SweetProductionStream select * insert into InMemoryCustomSweetProductionInputData; from UsageStream2 select * insert into CustomOutputStream;","title":"Text Mapping with In-memory Transport"},{"location":"samples/TextMappingWithInmemoryTransport/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary format and check the custom text mapping and the default text mapping using inMemory transport and log the events in OutputStreams accordingly to the output console.","title":"Purpose:"},{"location":"samples/TextMappingWithInmemoryTransport/#prerequisites","text":"Save this sample","title":"Prerequisites:"},{"location":"samples/TextMappingWithInmemoryTransport/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * Tcp Server started in 0.0.0.0:9892 * TextMappingWithInmemoryTransport.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/TextMappingWithInmemoryTransport/#testing-the-sample","text":"In order to publish events with TCP client, 1. Go to {WSO2SIHome}/samples/sample-clients/tcp-client/ directory. 2. Run ant commant as following. ant -Dtype=binary If you want to publish custom number of events, you need to run \"ant\" command as follows. ant -Dtype=binary -DnoOfEventsToSend=5","title":"Testing the Sample:"},{"location":"samples/TextMappingWithInmemoryTransport/#viewing-the-results","text":"INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599736271, data=[Jelly Bean, 9.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599737255, data=[Froyo, 1534.87], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599737255, data=[Froyo, 1.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599738255, data=[Jelly Bean, 3030.71], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599738256, data=[Jelly Bean, 3.0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599739256, data=[Cupcake, 3212.83], isExpired=false}","title":"Viewing the Results:"},{"location":"samples/TextMappingWithInmemoryTransport/#notes","text":"If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console @App:name(\"TextMappingWithInmemoryTransport\") @App:description('Use inmemory transport to custom text mapping and the default text mapping and view the output on the console.') @source(type='tcp', context='SweetProductionStream', port='9892', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', prefix='Default Mapper') define stream DefaultOutputStream (name string, amount double); @sink(type='log', prefix='Custom Mapper') define stream CustomOutputStream (name string, amount double); -- Default text mapping. @sink(type='inMemory', topic='home', @map(type='text')) define stream InMemoryDefaultSweetProductionInputData (name string, amount double); @source(type='inMemory', topic='home', @map(type='text')) define stream UsageStream (name string, amount double); -- Custom text mapping. @sink(type='inMemory', topic='home1', @map(type='text', @payload(\"\"\"name:{{name}}, amount:{{amount}}\"\"\"))) define stream InMemoryCustomSweetProductionInputData (name string, amount double); @source(type='inMemory', topic='home1', @map(type='text' , regex.A='((?<=name:)(.*)(?=,))',regex.B='([-0-9]+)', @attributes(name = 'A', amount = 'B'))) define stream UsageStream2 (name string, amount double); from SweetProductionStream select * insert into InMemoryDefaultSweetProductionInputData; from UsageStream select * insert into DefaultOutputStream; from SweetProductionStream select * insert into InMemoryCustomSweetProductionInputData; from UsageStream2 select * insert into CustomOutputStream;","title":"Notes:"},{"location":"samples/TimeSeriesExtensionSample/","text":"Purpose: \u00b6 This application demonstrates how to use the siddhi-execution-timeseries with regress function which allows user to specify the batch size that defines the number of events to be considered for the calculation of regression. Prerequisites: \u00b6 Start editor. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * TimeSeriesExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run the following command. ant -Dtype=xml -DeventDefinition='<events><event><x>{0}</x><y>{1}</y></event></events>' -DfilePath={WSO2SIHome}/samples/artifacts/TimeExtensionSample/TimeSeries_events.txt -DnoOfEventsToSend=10 -DcontinuouslyReadFile=true You can edit -DfilePath={WSO2SIHome}/samples/artifacts/TimeExtensionSample/TimeSeries_events.txt -DnoOfEventsToSend=10 -DcontinuouslyReadFile=true file and give your own value set to check the lengthTimeRegress . Viewing the Results: \u00b6 See the output on timeSeriesExtensionSample console. INFO {io.siddhi.core.stream.output.sink.LogSink} - timeSeriesExtensionSample : logStream : Event{timestamp=1513592366610, data=[5000.0, 7000.0, 822.6805758329987, 0.0, 0.6129017613945887], isExpired=false} @App:name('TimeSeriesExtensionSample') @App:Description('Demonstrates how to use the siddhi-execution-timeseries with regress function.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml')) define stream ProductionStream (y double, x double); @sink(type='log') define stream logStream (y double, x double, stderr double, beta0 double, beta1 double); from ProductionStream#timeseries:regress(10, 50, 0.95, y, x) select * insert into OutputStream; from OutputStream select * insert into logStream;","title":"Processing data via the Timeseries Extension with the Regress Function"},{"location":"samples/TimeSeriesExtensionSample/#purpose","text":"This application demonstrates how to use the siddhi-execution-timeseries with regress function which allows user to specify the batch size that defines the number of events to be considered for the calculation of regression.","title":"Purpose:"},{"location":"samples/TimeSeriesExtensionSample/#prerequisites","text":"Start editor. Save this sample.","title":"Prerequisites:"},{"location":"samples/TimeSeriesExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * TimeSeriesExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/TimeSeriesExtensionSample/#testing-the-sample","text":"Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run the following command. ant -Dtype=xml -DeventDefinition='<events><event><x>{0}</x><y>{1}</y></event></events>' -DfilePath={WSO2SIHome}/samples/artifacts/TimeExtensionSample/TimeSeries_events.txt -DnoOfEventsToSend=10 -DcontinuouslyReadFile=true You can edit -DfilePath={WSO2SIHome}/samples/artifacts/TimeExtensionSample/TimeSeries_events.txt -DnoOfEventsToSend=10 -DcontinuouslyReadFile=true file and give your own value set to check the lengthTimeRegress .","title":"Testing the Sample:"},{"location":"samples/TimeSeriesExtensionSample/#viewing-the-results","text":"See the output on timeSeriesExtensionSample console. INFO {io.siddhi.core.stream.output.sink.LogSink} - timeSeriesExtensionSample : logStream : Event{timestamp=1513592366610, data=[5000.0, 7000.0, 822.6805758329987, 0.0, 0.6129017613945887], isExpired=false} @App:name('TimeSeriesExtensionSample') @App:Description('Demonstrates how to use the siddhi-execution-timeseries with regress function.') @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='xml')) define stream ProductionStream (y double, x double); @sink(type='log') define stream logStream (y double, x double, stderr double, beta0 double, beta1 double); from ProductionStream#timeseries:regress(10, 50, 0.95, y, x) select * insert into OutputStream; from OutputStream select * insert into logStream;","title":"Viewing the Results:"},{"location":"samples/TwitterPolling-sample/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling for retrieving historical tweets within one week from Twitter App using twitter source. Prerequisites: \u00b6 Go to the https://apps.twitter.com/ and create new App Select the app created in step 1 and go to \"permisstion\" tab and select \"read&write\" permission. Go to the \"keys and access tokens\" tab and generate new access token. Collect following value from \"keys and access tokens tab\". Consumer Key Consumer Secret Access Token Access Secret Token Update the values of the parameters with these values. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TwitterPolling-sample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. Testing the Sample: \u00b6 See the terminal that prints the events according to the given source configuaration. This sample is to retrieve tweets that mentioning \"NASA\". So the each tweet will contain '@NASA'. Viewing the Results: \u00b6 See the output on the terminal. @App:name(\"TwitterPolling-sample\") @App:description('Retrieving historical tweets within one week') @source(type='twitter' , consumer.key=\"xxxxxxxx\",consumer.secret='xxxxxxxx', access.token ='xxxxxxxxx', access.token.secret='xxxxxxxxx', mode='polling', count = '100', query = '@NASA', polling.interval = '300', @map(type='keyvalue', @attributes(createdAt = 'createdAt', id = 'tweetId', text= 'text', hashtags = 'hashtags', userMentions = 'userMentions'))) define stream rcvEvents(createdAt string, id long, text string ,hashtags string, userMentions string); @sink(type='log', prefix='LOGGER') define stream Outputstream(createdAt string, id long, text string ,hashtags string, userMentions string); @info(name='TwitterStream') from rcvEvents select * insert into Outputstream;","title":"Retrieving Historical Tweets for a Given Time Period"},{"location":"samples/TwitterPolling-sample/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling for retrieving historical tweets within one week from Twitter App using twitter source.","title":"Purpose:"},{"location":"samples/TwitterPolling-sample/#prerequisites","text":"Go to the https://apps.twitter.com/ and create new App Select the app created in step 1 and go to \"permisstion\" tab and select \"read&write\" permission. Go to the \"keys and access tokens\" tab and generate new access token. Collect following value from \"keys and access tokens tab\". Consumer Key Consumer Secret Access Token Access Secret Token Update the values of the parameters with these values. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TwitterPolling-sample successfully deployed.","title":"Prerequisites:"},{"location":"samples/TwitterPolling-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'.","title":"Executing the Sample:"},{"location":"samples/TwitterPolling-sample/#testing-the-sample","text":"See the terminal that prints the events according to the given source configuaration. This sample is to retrieve tweets that mentioning \"NASA\". So the each tweet will contain '@NASA'.","title":"Testing the Sample:"},{"location":"samples/TwitterPolling-sample/#viewing-the-results","text":"See the output on the terminal. @App:name(\"TwitterPolling-sample\") @App:description('Retrieving historical tweets within one week') @source(type='twitter' , consumer.key=\"xxxxxxxx\",consumer.secret='xxxxxxxx', access.token ='xxxxxxxxx', access.token.secret='xxxxxxxxx', mode='polling', count = '100', query = '@NASA', polling.interval = '300', @map(type='keyvalue', @attributes(createdAt = 'createdAt', id = 'tweetId', text= 'text', hashtags = 'hashtags', userMentions = 'userMentions'))) define stream rcvEvents(createdAt string, id long, text string ,hashtags string, userMentions string); @sink(type='log', prefix='LOGGER') define stream Outputstream(createdAt string, id long, text string ,hashtags string, userMentions string); @info(name='TwitterStream') from rcvEvents select * insert into Outputstream;","title":"Viewing the Results:"},{"location":"samples/TwitterStreaming-sample/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling for retrieving tweets from Twitter App using twitter source. Prerequisites: \u00b6 Go to the https://apps.twitter.com/ and create new App. Select the app created in step 1 and go to \"permisstion\" tab and select \"read&write\" permission. Go to the \"keys and access tokens\" tab and generate new access token. Collect following value from \"keys and access tokens tab\". Consumer Key Consumer Secret Access Token Access Secret Token Update the values of the parameters with these values. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TwitterStreaming-sample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. Testing the Sample: \u00b6 See the terminal that prints the events according to the given source configuration. This sample is to retrieve tweets that includes the given words. So the events will contain the given keywords. Viewing the Results: \u00b6 See the output on the terminal. @App:name(\"TwitterStreaming-sample\") @App:description('Retrieving real time tweets') @source(type='twitter' , consumer.key=\"xxxxxxxxx\",consumer.secret='xxxxxxxxxx',access.token ='xxxxxxxx', access.token.secret='xxxxxxxxx', mode='streaming', track = 'Amazon,Google', @map(type='keyvalue', @attributes(createdAt = 'createdAt', id = 'tweetId', text= 'text', hashtags = 'hashtags', userMentions = 'userMentions'))) define stream rcvEvents(createdAt string, id long, text string ,hashtags string, userMentions string); @sink(type='log', prefix='LOGGER') define stream Outputstream(createdAt string, id long, text string ,hashtags string, userMentions string); @info(name='TwitterStream') from rcvEvents select * insert into Outputstream;","title":"Retrieving Real-time Tweets"},{"location":"samples/TwitterStreaming-sample/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling for retrieving tweets from Twitter App using twitter source.","title":"Purpose:"},{"location":"samples/TwitterStreaming-sample/#prerequisites","text":"Go to the https://apps.twitter.com/ and create new App. Select the app created in step 1 and go to \"permisstion\" tab and select \"read&write\" permission. Go to the \"keys and access tokens\" tab and generate new access token. Collect following value from \"keys and access tokens tab\". Consumer Key Consumer Secret Access Token Access Secret Token Update the values of the parameters with these values. Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App TwitterStreaming-sample successfully deployed.","title":"Prerequisites:"},{"location":"samples/TwitterStreaming-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'.","title":"Executing the Sample:"},{"location":"samples/TwitterStreaming-sample/#testing-the-sample","text":"See the terminal that prints the events according to the given source configuration. This sample is to retrieve tweets that includes the given words. So the events will contain the given keywords.","title":"Testing the Sample:"},{"location":"samples/TwitterStreaming-sample/#viewing-the-results","text":"See the output on the terminal. @App:name(\"TwitterStreaming-sample\") @App:description('Retrieving real time tweets') @source(type='twitter' , consumer.key=\"xxxxxxxxx\",consumer.secret='xxxxxxxxxx',access.token ='xxxxxxxx', access.token.secret='xxxxxxxxx', mode='streaming', track = 'Amazon,Google', @map(type='keyvalue', @attributes(createdAt = 'createdAt', id = 'tweetId', text= 'text', hashtags = 'hashtags', userMentions = 'userMentions'))) define stream rcvEvents(createdAt string, id long, text string ,hashtags string, userMentions string); @sink(type='log', prefix='LOGGER') define stream Outputstream(createdAt string, id long, text string ,hashtags string, userMentions string); @info(name='TwitterStream') from rcvEvents select * insert into Outputstream;","title":"Viewing the Results:"},{"location":"samples/UnitConversionExtentionSample/","text":"Purpose: \u00b6 This sample demonstrates how to use unit conversion extension for converting units. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App UnitConversionExtensionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * UnitConversionExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 You can publish data event to the file, through event simulator Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: UnitConversionExtensionSample Stream name: SweetProductionStream Enter and send suitable values for the attributes of selected stream. Viewing the Results: \u00b6 Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - UnitConversionExtensionSample : WeightConvertedStream : Event{timestamp=1513588858315, data=[Chocolate, 1250.0], isExpired=false} @App:name(\"UnitConversionExtensionSample\") @App:description('Demonstrates how to use unit conversion extension for converting units.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream WeightConvertedStream(name string, weightInGrams double); from SweetProductionStream select name, unitconversion:kgTog(amount) as weightInGrams insert into WeightConvertedStream;","title":"Converting Units"},{"location":"samples/UnitConversionExtentionSample/#purpose","text":"This sample demonstrates how to use unit conversion extension for converting units.","title":"Purpose:"},{"location":"samples/UnitConversionExtentionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following messages would be shown on the console. * Siddhi App UnitConversionExtensionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/UnitConversionExtentionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * UnitConversionExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/UnitConversionExtentionSample/#testing-the-sample","text":"You can publish data event to the file, through event simulator Open event simulator by clicking on the second icon or press Ctrl+Shift+I. In the Single Simulation tab of the panel, select values as follows: Siddhi App Name: UnitConversionExtensionSample Stream name: SweetProductionStream Enter and send suitable values for the attributes of selected stream.","title":"Testing the Sample:"},{"location":"samples/UnitConversionExtentionSample/#viewing-the-results","text":"Messages similar to the following would be shown on the console. INFO {io.siddhi.core.stream.output.sink.LogSink} - UnitConversionExtensionSample : WeightConvertedStream : Event{timestamp=1513588858315, data=[Chocolate, 1250.0], isExpired=false} @App:name(\"UnitConversionExtensionSample\") @App:description('Demonstrates how to use unit conversion extension for converting units.') define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream WeightConvertedStream(name string, weightInGrams double); from SweetProductionStream select name, unitconversion:kgTog(amount) as weightInGrams insert into WeightConvertedStream;","title":"Viewing the Results:"},{"location":"samples/approximate-count-sample/","text":"Calculating the Approximate Count of Events within a Siddhi Window \u00b6 Purpose: \u00b6 This application demonstrates how to calculate the approximate count(frequency) of eventS within a Siddhi window. Before you begin: Copy the <SI_TOOLING_HOME>/samples/artifacts/ApproximateComputingSample/Test-approximate-count.json file and paste it in the <SI_TOOLING_HOME>/wso2/server/deployment/simulation-configs directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ApproximateComputingSample/approximate-count.csv file and paste it in the <SI_TOOLING_HOME>/wso2/server/deployment/csv-files directory. Save the sample Siddhi application. @App:name(\"approximate-count-sample\") @APP:description(\"Demonstrates how to calculate the approximate count(frequency) of eventS within a Siddhi window.\") define stream transactionStream (userId int, amount double); @sink(type='log') define stream OutputStream(count long, countLowerBound long, countUpperBound long); @info(name = 'query1') from transactionStream#window.length(50)#approximate:count(userId, 0.005, 0.9) select count, countLowerBound, countUpperBound insert into OutputStream; Executing the Sample \u00b6 To execute the sample, start the approximate-count-sample Siddhi application by clicking the Start button (shown below) or by clicking by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. approximate-count-sample.siddhi - Started Successfully! Testing the Sample \u00b6 To test the sample Siddhi application, simulate multiple events for it from a CSV file via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation . Test-approximate-count is available in the Active Feed Simulations list as shown below. Click the start button next to the Test-approximate-count simulation. Select Test-approximate-count under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator Viewing the Results: \u00b6 e output is logged i the console as shown in the extract below.","title":"Calculating the Approximate Count of Events within a Siddhi Window"},{"location":"samples/approximate-count-sample/#calculating-the-approximate-count-of-events-within-a-siddhi-window","text":"","title":"Calculating the Approximate Count of Events within a Siddhi Window"},{"location":"samples/approximate-count-sample/#purpose","text":"This application demonstrates how to calculate the approximate count(frequency) of eventS within a Siddhi window. Before you begin: Copy the <SI_TOOLING_HOME>/samples/artifacts/ApproximateComputingSample/Test-approximate-count.json file and paste it in the <SI_TOOLING_HOME>/wso2/server/deployment/simulation-configs directory. Copy the <SI_TOOLING_HOME>/samples/artifacts/ApproximateComputingSample/approximate-count.csv file and paste it in the <SI_TOOLING_HOME>/wso2/server/deployment/csv-files directory. Save the sample Siddhi application. @App:name(\"approximate-count-sample\") @APP:description(\"Demonstrates how to calculate the approximate count(frequency) of eventS within a Siddhi window.\") define stream transactionStream (userId int, amount double); @sink(type='log') define stream OutputStream(count long, countLowerBound long, countUpperBound long); @info(name = 'query1') from transactionStream#window.length(50)#approximate:count(userId, 0.005, 0.9) select count, countLowerBound, countUpperBound insert into OutputStream;","title":"Purpose:"},{"location":"samples/approximate-count-sample/#executing-the-sample","text":"To execute the sample, start the approximate-count-sample Siddhi application by clicking the Start button (shown below) or by clicking by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. approximate-count-sample.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/approximate-count-sample/#testing-the-sample","text":"To test the sample Siddhi application, simulate multiple events for it from a CSV file via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. In the Event Simulator panel, click Feed Simulation . Test-approximate-count is available in the Active Feed Simulations list as shown below. Click the start button next to the Test-approximate-count simulation. Select Test-approximate-count under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator","title":"Testing the Sample"},{"location":"samples/approximate-count-sample/#viewing-the-results","text":"e output is logged i the console as shown in the extract below.","title":"Viewing the Results:"},{"location":"samples/approximate-distinctCount-sample/","text":"Purpose: \u00b6 This application demonstrates how to calculate the number of distinct events within a Siddhi window with an accepted level of accuracy. Prerequisites: \u00b6 Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/Test-approximate-distinctCount.json file to {WSO2_SI_Home}/wso2/editor/deployment/simulation-configs Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/approximate-distinctCount.csv file to {WSO2_SI_Home}/wso2/editor/deployment/csv-files. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console approximate-distinctCount-sample.siddhi - Started Successfully!. Testing the Sample: \u00b6 Click on 'Event Simulator' (double arrows on left tab) Click 'Feed Simulation' Select Test-approximate-distinctCount under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator Viewing the Results: \u00b6 See the output on the console. @App:name(\"approximate-distinctCount-sample\") @App:description('Calculate the number of distinct events within a Siddhi window with an accepted level of accuracy.') @info(name = 'query') define stream sensorStream (sensorId int); @sink(type='log') define stream OutputStream(distinctCount long, distinctCountLowerBound long, distinctCountUpperBound long); from sensorStream#window.length(100)#approximate:distinctCount(sensorId, 0.01, 0.95) select distinctCount, distinctCountLowerBound, distinctCountUpperBound insert into OutputStream;","title":"Calculating the Number of Distinct Events within a Siddhi Window"},{"location":"samples/approximate-distinctCount-sample/#purpose","text":"This application demonstrates how to calculate the number of distinct events within a Siddhi window with an accepted level of accuracy.","title":"Purpose:"},{"location":"samples/approximate-distinctCount-sample/#prerequisites","text":"Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/Test-approximate-distinctCount.json file to {WSO2_SI_Home}/wso2/editor/deployment/simulation-configs Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/approximate-distinctCount.csv file to {WSO2_SI_Home}/wso2/editor/deployment/csv-files.","title":"Prerequisites:"},{"location":"samples/approximate-distinctCount-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console approximate-distinctCount-sample.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/approximate-distinctCount-sample/#testing-the-sample","text":"Click on 'Event Simulator' (double arrows on left tab) Click 'Feed Simulation' Select Test-approximate-distinctCount under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator","title":"Testing the Sample:"},{"location":"samples/approximate-distinctCount-sample/#viewing-the-results","text":"See the output on the console. @App:name(\"approximate-distinctCount-sample\") @App:description('Calculate the number of distinct events within a Siddhi window with an accepted level of accuracy.') @info(name = 'query') define stream sensorStream (sensorId int); @sink(type='log') define stream OutputStream(distinctCount long, distinctCountLowerBound long, distinctCountUpperBound long); from sensorStream#window.length(100)#approximate:distinctCount(sensorId, 0.01, 0.95) select distinctCount, distinctCountLowerBound, distinctCountUpperBound insert into OutputStream;","title":"Viewing the Results:"},{"location":"samples/approximate-distinctCountEver-sample/","text":"Purpose: \u00b6 This application demonstrates how to calculate the number of distinct events based on a specific relative error. Returns erroneous values if is is used with a Siddhi window. Prerequisites: \u00b6 Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/Test-approximate-distinctCountEver.json file to {WSO2_SI_Home}/wso2/editor/deployment/simulation-configs Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/approximate-distinctCountEver.csv file to {WSO2_SI_Home}/wso2/editor/deployment/csv-files. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console approximate-distinctCountEver-sample.siddhi - Started Successfully!. Testing the Sample: \u00b6 Click on 'Event Simulator' (double arrows on left tab) Click 'Feed Simulation' Select Test-approximate-distinctCountEver under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator Viewing the Results: \u00b6 See the output on the console. @App:name(\"approximate-distinctCountEver-sample\") @App:description('Calculate the number of distinct events without a Siddhi window.') define stream requestStream (ip string); @sink(type='log') define stream OutputStream(distinctCountEver long, distinctCountEverLowerBound long, distinctCountEverUpperBound long); @info(name = 'query') from requestStream#approximate:distinctCountEver(ip) select distinctCountEver, distinctCountEverLowerBound, distinctCountEverUpperBound insert into OutputStream;","title":"Calculating the Number of Distinct Events without a Siddhi Window"},{"location":"samples/approximate-distinctCountEver-sample/#purpose","text":"This application demonstrates how to calculate the number of distinct events based on a specific relative error. Returns erroneous values if is is used with a Siddhi window.","title":"Purpose:"},{"location":"samples/approximate-distinctCountEver-sample/#prerequisites","text":"Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/Test-approximate-distinctCountEver.json file to {WSO2_SI_Home}/wso2/editor/deployment/simulation-configs Copy {WSO2SIHome}/samples/artifacts/ApproximateComputingSample/approximate-distinctCountEver.csv file to {WSO2_SI_Home}/wso2/editor/deployment/csv-files.","title":"Prerequisites:"},{"location":"samples/approximate-distinctCountEver-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run' If the Siddhi application starts successfully, the following messages would be shown on the console approximate-distinctCountEver-sample.siddhi - Started Successfully!.","title":"Executing the Sample:"},{"location":"samples/approximate-distinctCountEver-sample/#testing-the-sample","text":"Click on 'Event Simulator' (double arrows on left tab) Click 'Feed Simulation' Select Test-approximate-distinctCountEver under 'Active Feed Simulations' Click on the start button (Arrow symbol) next to the simulator","title":"Testing the Sample:"},{"location":"samples/approximate-distinctCountEver-sample/#viewing-the-results","text":"See the output on the console. @App:name(\"approximate-distinctCountEver-sample\") @App:description('Calculate the number of distinct events without a Siddhi window.') define stream requestStream (ip string); @sink(type='log') define stream OutputStream(distinctCountEver long, distinctCountEverLowerBound long, distinctCountEverUpperBound long); @info(name = 'query') from requestStream#approximate:distinctCountEver(ip) select distinctCountEver, distinctCountEverLowerBound, distinctCountEverUpperBound insert into OutputStream;","title":"Viewing the Results:"},{"location":"samples/execution-geo-sample/","text":"Processing Geo Data \u00b6 Purpose: \u00b6 This application demonstrates how to retrieve the longitude and latitude based on location details provided. Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling. Executing the Sample \u00b6 To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. execution-geo-sample.siddhi - Started Successfully! Testing the Sample \u00b6 To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the geocodeStream stream of the execution-geo-sample Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name execution-geo-sample StreamName geocodeStream As a result, attributes specific to the geocodeStream are displayed as marked in the above image. Enter attribute values as follows. . Attribute Value location 5 Avenue Anatole France level 75007 Paris time France Click Start and Send . Viewing the Results \u00b6 The prediction for the location you provided via the event you simulated is displayed as follows in the Streaming Integrator Tooling console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - sentimentExtensionSample: Event :, StreamEvent{ timestamp=1513623526790, beforeWindowData=null, onAfterWindowData=null, outputData=[48.8583698, 2.2944833, Tour Eiffel, 5 Avenue Anatole France, 75007 Paris, France], type=CURRENT, next=null} Click here to view the sample Siddhi application. @App:name(\"execution-geo-sample\") @App:description('Use geo data related functionality to retrieve the longitude and latitude for the provided location details.') -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with streaming-integrator-tooling. define stream geocodeStream (location string, level string, time string); @sink(type='log') define stream dataOut(latitude double, longitude double, formattedAddress string); @info(name = 'query') from geocodeStream#geo:geocode(location) select latitude, longitude, formattedAddress insert into dataOut;","title":"Processing Geo Data"},{"location":"samples/execution-geo-sample/#processing-geo-data","text":"","title":"Processing Geo Data"},{"location":"samples/execution-geo-sample/#purpose","text":"This application demonstrates how to retrieve the longitude and latitude based on location details provided. Before you begin: Save the sample Siddhi application in Streaming Integrator Tooling.","title":"Purpose:"},{"location":"samples/execution-geo-sample/#executing-the-sample","text":"To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run => Run . If the Siddhi application starts successfully, the following message appears in the console. execution-geo-sample.siddhi - Started Successfully!","title":"Executing the Sample"},{"location":"samples/execution-geo-sample/#testing-the-sample","text":"To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows: To open the Event Simulator, click the Event Simulator icon. This opens the event simulation panel. To simulate events for the geocodeStream stream of the execution-geo-sample Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows. Field Value Siddhi App Name execution-geo-sample StreamName geocodeStream As a result, attributes specific to the geocodeStream are displayed as marked in the above image. Enter attribute values as follows. . Attribute Value location 5 Avenue Anatole France level 75007 Paris time France Click Start and Send .","title":"Testing the Sample"},{"location":"samples/execution-geo-sample/#viewing-the-results","text":"The prediction for the location you provided via the event you simulated is displayed as follows in the Streaming Integrator Tooling console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - sentimentExtensionSample: Event :, StreamEvent{ timestamp=1513623526790, beforeWindowData=null, onAfterWindowData=null, outputData=[48.8583698, 2.2944833, Tour Eiffel, 5 Avenue Anatole France, 75007 Paris, France], type=CURRENT, next=null} Click here to view the sample Siddhi application. @App:name(\"execution-geo-sample\") @App:description('Use geo data related functionality to retrieve the longitude and latitude for the provided location details.') -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with streaming-integrator-tooling. define stream geocodeStream (location string, level string, time string); @sink(type='log') define stream dataOut(latitude double, longitude double, formattedAddress string); @info(name = 'query') from geocodeStream#geo:geocode(location) select latitude, longitude, formattedAddress insert into dataOut;","title":"Viewing the Results"},{"location":"samples/hoeffding-adaptive-tree-sample/","text":"Purpose: \u00b6 This application demonstrates how to train a Hoeffding Classifier and to predict the sweet category from the sweet production stream in streaming manner. Prerequisites: \u00b6 Download siddhi-gpl-execution-streamingml-x.x.x.jar from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App streaming-hoeffding-classifier-sample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * streaming-hoeffding-classifier-sample.siddhi - Started Successfully! Notes: \u00b6 If you edit this application while it's running, stop the application -> Save -> Start. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Start the application and check whether the specified events from the jms provider appear on the console. Testing the Sample: \u00b6 Note: The Hoeffding Classifier for streaming machine learning needs to be trained prior to perform prediction. Training phase \u00b6 Send events through one or more of the following methods. Send events to ProductionTrainStream, via event simulator: \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-hoeffding-classifier-sample Stream Name: ProductionTrainStream In the name and amount fields, enter the following and then click Send to send the event. density: 50.4 solubility: 30.03 sweetType: candy Send more events including upto 3 unique sweet types as specified as the parameter during the training phase. @info(name = 'query-train') from ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType ) Send events to the simulator http endpoint through the curl command: \u00b6 Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Testing phase \u00b6 Send events through one or more of the following methods. You may send events to ProductionInputStream, via event simulator \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-hoeffding-classifier-sample Stream Name: SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. density: 30.4 emperature: 20.5 Send events to the simulator http endpoint through the curl command: \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal. INFO {io.siddhi.core.stream.output.sink.LogSink} - streaming-hoeffding-classifier-sample : PredictionStream : Event{timestamp=1513610806272, data=[30.4, 20.5, candy, 0.0], isExpired=false} @App:name(\"streaming-hoeffding-classifier-sample\") @App:description('Train a streaming Hoeffding Classifier and to predict the type of sweet.') define stream ProductionTrainStream (density double, solubility double, sweetType string ); define stream SweetProductionStream (density double, solubility double); @sink(type='log') define stream PredictionStream (density double, solubility double, prediction string, confidenceLevel double); @info(name = 'query-train') from ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType ) select * insert into trainOutputStream; @info(name = 'query-predict') from SweetProductionStream#streamingml:hoeffdingTreeClassifier('classifierModel', density, solubility ) select density, solubility, prediction, confidenceLevel insert into PredictionStream;","title":"Making Predictions via Hoeffding Classifier Model"},{"location":"samples/hoeffding-adaptive-tree-sample/#purpose","text":"This application demonstrates how to train a Hoeffding Classifier and to predict the sweet category from the sweet production stream in streaming manner.","title":"Purpose:"},{"location":"samples/hoeffding-adaptive-tree-sample/#prerequisites","text":"Download siddhi-gpl-execution-streamingml-x.x.x.jar from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to {WSO2SIHome}/lib . Save this sample. If there is no syntax error, the following message is shown on the console: * Siddhi App streaming-hoeffding-classifier-sample successfully deployed.","title":"Prerequisites:"},{"location":"samples/hoeffding-adaptive-tree-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages would be shown on the console. * streaming-hoeffding-classifier-sample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/hoeffding-adaptive-tree-sample/#notes","text":"If you edit this application while it's running, stop the application -> Save -> Start. * Stop this Siddhi application (Click 'Run' on menu bar -> 'Stop'). * Start the application and check whether the specified events from the jms provider appear on the console.","title":"Notes:"},{"location":"samples/hoeffding-adaptive-tree-sample/#testing-the-sample","text":"Note: The Hoeffding Classifier for streaming machine learning needs to be trained prior to perform prediction.","title":"Testing the Sample:"},{"location":"samples/hoeffding-adaptive-tree-sample/#training-phase","text":"Send events through one or more of the following methods.","title":"Training phase"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-productiontrainstream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-hoeffding-classifier-sample Stream Name: ProductionTrainStream In the name and amount fields, enter the following and then click Send to send the event. density: 50.4 solubility: 30.03 sweetType: candy Send more events including upto 3 unique sweet types as specified as the parameter during the training phase. @info(name = 'query-train') from ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType )","title":"Send events to ProductionTrainStream, via event simulator:"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","text":"Open a new terminal and issue the following command: * curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl command:"},{"location":"samples/hoeffding-adaptive-tree-sample/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/hoeffding-adaptive-tree-sample/#testing-phase","text":"Send events through one or more of the following methods.","title":"Testing phase"},{"location":"samples/hoeffding-adaptive-tree-sample/#you-may-send-events-to-productioninputstream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-hoeffding-classifier-sample Stream Name: SweetProductionStream In the name and amount fields, enter the following and then click Send to send the event. density: 30.4 emperature: 20.5","title":"You may send events to ProductionInputStream, via event simulator"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command_1","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl command:"},{"location":"samples/hoeffding-adaptive-tree-sample/#publish-events-with-postman_1","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/hoeffding-adaptive-tree-sample/#viewing-the-results","text":"See the output on the terminal. INFO {io.siddhi.core.stream.output.sink.LogSink} - streaming-hoeffding-classifier-sample : PredictionStream : Event{timestamp=1513610806272, data=[30.4, 20.5, candy, 0.0], isExpired=false} @App:name(\"streaming-hoeffding-classifier-sample\") @App:description('Train a streaming Hoeffding Classifier and to predict the type of sweet.') define stream ProductionTrainStream (density double, solubility double, sweetType string ); define stream SweetProductionStream (density double, solubility double); @sink(type='log') define stream PredictionStream (density double, solubility double, prediction string, confidenceLevel double); @info(name = 'query-train') from ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType ) select * insert into trainOutputStream; @info(name = 'query-predict') from SweetProductionStream#streamingml:hoeffdingTreeClassifier('classifierModel', density, solubility ) select density, solubility, prediction, confidenceLevel insert into PredictionStream;","title":"Viewing the Results:"},{"location":"samples/sample/","text":"List of Streaming Integrator Samples \u00b6 Consume \u00b6 Recieve TCP in JSON Format Publish Google PubSub Messages In Text Format CSV Default Mapping CDC With Listening Mode Receive Google PubSub Messages In Text Format Receive MQTT In XML Format Receive TCP in Binary Format Receive Kafka In Binary Format Receive HTTP In XML Format With Default Mapping CDC With Polling Mode Twitter Polling sample Publish Http OAuth Request With Refresh Token Publish Http OAuth Request Response Store solr Receive WSO2 Events Receive Events From File Receive Email In Xml Format Receive Rabbitmq In JSON Format Store cassandra Transform \u00b6 Publish Http OAuth Request Publish Http OAuth Request Without Access Token Receive Hl7 In ER7 Format Data Preprocessing Publish Hl7 In ER7 Format Sweet Production Data Pre-processing CSV Custom Mapping Publish HTTP In Json Format With Custom Mapping Publish Email In Text Format Receive Kafka In Text Format With Custom Mapping Script js sample Publish Kafka In Custom Avro Format Receive Hl7 In Xml Format Regex Execution Sample Kalman Filter Execution Sample Publish Hl7 In Xml Format Publish Kafka In Json Format Publish Events To File Receive HTTP in XML Format With Custom Mapping Publish Rabbitmq In Xml Format Math Extension Sample Receive TCP in Text Format Publish Email In Xml Format Time Series Extension Sample Twitter Streaming sample String Extension Sample Text Mapping With In memory Transport Receive HTTP In Json Format With Default Mapping Hello Kafka Publish Http In Xml Format With Custom Mapping SNMP Set Request App Publish Tcp In Json Format Publish Http OAuth Request With OAuth User Map Extension Sample Publish Jms In Xml Format Receive JMS In Json Format Receive HTTP In Json Format With Custom Mapping Publish \u00b6 Publish Tcp In Text Format Receive Hl7 In ER7 Format Publish Google PubSub Messages In Text Format Publish Tcp In Binary Format Publish Web Socket In Xml Format Publish Prometheus Metrics HTTP Server store influxdb Publish Kafka In BinaryFormat Publish Jms In Key value Format Http Request Response Sample Publish WSO2 Events Publish Http In Xml Format Publish Http OAuth Request With Refresh Token Publish Http OAuth Request Response Store solr Publish Http In Json Format Store rdbms Store cassandra Publish Kafka In Avro Format Using Schema Registry Insights \u00b6 Pmml Model Processor Streaming Regressor Gpl NLP Find Name Entity Type Tensor Flow TestApp Markov Chain Sample Priority Extension Sample approximate distinctCountEver sample approximate distinctCount sample sentiment Extension Sample Pattern Matching Streaming KMeans Sample R Script Sample Receive Prometheus Metrics hoeffding adaptive tree sample Alerts And Thresholds streaming perceptron sample Geo Distance Calculation Aggregate \u00b6 Data Preprocessing Aggregate Over Time approximate distinctCountEver sample approximate distinctCount sample Receive And Count Aggregate Data Incrementally Store redis approximate count sample store mongodb Enrich \u00b6 Extrema Bottom K Gpl NLP Find Name Entity Type Join With Stored Data Script js sample store hbase Unit Conversion Extention Sample sentiment Extension Sample execution geo sample Clus Tree Test App Receive WebSocket In XML Format Receive \u00b6 Receive And Count Correlate \u00b6 Publish Mqtt In Xml Format SNMP Get Request App Receive JMS In Key value Format IBM Message Queue","title":"List of Streaming Integrator Samples"},{"location":"samples/sample/#list-of-streaming-integrator-samples","text":"","title":"List of Streaming Integrator Samples"},{"location":"samples/sample/#consume","text":"Recieve TCP in JSON Format Publish Google PubSub Messages In Text Format CSV Default Mapping CDC With Listening Mode Receive Google PubSub Messages In Text Format Receive MQTT In XML Format Receive TCP in Binary Format Receive Kafka In Binary Format Receive HTTP In XML Format With Default Mapping CDC With Polling Mode Twitter Polling sample Publish Http OAuth Request With Refresh Token Publish Http OAuth Request Response Store solr Receive WSO2 Events Receive Events From File Receive Email In Xml Format Receive Rabbitmq In JSON Format Store cassandra","title":"Consume"},{"location":"samples/sample/#transform","text":"Publish Http OAuth Request Publish Http OAuth Request Without Access Token Receive Hl7 In ER7 Format Data Preprocessing Publish Hl7 In ER7 Format Sweet Production Data Pre-processing CSV Custom Mapping Publish HTTP In Json Format With Custom Mapping Publish Email In Text Format Receive Kafka In Text Format With Custom Mapping Script js sample Publish Kafka In Custom Avro Format Receive Hl7 In Xml Format Regex Execution Sample Kalman Filter Execution Sample Publish Hl7 In Xml Format Publish Kafka In Json Format Publish Events To File Receive HTTP in XML Format With Custom Mapping Publish Rabbitmq In Xml Format Math Extension Sample Receive TCP in Text Format Publish Email In Xml Format Time Series Extension Sample Twitter Streaming sample String Extension Sample Text Mapping With In memory Transport Receive HTTP In Json Format With Default Mapping Hello Kafka Publish Http In Xml Format With Custom Mapping SNMP Set Request App Publish Tcp In Json Format Publish Http OAuth Request With OAuth User Map Extension Sample Publish Jms In Xml Format Receive JMS In Json Format Receive HTTP In Json Format With Custom Mapping","title":"Transform"},{"location":"samples/sample/#publish","text":"Publish Tcp In Text Format Receive Hl7 In ER7 Format Publish Google PubSub Messages In Text Format Publish Tcp In Binary Format Publish Web Socket In Xml Format Publish Prometheus Metrics HTTP Server store influxdb Publish Kafka In BinaryFormat Publish Jms In Key value Format Http Request Response Sample Publish WSO2 Events Publish Http In Xml Format Publish Http OAuth Request With Refresh Token Publish Http OAuth Request Response Store solr Publish Http In Json Format Store rdbms Store cassandra Publish Kafka In Avro Format Using Schema Registry","title":"Publish"},{"location":"samples/sample/#insights","text":"Pmml Model Processor Streaming Regressor Gpl NLP Find Name Entity Type Tensor Flow TestApp Markov Chain Sample Priority Extension Sample approximate distinctCountEver sample approximate distinctCount sample sentiment Extension Sample Pattern Matching Streaming KMeans Sample R Script Sample Receive Prometheus Metrics hoeffding adaptive tree sample Alerts And Thresholds streaming perceptron sample Geo Distance Calculation","title":"Insights"},{"location":"samples/sample/#aggregate","text":"Data Preprocessing Aggregate Over Time approximate distinctCountEver sample approximate distinctCount sample Receive And Count Aggregate Data Incrementally Store redis approximate count sample store mongodb","title":"Aggregate"},{"location":"samples/sample/#enrich","text":"Extrema Bottom K Gpl NLP Find Name Entity Type Join With Stored Data Script js sample store hbase Unit Conversion Extention Sample sentiment Extension Sample execution geo sample Clus Tree Test App Receive WebSocket In XML Format","title":"Enrich"},{"location":"samples/sample/#receive","text":"Receive And Count","title":"Receive"},{"location":"samples/sample/#correlate","text":"Publish Mqtt In Xml Format SNMP Get Request App Receive JMS In Key value Format IBM Message Queue","title":"Correlate"},{"location":"samples/sentimentExtensionSample/","text":"Purpose: \u00b6 This application demonstrates calculating the sentiment value for a given string as per Affin word list. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following message is shown on the console. * Siddhi App sentimentExtensionSample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console. * sentimentExtensionSample.siddhi - Started Successfully! Testing the Sample: \u00b6 Send events through one or more of the following methods. Send events to userWallPostStream , via event simulator. \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: sentimentExtensionSample Stream Name: userWallPostStream In the userId and text fields, enter the following and then click Send to send the event. userId: \"Mohan\" text: \"David is a good person. David is a bad person\" Send another event. userId: \"Nuwan\", text: \"David is a good person.\" Send events to the simulator http endpoint through the curl commands. \u00b6 Open a new terminal and issue the following commands one after the other: curl -X POST \\ http://localhost:9390/simulation/single \\ -u admin:admin \\ -H 'content-type: text/plain' \\ -d '{ \"siddhiAppName\": \"sentimentExtensionSample\", \"streamName\": \"userWallPostStream\", \"timestamp\": null, \"data\": [ \"Mohan\", \"David is a good person. David is a bad person\" ] }' curl -X POST \\ http://localhost:9390/simulation/single \\ -u admin:admin \\ -H 'content-type: text/plain' \\ -d '{ \"siddhiAppName\": \"sentimentExtensionSample\", \"streamName\": \"userWallPostStream\", \"timestamp\": null, \"data\": [ \"Nuwan\", \"David is a good person.\" ] }' If there is no error, the following messages are shown on the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman. \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make 'POST' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain'. Set the following request body in text and send the first event. { \"streamName\": \"userWallPostStream\", \"siddhiAppName\": \"sentimentExtensionSample\", \"data\": [ \"Mohan\", \"David is a good person. David is a bad person\" ] } If there are no errors, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\" Send another event by setting the request body in text. { \"streamName\": \"userWallPostStream\", \"siddhiAppName\": \"sentimentExtensionSample\", \"data\": [ \"Nuwan\", \"David is a good person.\" ] } Viewing the Results: \u00b6 See the output on the terminal. INFO {io.siddhi.core.stream.output.sink.LogSink} - sentimentExtensionSample : outputStream : Event{timestamp=1513619669217, data=[Mohan , David is a good person. David is a bad person, 0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - sentimentExtensionSample : outputStream : Event{timestamp=1513619687568, data=[Nuwan , David is a good person., 3], isExpired=false} @APP:name('sentimentExtensionSample') @App:Description('Demonstrates calculating the sentiment value for a given string as per Affin word list..') -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with SP editor. define stream userWallPostStream (userId string, text string); @sink(type='log') define stream outputStream(userId string, text string, rate int) ; from userWallPostStream select userId, text, sentiment:getRate(text) as rate insert into outputStream;","title":"Calculating the Sentiment Value for a Given String"},{"location":"samples/sentimentExtensionSample/#purpose","text":"This application demonstrates calculating the sentiment value for a given string as per Affin word list.","title":"Purpose:"},{"location":"samples/sentimentExtensionSample/#prerequisites","text":"Save this sample. If there is no syntax error, the following message is shown on the console. * Siddhi App sentimentExtensionSample successfully deployed.","title":"Prerequisites:"},{"location":"samples/sentimentExtensionSample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console. * sentimentExtensionSample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/sentimentExtensionSample/#testing-the-sample","text":"Send events through one or more of the following methods.","title":"Testing the Sample:"},{"location":"samples/sentimentExtensionSample/#send-events-to-userwallpoststream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: sentimentExtensionSample Stream Name: userWallPostStream In the userId and text fields, enter the following and then click Send to send the event. userId: \"Mohan\" text: \"David is a good person. David is a bad person\" Send another event. userId: \"Nuwan\", text: \"David is a good person.\"","title":"Send events to userWallPostStream, via event simulator."},{"location":"samples/sentimentExtensionSample/#send-events-to-the-simulator-http-endpoint-through-the-curl-commands","text":"Open a new terminal and issue the following commands one after the other: curl -X POST \\ http://localhost:9390/simulation/single \\ -u admin:admin \\ -H 'content-type: text/plain' \\ -d '{ \"siddhiAppName\": \"sentimentExtensionSample\", \"streamName\": \"userWallPostStream\", \"timestamp\": null, \"data\": [ \"Mohan\", \"David is a good person. David is a bad person\" ] }' curl -X POST \\ http://localhost:9390/simulation/single \\ -u admin:admin \\ -H 'content-type: text/plain' \\ -d '{ \"siddhiAppName\": \"sentimentExtensionSample\", \"streamName\": \"userWallPostStream\", \"timestamp\": null, \"data\": [ \"Nuwan\", \"David is a good person.\" ] }' If there is no error, the following messages are shown on the terminal. {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl commands."},{"location":"samples/sentimentExtensionSample/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make 'POST' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain'. Set the following request body in text and send the first event. { \"streamName\": \"userWallPostStream\", \"siddhiAppName\": \"sentimentExtensionSample\", \"data\": [ \"Mohan\", \"David is a good person. David is a bad person\" ] } If there are no errors, the following messages are shown on the console: * \"status\": \"OK\", * \"message\": \"Single Event simulation started successfully\" Send another event by setting the request body in text. { \"streamName\": \"userWallPostStream\", \"siddhiAppName\": \"sentimentExtensionSample\", \"data\": [ \"Nuwan\", \"David is a good person.\" ] }","title":"Publish events with Postman."},{"location":"samples/sentimentExtensionSample/#viewing-the-results","text":"See the output on the terminal. INFO {io.siddhi.core.stream.output.sink.LogSink} - sentimentExtensionSample : outputStream : Event{timestamp=1513619669217, data=[Mohan , David is a good person. David is a bad person, 0], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - sentimentExtensionSample : outputStream : Event{timestamp=1513619687568, data=[Nuwan , David is a good person., 3], isExpired=false} @APP:name('sentimentExtensionSample') @App:Description('Demonstrates calculating the sentiment value for a given string as per Affin word list..') -- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with SP editor. define stream userWallPostStream (userId string, text string); @sink(type='log') define stream outputStream(userId string, text string, rate int) ; from userWallPostStream select userId, text, sentiment:getRate(text) as rate insert into outputStream;","title":"Viewing the Results:"},{"location":"samples/store-hbase/","text":"Purpose: \u00b6 This application demonstrates how to perform CRUD operations in HBase stores using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store. * Search - searchSweetProductionStream * insert - insertSweetProductionStream * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store). Prerequisites: \u00b6 Ensure that HBase is installed and the server is running on your machine. Add HBase client into {WSO2_SI_HOME}/lib as follows: Download the following jars: HBase client (http://central.maven.org/maven2/org/apache/hbase/hbase-shaded-client/1.3.1/hbase-shaded-client-1.3.1.jar) Apache HTrace core (http://central.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar) Use the jartobundle tool in {WSO2_SI_Home}/bin to convert the above jar into a OSGi bundle. For Windows: <SI_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux: <SI_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> Copy the converted bundles to the {WSO2_SI_Home}/lib directory. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console: store-hbase.siddhi - Started Successfully! Testing the Sample: \u00b6 Simulate single events as follows: Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'. For the Siddhi App Name, select 'store-hbase'. For the Stream Name, select searchSweetProductionStream . Enter attribute values and click Send. Send an event where the name matches a name value in the data you just inserted to the SweetProductionTable . This will satisfy the 'on' condition of the join query. Notes: \u00b6 You can send events to the other corresponding streams to add, delete, update, insert, and search events. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"store-hbase\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type='hbase', hbase.zookeeper.quorum='localhost') --@primaryKey('name') define table SweetProductionTable (name string, amount double); /* Inserting event into the store */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from the store */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from the store */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from the store */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in store */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Receiving Events via Simulator and Persisting in a Store"},{"location":"samples/store-hbase/#purpose","text":"This application demonstrates how to perform CRUD operations in HBase stores using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store. * Search - searchSweetProductionStream * insert - insertSweetProductionStream * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream (verifies whether all the attributes that enter in the stream exists in the store).","title":"Purpose:"},{"location":"samples/store-hbase/#prerequisites","text":"Ensure that HBase is installed and the server is running on your machine. Add HBase client into {WSO2_SI_HOME}/lib as follows: Download the following jars: HBase client (http://central.maven.org/maven2/org/apache/hbase/hbase-shaded-client/1.3.1/hbase-shaded-client-1.3.1.jar) Apache HTrace core (http://central.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar) Use the jartobundle tool in {WSO2_SI_Home}/bin to convert the above jar into a OSGi bundle. For Windows: <SI_HOME>/bin/jartobundle.bat <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> For Linux: <SI_HOME>/bin/jartobundle.sh <PATH_OF_DOWNLOADED_JAR> <PATH_OF_CONVERTED_JAR> Copy the converted bundles to the {WSO2_SI_Home}/lib directory. Save this sample.","title":"Prerequisites:"},{"location":"samples/store-hbase/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console: store-hbase.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/store-hbase/#testing-the-sample","text":"Simulate single events as follows: Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'. For the Siddhi App Name, select 'store-hbase'. For the Stream Name, select searchSweetProductionStream . Enter attribute values and click Send. Send an event where the name matches a name value in the data you just inserted to the SweetProductionTable . This will satisfy the 'on' condition of the join query.","title":"Testing the Sample:"},{"location":"samples/store-hbase/#notes","text":"You can send events to the other corresponding streams to add, delete, update, insert, and search events. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Notes:"},{"location":"samples/store-hbase/#viewing-the-results","text":"See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"store-hbase\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type='hbase', hbase.zookeeper.quorum='localhost') --@primaryKey('name') define table SweetProductionTable (name string, amount double); /* Inserting event into the store */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from the store */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from the store */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from the store */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in store */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Viewing the Results:"},{"location":"samples/store-influxdb/","text":"Purpose: \u00b6 This application demostrates how to perform InfluxDB operations using Siddhi queries.The following streams can be used to insert, search, delete, and update or insert data into the InfluxDB store. insert - insertStream,stockStream delete - deleteStream search - searchStream update or insert - updateStream contains - containStream (verifies whether all the attributes that enter in the stream exists in the store). Prerequisites: \u00b6 Ensure that InfluxDB is installed in your machine (https://portal.influxdata.com/downloads/) Create a database named 'aTimeSeries' in InfluxDB. Replace influxdb.database in store configuration of this application with this database name. In the store configuration of this application, replace username and password with your influxDB credentials. Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console, store-influxdb.siddhi - Started Successfully! Note: \u00b6 If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again. Testing the Sample : \u00b6 Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'store-influxdb' as 'Siddhi App Name' and select 'insertStream' as 'Stream Name'. Provide attribute values, and then click send. Send events to the other corresponding streams to delete, update or insert, and search events. Notes: \u00b6 After a change in the store, you can use the search stream to see whether the operation is successful. Viewing the Results : \u00b6 You can use searchStream to check for inserted,deleted, and updated events. See the console for output events for searchStream. @App:name(\"store-influxdb\") @App:description(\"Perform inserting,deleting,updating Or inserting and reading events from influxDB store\") define stream stockStream (symbol string,volume long,price float,time long); define stream insertStream(symbol string,volume long,price float); define stream deleteStream(symbol string); define stream updateStream(symbol string,volume long,price float,time long); define stream searchStream(symbols string); define stream containStream(name string,value long); @sink (type='log') define stream OutputStream (checkName string, checkCategory float, checkVolume long,checkTime long); @sink (type='log') define stream logStream(name string,value long); @Store (type = \"influxdb\", url = \"http://localhost:8086\", username = \"root\", password = \"root\" , influxdb.database =\"aTimeSeries\") @Index(\"time\",\"symbol\") define table StockTable(symbol string,volume long,price float,time long) ; /* Inserting event into influxDB store */ @info(name='query0') from insertStream select symbol,volume,price,currentTimeMillis() as time insert into StockTable; /* Inserting event into influxDB store */ @info(name = 'query1') from stockStream select symbol, volume, price, time insert into StockTable ; /* deleting events from influxDB store */ @info(name= 'query2') from deleteStream delete StockTable on StockTable.symbol==symbol; /* Inserting or updating event into influxDB store */ @info(name='query3') from updateStream#window.timeBatch(1 sec) update or insert into StockTable on StockTable.symbol==symbol and StockTable.time==time; /* Reading events from influxDB store */ @info(name = 'query4') from searchStream#window.length(1) join StockTable on StockTable.symbol==symbols select StockTable.symbol as checkName, StockTable.price as checkCategory, StockTable.volume as checkVolume,StockTable.time as checkTime insert into OutputStream; /* Siddhi In in influxDB store */ @info(name = 'query6') from containStream [(StockTable.symbol == name) in StockTable] insert into logStream;","title":"Working with an influxDB Store"},{"location":"samples/store-influxdb/#purpose","text":"This application demostrates how to perform InfluxDB operations using Siddhi queries.The following streams can be used to insert, search, delete, and update or insert data into the InfluxDB store. insert - insertStream,stockStream delete - deleteStream search - searchStream update or insert - updateStream contains - containStream (verifies whether all the attributes that enter in the stream exists in the store).","title":"Purpose:"},{"location":"samples/store-influxdb/#prerequisites","text":"Ensure that InfluxDB is installed in your machine (https://portal.influxdata.com/downloads/) Create a database named 'aTimeSeries' in InfluxDB. Replace influxdb.database in store configuration of this application with this database name. In the store configuration of this application, replace username and password with your influxDB credentials. Save this sample.","title":"Prerequisites:"},{"location":"samples/store-influxdb/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console, store-influxdb.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/store-influxdb/#note","text":"If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.","title":"Note:"},{"location":"samples/store-influxdb/#testing-the-sample","text":"Simulate single events: Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation' Select 'store-influxdb' as 'Siddhi App Name' and select 'insertStream' as 'Stream Name'. Provide attribute values, and then click send. Send events to the other corresponding streams to delete, update or insert, and search events.","title":"Testing the Sample :"},{"location":"samples/store-influxdb/#notes","text":"After a change in the store, you can use the search stream to see whether the operation is successful.","title":"Notes:"},{"location":"samples/store-influxdb/#viewing-the-results","text":"You can use searchStream to check for inserted,deleted, and updated events. See the console for output events for searchStream. @App:name(\"store-influxdb\") @App:description(\"Perform inserting,deleting,updating Or inserting and reading events from influxDB store\") define stream stockStream (symbol string,volume long,price float,time long); define stream insertStream(symbol string,volume long,price float); define stream deleteStream(symbol string); define stream updateStream(symbol string,volume long,price float,time long); define stream searchStream(symbols string); define stream containStream(name string,value long); @sink (type='log') define stream OutputStream (checkName string, checkCategory float, checkVolume long,checkTime long); @sink (type='log') define stream logStream(name string,value long); @Store (type = \"influxdb\", url = \"http://localhost:8086\", username = \"root\", password = \"root\" , influxdb.database =\"aTimeSeries\") @Index(\"time\",\"symbol\") define table StockTable(symbol string,volume long,price float,time long) ; /* Inserting event into influxDB store */ @info(name='query0') from insertStream select symbol,volume,price,currentTimeMillis() as time insert into StockTable; /* Inserting event into influxDB store */ @info(name = 'query1') from stockStream select symbol, volume, price, time insert into StockTable ; /* deleting events from influxDB store */ @info(name= 'query2') from deleteStream delete StockTable on StockTable.symbol==symbol; /* Inserting or updating event into influxDB store */ @info(name='query3') from updateStream#window.timeBatch(1 sec) update or insert into StockTable on StockTable.symbol==symbol and StockTable.time==time; /* Reading events from influxDB store */ @info(name = 'query4') from searchStream#window.length(1) join StockTable on StockTable.symbol==symbols select StockTable.symbol as checkName, StockTable.price as checkCategory, StockTable.volume as checkVolume,StockTable.time as checkTime insert into OutputStream; /* Siddhi In in influxDB store */ @info(name = 'query6') from containStream [(StockTable.symbol == name) in StockTable] insert into logStream;","title":"Viewing the Results :"},{"location":"samples/store-mongodb/","text":"Purpose: \u00b6 This application demonstrates how to perform CRUD operations using Siddhi queries in MongoDB stores. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to insert, update, search, delete and update or insert the existing data in the store. * Search - searchSweetProductionStream * insert - insertSweetProductionStream`` * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream` (verifies whether all the attributes that enter in the stream exists in the store). Prerequisites: \u00b6 Ensure that MongoDB is installed on your machine. (https://docs.mongodb.com/manual/administration/install-community/). If auth is not enabled in the MongoDB instance, skip steps 3 and 4. Create a data store named production in MongoD with relevant access privileges. Create a collection named SweetProductionTable and insert values into SweetProductionTable . Save this sample. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console: store-mongodb.siddhi - Started Successfully! Note: \u00b6 If you want to edit this application, stop the application, make your edits and save, and then start the application again. Testing the Sample: \u00b6 Simulate single events as follows: Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'. For the Siddhi App Name, select 'store-mongodb'. For the Stream Name, select 'searchSweetProductionStream'. Enter attribute values and click Send. Send an event where the name matches a name value in the data you just inserted to the SweetProductionTable . This will satisfy the on condition of the join query. Notes: \u00b6 You can send events to the other corresponding streams to add, delete, update, insert, and search events. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function) Viewing the Results: \u00b6 See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"store-mongodb.siddhi\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type=\"mongodb\",mongodb.uri='mongodb://localhost/production') --@PrimaryKey(\"name\") @IndexBy(\"amount {background:true}\") define table SweetProductionTable (name string, amount double); /* Inserting event into the mongo store */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from mongo store */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from mongo store */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from mongo store */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in mongo store */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Receiving Events and Persisting in MongoDB Store"},{"location":"samples/store-mongodb/#purpose","text":"This application demonstrates how to perform CRUD operations using Siddhi queries in MongoDB stores. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using insertSweetProductionStream . The following streams can be used to insert, update, search, delete and update or insert the existing data in the store. * Search - searchSweetProductionStream * insert - insertSweetProductionStream`` * delete - deleteSweetProductionStream * update - updateSweetProductionStream * update or insert - updateOrInsertSweetProductionStream * contains - containsSweetProductionStream` (verifies whether all the attributes that enter in the stream exists in the store).","title":"Purpose:"},{"location":"samples/store-mongodb/#prerequisites","text":"Ensure that MongoDB is installed on your machine. (https://docs.mongodb.com/manual/administration/install-community/). If auth is not enabled in the MongoDB instance, skip steps 3 and 4. Create a data store named production in MongoD with relevant access privileges. Create a collection named SweetProductionTable and insert values into SweetProductionTable . Save this sample.","title":"Prerequisites:"},{"location":"samples/store-mongodb/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following message is shown on the console: store-mongodb.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/store-mongodb/#note","text":"If you want to edit this application, stop the application, make your edits and save, and then start the application again.","title":"Note:"},{"location":"samples/store-mongodb/#testing-the-sample","text":"Simulate single events as follows: Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'. For the Siddhi App Name, select 'store-mongodb'. For the Stream Name, select 'searchSweetProductionStream'. Enter attribute values and click Send. Send an event where the name matches a name value in the data you just inserted to the SweetProductionTable . This will satisfy the on condition of the join query.","title":"Testing the Sample:"},{"location":"samples/store-mongodb/#notes","text":"You can send events to the other corresponding streams to add, delete, update, insert, and search events. The Primary Key constraint in SweetProductionTable is disabled, because the name cannot be used as a PrimaryKey in a ProductionTable . You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)","title":"Notes:"},{"location":"samples/store-mongodb/#viewing-the-results","text":"See the output for raw materials on the console. You can use searchSweetProductionStream to check for inserted, deleted, and updated events. @App:name(\"store-mongodb.siddhi\") @App:description('Receive events via simulator and persist the received data in the store.') define stream insertSweetProductionStream (name string, amount double); define stream deleteSweetProductionStream (name string); define stream searchSweetProductionStream (name string); define stream updateSweetProductionStream (name string, amount double); define stream updateOrInsertSweetProductionStream (name string, amount double); define stream containsSweetProductionStream (name string, amount double); @sink(type='log') define stream logStream(name string, amount double); @Store(type=\"mongodb\",mongodb.uri='mongodb://localhost/production') --@PrimaryKey(\"name\") @IndexBy(\"amount {background:true}\") define table SweetProductionTable (name string, amount double); /* Inserting event into the mongo store */ @info(name='query1') from insertSweetProductionStream insert into SweetProductionTable; /* Deleting event from mongo store */ @info(name = 'query2') from deleteSweetProductionStream delete SweetProductionTable on SweetProductionTable.name == name ; /* Updating event from mongo store */ @info(name = 'query3') from updateSweetProductionStream update SweetProductionTable on SweetProductionTable.name == name ; /* Updating or inserting event from mongo store */ @info(name = 'query4') from updateOrInsertSweetProductionStream update or insert into SweetProductionTable on SweetProductionTable.name == name; /* Siddhi In in mongo store */ @info(name = 'query5') from containsSweetProductionStream [(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable] insert into logStream; --Perform a join on raw material name so that the data in the store can be viewed @info(name='query6') from searchSweetProductionStream as s join SweetProductionTable as sp on s.name == sp.name select sp.name, sp.amount insert into logStream;","title":"Viewing the Results:"},{"location":"samples/streaming-perceptron-sample/","text":"Purpose: \u00b6 This application demonstrates how to configure WSO2 Streaming Integrator Tooling to perform binary classification using a streaming Perceptron. Prerequisites: \u00b6 Save this sample. If there is no syntax error, the following message is shown on the console. * Siddhi App streaming-perceptron-sample successfully deployed. Executing the Sample: \u00b6 Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console. * streaming-perceptron-sample.siddhi - Started Successfully! Testing the Sample: \u00b6 Note: The Streaming Perceptron for streaming machine learning needs to be trained prior to perform prediction. Training phase \u00b6 Send events through one or more of the following methods. Send events to ProductionTrainStream, via event simulator. \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-perceptron-sample Stream Name: ProductionTrainStream In the name and amount fields, enter the following and then click Send to send the event. density: 50.4 emperature: 30.03 Send some more events. Send events to the simulator http endpoint through the curl command: \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Testing phase \u00b6 Send events through one or more of the following methods. Send events to ProductionInputStream, via event simulator \u00b6 Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-perceptron-sample Stream Name: ProductionInputStream In the name and amount fields, enter the following and then click Send to send the event. density: 30.4 emperature: 20.5 Send some more events. Send events to the simulator http endpoint through the curl command: \u00b6 Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"} Publish events with Postman: \u00b6 Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\" Viewing the Results: \u00b6 See the output on the terminal: INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - streaming-perceptron-sample: LOGGER, StreamEvent{ timestamp=1513596699142, beforeWindowData=null, onAfterWindowData=null, outputData=[34.0, 12.0, false, 0.0], type=CURRENT, next=null} @App:name(\"streaming-perceptron-sample\") @App:description('Train a streaming Perceptron model to predict whether an item passes quality check.') define stream ProductionTrainStream (density double, temperature double, qualityCheck_pass bool ); define stream ProductionInputStream (density double, temperature double); @sink(type='log') define stream PredictedQCStream (density double, temperature double, prediction bool, confidenceLevel double); @info(name = 'query-train') from ProductionTrainStream#streamingml:updatePerceptronClassifier('QCmodel', qualityCheck_pass, 0.1, density, temperature) select * insert into trainOutputStream; @info(name = 'query-predict') from ProductionInputStream#streamingml:perceptronClassifier('QCmodel', 0.0, 0.5, density, temperature) select * insert into PredictedQCStream;","title":"Making Predictions via a Streaming Perceptron Model"},{"location":"samples/streaming-perceptron-sample/#purpose","text":"This application demonstrates how to configure WSO2 Streaming Integrator Tooling to perform binary classification using a streaming Perceptron.","title":"Purpose:"},{"location":"samples/streaming-perceptron-sample/#prerequisites","text":"Save this sample. If there is no syntax error, the following message is shown on the console. * Siddhi App streaming-perceptron-sample successfully deployed.","title":"Prerequisites:"},{"location":"samples/streaming-perceptron-sample/#executing-the-sample","text":"Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console. * streaming-perceptron-sample.siddhi - Started Successfully!","title":"Executing the Sample:"},{"location":"samples/streaming-perceptron-sample/#testing-the-sample","text":"Note: The Streaming Perceptron for streaming machine learning needs to be trained prior to perform prediction.","title":"Testing the Sample:"},{"location":"samples/streaming-perceptron-sample/#training-phase","text":"Send events through one or more of the following methods.","title":"Training phase"},{"location":"samples/streaming-perceptron-sample/#send-events-to-productiontrainstream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-perceptron-sample Stream Name: ProductionTrainStream In the name and amount fields, enter the following and then click Send to send the event. density: 50.4 emperature: 30.03 Send some more events.","title":"Send events to ProductionTrainStream, via event simulator."},{"location":"samples/streaming-perceptron-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl command:"},{"location":"samples/streaming-perceptron-sample/#publish-events-with-postman","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/streaming-perceptron-sample/#testing-phase","text":"Send events through one or more of the following methods.","title":"Testing phase"},{"location":"samples/streaming-perceptron-sample/#send-events-to-productioninputstream-via-event-simulator","text":"Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. In the Single Simulation tab of the panel, specify the values as follows: Siddhi App Name: streaming-perceptron-sample Stream Name: ProductionInputStream In the name and amount fields, enter the following and then click Send to send the event. density: 30.4 emperature: 20.5 Send some more events.","title":"Send events to ProductionInputStream, via event simulator"},{"location":"samples/streaming-perceptron-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command_1","text":"Open a new terminal and issue the following command: curl -X POST -d '{\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' If there is no error, the following messages are shown on the terminal: {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}","title":"Send events to the simulator http endpoint through the curl command:"},{"location":"samples/streaming-perceptron-sample/#publish-events-with-postman_1","text":"Install 'Postman' application from Chrome web store. Launch the application. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]} Click 'send'. If there is no error, the following messages are shown on the console: \"status\": \"OK\", \"message\": \"Single Event simulation started successfully\"","title":"Publish events with Postman:"},{"location":"samples/streaming-perceptron-sample/#viewing-the-results","text":"See the output on the terminal: INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - streaming-perceptron-sample: LOGGER, StreamEvent{ timestamp=1513596699142, beforeWindowData=null, onAfterWindowData=null, outputData=[34.0, 12.0, false, 0.0], type=CURRENT, next=null} @App:name(\"streaming-perceptron-sample\") @App:description('Train a streaming Perceptron model to predict whether an item passes quality check.') define stream ProductionTrainStream (density double, temperature double, qualityCheck_pass bool ); define stream ProductionInputStream (density double, temperature double); @sink(type='log') define stream PredictedQCStream (density double, temperature double, prediction bool, confidenceLevel double); @info(name = 'query-train') from ProductionTrainStream#streamingml:updatePerceptronClassifier('QCmodel', qualityCheck_pass, 0.1, density, temperature) select * insert into trainOutputStream; @info(name = 'query-predict') from ProductionInputStream#streamingml:perceptronClassifier('QCmodel', 0.0, 0.5, density, temperature) select * insert into PredictedQCStream;","title":"Viewing the Results:"},{"location":"setup/configuring-data-sources/","text":"Configuring Data Sources \u00b6 In the Streaming Integrator, there are datasources specific to both the Streaming Integrator server and Streaming Integrator Tooling. The data sources of each runtime are defined in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. e.g., To configure a data source in the server runtime, the relevant configurations need to be added in the <SI_Home>/conf/server/deployment.yaml file. To view a sample data source configuration for each database type supported, expand the following sections: Info If the database driver is not an OSGI bundle, then it should be converted to OSGI (using jartobundle.sh) before placing it in the <SI_HOME>|<SI_TOOLING_HOME>/lib directory. For detailed instructions,see Adding Third Party Non OSGi Libraries . e.g., sh WSO2_SI_HOME/bin/jartobundle.sh ojdbc6.jar WSO2_SI_HOME/lib/ The database should be tuned to handle the total maxPoolSize (The maximum number of threads that should be reserved at any given time to handle events) that is defined in the deployment.yaml file. MySQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false POSTGRES wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle There are two ways to configure this database type. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The Oracle driver need to be converted to OSGi (using jartobundle.sh ) before put into SI_HOME/lib directory. For detailed instructions, see Adding Third Party Non OSGi Libraries . MSSQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following tables explain the default data sources configured in the Streaming Integrator components for different purposes, and how to change them. RDBMS data provider \u00b6 Database Access Requirement The RDBMS provider publishes records from RDBMS tables into generated widgets. It can also be configured to purge records in tables. In order to carry out these actions, this provider requires access to read and delete records in user defined tables of the database. For more information about the RDBMS data provider, see Generating Widgets . Required/Optional This is required if you select a datasource when generating the widget or use existing widgets that connect to the RDBMS data provider when you run the dashboard server for the Streaming Integrator. Default Datasource Name SAMPLE_DB Default Database The default H2 database location is <DASHBOARD_HOME>/wso2/dashboard/database/SAMPLE_DB . Tables The default database shipped with a sample table named TRANSACTION_TABLE . Schemas and Queries The schema for the sample table is TRANSACTIONS_TABLE (creditCardNo VARCHAR(50), country VARCHAR(50), transaction VARCHAR(50), amount INT) The default queries can be viewed here . Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g Carbon coordination \u00b6 Database Access Requirement Carbon coordination supports zookeeper and RDBMS based coordination. In RDBMS coordination, database access is required for updating the heartbeats of the nodes. In addition, database access is required to update the coordinator and the other members in the cluster. For more information, see Configuring Cluster Coordination . Required/Optional This is required. However, you can also use Zookeeper coordination instead of RDBMS. Default Datasource Name WSO2_CARBON_DB Tables LEADER_STATUS_TABLE , MEMBERSHIP_EVENT_TABLE , REMOVED_MEMBERS_TABLE , CLUSTER_NODE_STATUS_TABLE Schemas and Queries Information about the default queries and the schema can be viewed here . Tested Database Types MySQL, Postgres, Mssql, Oracle 11g Streaming Integrator core - persistence \u00b6 Database Access Requirement This involves persisting the state of Siddhi Applications periodically in the database. State persistence is enabled by selecting the org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore class in the state.persistence section of the <SI_Home>/conf/<server>/deployment.yaml file. For more information, see Configuring Database and File System State Persistence . Required/Optional This is optional. WSO2 is configured to persist the state of Siddhi applications by default. Default Datasource Name N/A. If state persistence is required, you need to configure the datasource in the <SI_Home>/conf/<server>/deployment.yaml file under state.persistence > config > datasource . Tables N/A. If state persistence is required, you need to specify the table name to be used when persisting the state in the <SI_Home>/conf/<server>/deployment.yaml file under state.persistence > config > table . Schemas and Queries Information about the default queries and schema can be viewed here . Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g Streaming Integrator - Status Dashboard \u00b6 Database Access Requirement To display information relating to the status of your Streaming Integrator deployment, the Status Dashboard needs to retrieve carbon metrics data, registered Streaming Integrator server details and authentication details within the cluster from the database. For more information, see Monitoring Stream Processor . Required/Optional Required Default Datasource Name WSO2_STATUS_DASHBOARD_DB , WSO2_METRICS_DB Tables METRIC_COUNTER , METRIC_GAUGE , METRIC_HISTOGRAM , METRIC_METER , METRIC_TIMER , WORKERS_CONFIGURATIONS , WORKERS_DETAILS Schemas and Queries Information about the default queries and schema can be viewed [here](https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml). Tested Database Types H2, MySQL, Mssql, Oracle 11g ( Postgres is tested with Carbon-Metrics only) Siddhi RDBMS store \u00b6 Database Access Requirement It gives the capability of creating the tables at the siddhi application runtime and access the existing tables if a user-defined carbon data source or JNDI property in a siddhi application. Documentation can be found in [Siddhi Extensions Documentation](https://siddhi-io.github.io/siddhi-store-rdbms/api/latest/). Required/Optional Optional Default Datasource Name No such default Datasource. User has to create the datasource in the Siddhi application Tables No such default tables. User has to define the tables Schemas and Queries See [information about the default queries and schema](https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml). Tested Database Types H2, MySQL, Mssql, Oracle 11g, DB2, PostgreSQL Carbon Dashboards \u00b6 Database Access Requirement Carbon Dashboard feature uses its datasource to persist the dashboard related information Required/Optional Optional Default Datasource Name WSO2_DASHBOARD_DB Tables DASHBOARD_RESOURCES Schemas and Queries Tested Database Types H2, MySQL, Postgres Business Rules \u00b6 Database Access Requirement Business Rules feature uses database to persist the derived business rules Required/Optional Mandatory Default Datasource Name BUSINESS_RULES_DB Tables BUSINESS_RULES , RULES_TEMPLATES Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml). Tested Database Types H2, MySQL, Oracle 11g IdP client \u00b6 Database Access Requirement IdP client access the DB layer to persist the client id and the client secret of dynamic client registration Required/Optional Mandatory for external IdP client Default Datasource Name DB_AUTH_DB Tables OAUTH_APPS Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml) Tested Database Types H2, MySQL, Oracle 11g Permission provider \u00b6 Database Access Requirement Permission provider will access the DB to persist permissions and role - permission mappings. Required/Optional Mandatory, default is in H2 Default Datasource Name PERMISSIONS_DB Tables PERMISSIONS, ROLE_PERMISSIONS Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml) Tested Database Types H2, MySQL, Mssql, Oracle 11g , Postgres","title":"Configuring Datasources"},{"location":"setup/configuring-data-sources/#configuring-data-sources","text":"In the Streaming Integrator, there are datasources specific to both the Streaming Integrator server and Streaming Integrator Tooling. The data sources of each runtime are defined in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file. e.g., To configure a data source in the server runtime, the relevant configurations need to be added in the <SI_Home>/conf/server/deployment.yaml file. To view a sample data source configuration for each database type supported, expand the following sections: Info If the database driver is not an OSGI bundle, then it should be converted to OSGI (using jartobundle.sh) before placing it in the <SI_HOME>|<SI_TOOLING_HOME>/lib directory. For detailed instructions,see Adding Third Party Non OSGi Libraries . e.g., sh WSO2_SI_HOME/bin/jartobundle.sh ojdbc6.jar WSO2_SI_HOME/lib/ The database should be tuned to handle the total maxPoolSize (The maximum number of threads that should be reserved at any given time to handle events) that is defined in the deployment.yaml file. MySQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false POSTGRES wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle There are two ways to configure this database type. If you have a System Identifier (SID), use this (older) format: jdbc:oracle:thin:@[HOST][:PORT]:SID wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The Oracle driver need to be converted to OSGi (using jartobundle.sh ) before put into SI_HOME/lib directory. For detailed instructions, see Adding Third Party Non OSGi Libraries . MSSQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following tables explain the default data sources configured in the Streaming Integrator components for different purposes, and how to change them.","title":"Configuring Data Sources"},{"location":"setup/configuring-data-sources/#rdbms-data-provider","text":"Database Access Requirement The RDBMS provider publishes records from RDBMS tables into generated widgets. It can also be configured to purge records in tables. In order to carry out these actions, this provider requires access to read and delete records in user defined tables of the database. For more information about the RDBMS data provider, see Generating Widgets . Required/Optional This is required if you select a datasource when generating the widget or use existing widgets that connect to the RDBMS data provider when you run the dashboard server for the Streaming Integrator. Default Datasource Name SAMPLE_DB Default Database The default H2 database location is <DASHBOARD_HOME>/wso2/dashboard/database/SAMPLE_DB . Tables The default database shipped with a sample table named TRANSACTION_TABLE . Schemas and Queries The schema for the sample table is TRANSACTIONS_TABLE (creditCardNo VARCHAR(50), country VARCHAR(50), transaction VARCHAR(50), amount INT) The default queries can be viewed here . Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g","title":"RDBMS data provider"},{"location":"setup/configuring-data-sources/#carbon-coordination","text":"Database Access Requirement Carbon coordination supports zookeeper and RDBMS based coordination. In RDBMS coordination, database access is required for updating the heartbeats of the nodes. In addition, database access is required to update the coordinator and the other members in the cluster. For more information, see Configuring Cluster Coordination . Required/Optional This is required. However, you can also use Zookeeper coordination instead of RDBMS. Default Datasource Name WSO2_CARBON_DB Tables LEADER_STATUS_TABLE , MEMBERSHIP_EVENT_TABLE , REMOVED_MEMBERS_TABLE , CLUSTER_NODE_STATUS_TABLE Schemas and Queries Information about the default queries and the schema can be viewed here . Tested Database Types MySQL, Postgres, Mssql, Oracle 11g","title":"Carbon coordination"},{"location":"setup/configuring-data-sources/#streaming-integrator-core-persistence","text":"Database Access Requirement This involves persisting the state of Siddhi Applications periodically in the database. State persistence is enabled by selecting the org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore class in the state.persistence section of the <SI_Home>/conf/<server>/deployment.yaml file. For more information, see Configuring Database and File System State Persistence . Required/Optional This is optional. WSO2 is configured to persist the state of Siddhi applications by default. Default Datasource Name N/A. If state persistence is required, you need to configure the datasource in the <SI_Home>/conf/<server>/deployment.yaml file under state.persistence > config > datasource . Tables N/A. If state persistence is required, you need to specify the table name to be used when persisting the state in the <SI_Home>/conf/<server>/deployment.yaml file under state.persistence > config > table . Schemas and Queries Information about the default queries and schema can be viewed here . Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g","title":"Streaming Integrator core - persistence"},{"location":"setup/configuring-data-sources/#streaming-integrator-status-dashboard","text":"Database Access Requirement To display information relating to the status of your Streaming Integrator deployment, the Status Dashboard needs to retrieve carbon metrics data, registered Streaming Integrator server details and authentication details within the cluster from the database. For more information, see Monitoring Stream Processor . Required/Optional Required Default Datasource Name WSO2_STATUS_DASHBOARD_DB , WSO2_METRICS_DB Tables METRIC_COUNTER , METRIC_GAUGE , METRIC_HISTOGRAM , METRIC_METER , METRIC_TIMER , WORKERS_CONFIGURATIONS , WORKERS_DETAILS Schemas and Queries Information about the default queries and schema can be viewed [here](https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml). Tested Database Types H2, MySQL, Mssql, Oracle 11g ( Postgres is tested with Carbon-Metrics only)","title":"Streaming Integrator - Status Dashboard"},{"location":"setup/configuring-data-sources/#siddhi-rdbms-store","text":"Database Access Requirement It gives the capability of creating the tables at the siddhi application runtime and access the existing tables if a user-defined carbon data source or JNDI property in a siddhi application. Documentation can be found in [Siddhi Extensions Documentation](https://siddhi-io.github.io/siddhi-store-rdbms/api/latest/). Required/Optional Optional Default Datasource Name No such default Datasource. User has to create the datasource in the Siddhi application Tables No such default tables. User has to define the tables Schemas and Queries See [information about the default queries and schema](https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml). Tested Database Types H2, MySQL, Mssql, Oracle 11g, DB2, PostgreSQL","title":"Siddhi RDBMS store"},{"location":"setup/configuring-data-sources/#carbon-dashboards","text":"Database Access Requirement Carbon Dashboard feature uses its datasource to persist the dashboard related information Required/Optional Optional Default Datasource Name WSO2_DASHBOARD_DB Tables DASHBOARD_RESOURCES Schemas and Queries Tested Database Types H2, MySQL, Postgres","title":"Carbon Dashboards"},{"location":"setup/configuring-data-sources/#business-rules","text":"Database Access Requirement Business Rules feature uses database to persist the derived business rules Required/Optional Mandatory Default Datasource Name BUSINESS_RULES_DB Tables BUSINESS_RULES , RULES_TEMPLATES Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml). Tested Database Types H2, MySQL, Oracle 11g","title":"Business Rules"},{"location":"setup/configuring-data-sources/#idp-client","text":"Database Access Requirement IdP client access the DB layer to persist the client id and the client secret of dynamic client registration Required/Optional Mandatory for external IdP client Default Datasource Name DB_AUTH_DB Tables OAUTH_APPS Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml) Tested Database Types H2, MySQL, Oracle 11g","title":"IdP client"},{"location":"setup/configuring-data-sources/#permission-provider","text":"Database Access Requirement Permission provider will access the DB to persist permissions and role - permission mappings. Required/Optional Mandatory, default is in H2 Default Datasource Name PERMISSIONS_DB Tables PERMISSIONS, ROLE_PERMISSIONS Schemas and Queries See [information about schemas and queries](https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml) Tested Database Types H2, MySQL, Mssql, Oracle 11g , Postgres < /table","title":"Permission \u00a0provider"},{"location":"setup/defining-Data-Tables/","text":"Defining Data Tables \u00b6 This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for External Data Stores . Adding primary and index keys \u00b6 Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner. Adding primary keys \u00b6 Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')), those attributes would act as a composite primary key. Syntax \u00b6 @PrimaryKey('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example \u00b6 @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key. Adding indexes \u00b6 An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table. Syntax \u00b6 To index by a single attribute: @Index('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); To index by multiple attributes: @Index('<attribute_1>''<attribute_2>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example \u00b6 @Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute. Defining Tables for Physical Stores \u00b6","title":"Defining Data Tables"},{"location":"setup/defining-Data-Tables/#defining-data-tables","text":"This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for External Data Stores .","title":"Defining Data Tables"},{"location":"setup/defining-Data-Tables/#adding-primary-and-index-keys","text":"Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner.","title":"Adding primary and index keys"},{"location":"setup/defining-Data-Tables/#adding-primary-keys","text":"Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')), those attributes would act as a composite primary key.","title":"Adding primary keys"},{"location":"setup/defining-Data-Tables/#syntax","text":"@PrimaryKey('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>);","title":"Syntax"},{"location":"setup/defining-Data-Tables/#example","text":"@PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key.","title":"Example"},{"location":"setup/defining-Data-Tables/#adding-indexes","text":"An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table.","title":"Adding indexes"},{"location":"setup/defining-Data-Tables/#syntax_1","text":"To index by a single attribute: @Index('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); To index by multiple attributes: @Index('<attribute_1>''<attribute_2>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>);","title":"Syntax"},{"location":"setup/defining-Data-Tables/#example_1","text":"@Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute.","title":"Example"},{"location":"setup/defining-Data-Tables/#defining-tables-for-physical-stores","text":"","title":"Defining Tables for Physical Stores"},{"location":"setup/defining-Tables-for-Physical-Stores/","text":"Defining Tables for Physical Stores \u00b6 This section explains how to define data tables to store data handled by WSO2 Stream Processor in physical databases. T he @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); !!! info This method is not recommended in a production environment because is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs:) as shown in the example below. !!! info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this metho is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source defined in the wso2.datasources section of the file, <SP_HOME>/conf/<PROFILE>/deployment.yaml, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type='<DATABASE_TYPE>', datasource=\u2019<carbon.datasource.name>\u2019) !!! info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about definig datasources, see Configuring Datasources . The following database types are currently supported for WSO2 SP. Tip Before you begin: In order to create and use an event table to store data, the following should be completed: The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. A database instance should be started. The user IDs used to perform the required table operations should be granted the relevant privileges. The relevant JDBC Driver must be downloaded and the jar must be put in the <SP_HOME>/lib directory. RDBMS Apache HBase Apache Solr MongoDB RDBMS \u00b6 The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2 Query syntax \u00b6 The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\"<jdbc.url>\", username=\"<username>\", password=\"<password>\",pool.properties=\"<prop1>:<val1>,<prop2>:<val2>\") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table <table_name>(col1 datatype1, col2 datatype2, col3 datatype3); Parameters \u00b6 The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. Example \u00b6 The following is an example of an RDBMS table definition: @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); Apache HBase \u00b6 Query syntax \u00b6 The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\"<STRING>\", table.name=\"<STRING>\", column.family.name=\"<STRING>\") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") Parameters \u00b6 The following parameters are configured in the definition of an HBase event table: Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. Example \u00b6 @Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long); Apache Solr \u00b6 Query syntax \u00b6 The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url=<solr-cloud-zookeeper-url>, collection=<solr-collection-name>, base.config=<config-name>, shards=<no-of-shards>, replicas=<no-of-replicas>, schema=<schema-definition>, commit.async=true|false) define table Footable (time long, date string); Parameters \u00b6 The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A Example \u00b6 This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string); ## MongoDB #### Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\"<MONGODB CONNECTION URI>\") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\"<ATTRIBUTE_NAME> <SORTING ORDER> <INDEX OPTIONS>\") define table <TABLE_NME> (<ATTRIBUTE1_NAME> <ATTRIBUTE1_TYPE>, <ATTRIBUTE2_NAME> <ATTRIBUTE2_TYPE>, <ATTRIBUTE3_NAME> <ATTRIBUTE3_TYPE>, ...); The ` mongodb.uri ` parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. - ` @primary ` : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. - ` @index ` : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. #### Example The following query defines a MongoDB table named ` FooTable ` with the ` symbol ` , ` price ` , and ` volume ` attributes. The ` symbol ` attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long);","title":"Defining Tables for Physical Stores"},{"location":"setup/defining-Tables-for-Physical-Stores/#defining-tables-for-physical-stores","text":"This section explains how to define data tables to store data handled by WSO2 Stream Processor in physical databases. T he @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); !!! info This method is not recommended in a production environment because is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the <SP_HOME>/conf/<PROFILE>/deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs:) as shown in the example below. !!! info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this metho is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source defined in the wso2.datasources section of the file, <SP_HOME>/conf/<PROFILE>/deployment.yaml, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type='<DATABASE_TYPE>', datasource=\u2019<carbon.datasource.name>\u2019) !!! info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about definig datasources, see Configuring Datasources . The following database types are currently supported for WSO2 SP. Tip Before you begin: In order to create and use an event table to store data, the following should be completed: The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. A database instance should be started. The user IDs used to perform the required table operations should be granted the relevant privileges. The relevant JDBC Driver must be downloaded and the jar must be put in the <SP_HOME>/lib directory. RDBMS Apache HBase Apache Solr MongoDB","title":"Defining Tables for Physical Stores"},{"location":"setup/defining-Tables-for-Physical-Stores/#rdbms","text":"The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2","title":"RDBMS"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax","text":"The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\"<jdbc.url>\", username=\"<username>\", password=\"<password>\",pool.properties=\"<prop1>:<val1>,<prop2>:<val2>\") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table <table_name>(col1 datatype1, col2 datatype2, col3 datatype3);","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters","text":"The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list.","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example","text":"The following is an example of an RDBMS table definition: @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-hbase","text":"","title":"Apache HBase"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_1","text":"The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\"<STRING>\", table.name=\"<STRING>\", column.family.name=\"<STRING>\") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\")","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_1","text":"The following parameters are configured in the definition of an HBase event table: Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list.","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_1","text":"@Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-solr","text":"","title":"Apache Solr"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_2","text":"The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url=<solr-cloud-zookeeper-url>, collection=<solr-collection-name>, base.config=<config-name>, shards=<no-of-shards>, replicas=<no-of-replicas>, schema=<schema-definition>, commit.async=true|false) define table Footable (time long, date string);","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_2","text":"The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_2","text":"This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string); ## MongoDB #### Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\"<MONGODB CONNECTION URI>\") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\"<ATTRIBUTE_NAME> <SORTING ORDER> <INDEX OPTIONS>\") define table <TABLE_NME> (<ATTRIBUTE1_NAME> <ATTRIBUTE1_TYPE>, <ATTRIBUTE2_NAME> <ATTRIBUTE2_TYPE>, <ATTRIBUTE3_NAME> <ATTRIBUTE3_TYPE>, ...); The ` mongodb.uri ` parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. - ` @primary ` : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. - ` @index ` : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. #### Example The following query defines a MongoDB table named ` FooTable ` with the ` symbol ` , ` price ` , and ` volume ` attributes. The ` symbol ` attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/deploying-si-as-a-scalable-cluster/","text":"Scalable Deployment \u00b6 Scalable high availability deployment predominantly focuses on scaling the system according to the load or the TPS of the system. This is achieved with the help of horizontal scalability. WSO2 Streaming Integrator uses Siddhi as the streaming language. Siddhi allows you to write Siddhi logic in a stateless way as well as a stateful way. Stateless operations include filters, database operations etc., and stateful operations include window operations, aggregations etc., that keep data in memory to carry out calculations. The deployment options for a scalable streaming integrator depends on the statelessness and the statefulness of Siddhi applications. The following topics provide detailed descriptions of two approaches. System Requirements For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine . Stateless scalable high availability (HA) deployment \u00b6 In stateless scenarios, the system does not work with any in-memory state. Thus in order to scale, you can keep adding Streaming Integrator servers to the system and front them with a load balancer that publishes the events in round robin manner. This is depicted in the diagram below. Stateful scalable high availability (HA) deployment \u00b6 As described before, stateful operations keep state data in memory. Therefore, in order to scale such a system, you need to process specific data on the same node without processing same-state data in different servers. You can achieve this via data partitioning where one bucket of partitioned data is processed only in one specific server. Info In order to scale stateful operations, it is required to have some partitioning attribute available enabling the partitioned data to be processed independently. The following is a high level diagram of event flow and components to achieve scalable, stateful, and highly available deployment. The following sections describe each component in detail and how to configure them with WSO2 Streaming Integrator. Partitioning layer \u00b6 As shown in the above diagram, first you need to have a partitioning layer. Here, you are using an SI server to achieve it. The function of this layer is to consume events from output sources and then partition the events based on a partitioning condition. In order to partition you can leverage on the Distributed sink extension in WSO2 Streaming Integrator. The following is a sample Siddhi application syntax that defines a stream. It shows how the distributed sink can be applied to partition data. In this example, data is partitioned from tenant domain. For more information, see Siddhi Query Guide - Distributed Sink . Note In the following example, the definition of the Request stream(Request stream) only includes the logicto send events out for load balancers via http for each partition. In addition, there should be a logic to consume eventsfrom outside and direct them to the Request stream. e.g., // Stream defined with distributed sink with partitioned stratergy @Sink(type = 'http', @map(type='json'), @distribution(strategy='partitioned', partitionKey='userTenantDomain', @destination(publisher.url='http://Ip1:8009/Request'), @destination(publisher.url='http://Ip2:8009/Request'))) define stream Request (meta_clientType string, applicationConsumerKey string, applicationName string, applicationId string, applicationOwner string, apiContext string, apiName string, apiVersion string, apiCreator string,, apiTier string, apiHostname string, username string, userTenantDomain string,, requestTimestamp long, throttledOut bool, responseTime long, backendTime long); According to above distributed sink configuration, events that arrive at the Request stream are partitioned based on the userTenantDomain attribute. Therefore, if there are two tenant domain values fooDomain and barDomain , then events of fooDomain can be published to Ip1 , and the events of barDomain can be published to Ip2 . The distributed sink ensures that the unique partitioned events are not distributed across the cluster. Here, Ip1 and Ip2 represent the load balancer IPs. The reason for using load balancers is because the stateful layer also contains two SI servers to handle the high availability. Therefore, you need a load balancer to direct the traffic in a failover manner. According to the above diagram, there are four partitions. Therefore, four load balancers are used. You need the high availability in the partitioning layer. Therefore, you can use two WSO2 Streaming Integrator servers (minimum) as depicted in the diagram. Stateful Layer \u00b6 The function of this layer is to consume events according to each partition and carry out the rest of the stateful operations. When you have partitioned the date, you can seamlessly handle the scalability of the system as well as address the requirement for high availability of the system. Thus for each partition we can deploy the system as mentioned in two node minimum high available deployment section. Basically for each partition or partitions there will be a separate cluster of two SI nodes. So if active node fails for a particular partition the other node in the cluster will carry out the work. In order to configure the stateful layer you can follow the minimum high availability deployment guide. The only difference in configurations regarding this layer would be , as mentioned since we maintain separate clusters for each partition the Cluster Configuration group id should be different for each cluster. Sample cluster configuration can be as below : - cluster.config: enabled: true groupId: group 1 coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: WSO2_CLUSTER_DB heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000","title":"Scalable Cluster"},{"location":"setup/deploying-si-as-a-scalable-cluster/#scalable-deployment","text":"Scalable high availability deployment predominantly focuses on scaling the system according to the load or the TPS of the system. This is achieved with the help of horizontal scalability. WSO2 Streaming Integrator uses Siddhi as the streaming language. Siddhi allows you to write Siddhi logic in a stateless way as well as a stateful way. Stateless operations include filters, database operations etc., and stateful operations include window operations, aggregations etc., that keep data in memory to carry out calculations. The deployment options for a scalable streaming integrator depends on the statelessness and the statefulness of Siddhi applications. The following topics provide detailed descriptions of two approaches. System Requirements For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine .","title":"Scalable Deployment"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateless-scalable-high-availability-ha-deployment","text":"In stateless scenarios, the system does not work with any in-memory state. Thus in order to scale, you can keep adding Streaming Integrator servers to the system and front them with a load balancer that publishes the events in round robin manner. This is depicted in the diagram below.","title":"Stateless scalable high availability (HA) deployment"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateful-scalable-high-availability-ha-deployment","text":"As described before, stateful operations keep state data in memory. Therefore, in order to scale such a system, you need to process specific data on the same node without processing same-state data in different servers. You can achieve this via data partitioning where one bucket of partitioned data is processed only in one specific server. Info In order to scale stateful operations, it is required to have some partitioning attribute available enabling the partitioned data to be processed independently. The following is a high level diagram of event flow and components to achieve scalable, stateful, and highly available deployment. The following sections describe each component in detail and how to configure them with WSO2 Streaming Integrator.","title":"Stateful scalable high availability (HA) deployment"},{"location":"setup/deploying-si-as-a-scalable-cluster/#partitioning-layer","text":"As shown in the above diagram, first you need to have a partitioning layer. Here, you are using an SI server to achieve it. The function of this layer is to consume events from output sources and then partition the events based on a partitioning condition. In order to partition you can leverage on the Distributed sink extension in WSO2 Streaming Integrator. The following is a sample Siddhi application syntax that defines a stream. It shows how the distributed sink can be applied to partition data. In this example, data is partitioned from tenant domain. For more information, see Siddhi Query Guide - Distributed Sink . Note In the following example, the definition of the Request stream(Request stream) only includes the logicto send events out for load balancers via http for each partition. In addition, there should be a logic to consume eventsfrom outside and direct them to the Request stream. e.g., // Stream defined with distributed sink with partitioned stratergy @Sink(type = 'http', @map(type='json'), @distribution(strategy='partitioned', partitionKey='userTenantDomain', @destination(publisher.url='http://Ip1:8009/Request'), @destination(publisher.url='http://Ip2:8009/Request'))) define stream Request (meta_clientType string, applicationConsumerKey string, applicationName string, applicationId string, applicationOwner string, apiContext string, apiName string, apiVersion string, apiCreator string,, apiTier string, apiHostname string, username string, userTenantDomain string,, requestTimestamp long, throttledOut bool, responseTime long, backendTime long); According to above distributed sink configuration, events that arrive at the Request stream are partitioned based on the userTenantDomain attribute. Therefore, if there are two tenant domain values fooDomain and barDomain , then events of fooDomain can be published to Ip1 , and the events of barDomain can be published to Ip2 . The distributed sink ensures that the unique partitioned events are not distributed across the cluster. Here, Ip1 and Ip2 represent the load balancer IPs. The reason for using load balancers is because the stateful layer also contains two SI servers to handle the high availability. Therefore, you need a load balancer to direct the traffic in a failover manner. According to the above diagram, there are four partitions. Therefore, four load balancers are used. You need the high availability in the partitioning layer. Therefore, you can use two WSO2 Streaming Integrator servers (minimum) as depicted in the diagram.","title":"Partitioning layer"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateful-layer","text":"The function of this layer is to consume events according to each partition and carry out the rest of the stateful operations. When you have partitioned the date, you can seamlessly handle the scalability of the system as well as address the requirement for high availability of the system. Thus for each partition we can deploy the system as mentioned in two node minimum high available deployment section. Basically for each partition or partitions there will be a separate cluster of two SI nodes. So if active node fails for a particular partition the other node in the cluster will carry out the work. In order to configure the stateful layer you can follow the minimum high availability deployment guide. The only difference in configurations regarding this layer would be , as mentioned since we maintain separate clusters for each partition the Cluster Configuration group id should be different for each cluster. Sample cluster configuration can be as below : - cluster.config: enabled: true groupId: group 1 coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: WSO2_CLUSTER_DB heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000","title":"Stateful Layer"},{"location":"setup/deploying-si-as-a-single-deployment/","text":"Single Node Deployment \u00b6 You can deploy the WSO2 Streaming Integrator as a single node deployment to achieve most of the use cases that commonly arise in the streaming integration world. The other deployment options, namely Minimum High Availability (HA) Deployment and Scalable High Available(HA) Deployment are mainly introduced to handle high availability, scalability, and resiliency. However, single node deployment too allows you to achieve resilient deployment as explained in the Resilient Deployment subsection. System Requirements For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine . Resilient deployment \u00b6 Resiliency guarantees the ability to withstand or recover from any system failure and carry out the process without loosing any data. Streaming integrator has the capability to achieve the above via a broker that can replay data from a certain checkpoint. Kafka is one such broker that you can configure with WSO2 Streaming Integrator to achieve this. The only additional configuration that you need to do in WSO2 Streaming Integrator is state persistence. For detailed instructions, see Configuring Database and File System State Persistence . \u00b6 If the single Streaming Integrator node fails to receive incoming events and if you have configured state persistence, the single node is able to retrieve the latest snapshot from the database and request the broker to send the events that it was unable to process due to the failure The only disadvantage of this approach is that you need to ensure that there is a reliable mechanism to restart the server once it fails.","title":"Single Deployment"},{"location":"setup/deploying-si-as-a-single-deployment/#single-node-deployment","text":"You can deploy the WSO2 Streaming Integrator as a single node deployment to achieve most of the use cases that commonly arise in the streaming integration world. The other deployment options, namely Minimum High Availability (HA) Deployment and Scalable High Available(HA) Deployment are mainly introduced to handle high availability, scalability, and resiliency. However, single node deployment too allows you to achieve resilient deployment as explained in the Resilient Deployment subsection. System Requirements For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine .","title":"Single Node Deployment"},{"location":"setup/deploying-si-as-a-single-deployment/#resilient-deployment","text":"Resiliency guarantees the ability to withstand or recover from any system failure and carry out the process without loosing any data. Streaming integrator has the capability to achieve the above via a broker that can replay data from a certain checkpoint. Kafka is one such broker that you can configure with WSO2 Streaming Integrator to achieve this. The only additional configuration that you need to do in WSO2 Streaming Integrator is state persistence. For detailed instructions, see Configuring Database and File System State Persistence .","title":"Resilient deployment"},{"location":"setup/deploying-si-as-an-active-active-deployment/","text":"Active-Active Deployment \u00b6 The recommended deployment for WSO2 Streaming Integrator (SI) is the Minimum HA Deployment. However, that deployment pattern involves using only two nodes and it is not scalable beyond that. If you want to configure WSO2 SI as a scalable deployment, you can use the Active-Active deployment pattern. This section provides an overview of the Active-Active deployment pattern and instructions to configure it. Overview \u00b6 The above diagram represents a deployment where you are not limited to two nodes. You can scale the event processing horizontally by adding more SI nodes to the deployment. In this deployment, it is recommended to configure the client application to publish events to multiple SI nodes in a Round Robin manner to ensure better fault tolerance. The publishing of events can be carried out by one or more clients. In order to perform aggregations in a distributed manner and achieve the scalability, this setup uses distributed aggregations. Distributed aggregations partially process aggregations in different nodes. This allows you to assign one node to process only a part of an aggregation (regional scaling, etc.). In order to do this all the aggregations must have a physical database and must be linked to the same database. Partitioning aggregations can be enabled at aggregation level and also at a global level. To enable it at the global level, add the following section with the @PartitionById annotation set to true in the <SI_HOME>/conf/server/deployment.yaml file. siddhi: properties: partitionById: true shardId: wso2-si If you want to enable for a specific aggregation then the @PartitionById annotation must be added before the aggregation definition as shown in the example below. e.g., To understand how an active-active cluster processes aggregations when aggregations are partitioned and assigned to different nodes, consider the following Siddhi query. To learn more about Siddhi queries, see Siddhi Query Guide . define stream TradeStream (symbol string, price double, quantity long, ;timestamp long); @store(type='rdbms',jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PartitionById(enable='true') define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year This query captures the information relating to a trade. Each transaction represents an event, and the information captured includes the symbol of the product, the price at which it is sold, the quantity sold during the transaction, and the timestamp of the transaction. Each node stores this information in the TEST_DB data store defined in the <SI_WORKER_HOME>/conf/server/deployment.yaml file. Now let's assume that the following input events were generated for the two nodes during a specific hour. Node 1 Event symbol price quantity 1 wso2 100 10 2 wso2 100 20 Node 2 Event symbol price quantity 1 wso2 100 10 2 wso2 100 30 Here, node 1 calculates an hourly total of 30, and node 2 calculates an hourly total of 40. When you retrieve the total for this hour via a retrieval query, the result is 70. You can find the steps to enable aggregation partitioning within the next subsection. Configuring an active-active cluster \u00b6 To configure the Streaming Integrator nodes and deploy them as an active-active cluster, edit the <SI_HOME>/conf/server/deployment.yaml file as follows: Before you begin: Download two binary packs of the WSO2 Streaming Integrator. Set up a working RDBMS instance to be used by the WSO2 Streaming Integrator cluster. For each node, enter a unique ID for the id property under the wso2.carbon section. This is used to identify each node within a cluster. For example, you can add IDs as shown below. For node 1: wso2.carbon: id: wso2-si-1 For node 2: wso2.carbon: id: wso2-si-2 Enable partitioning aggregations for each node, and assign a unique shard ID for each node. To do this, set the partitionById and shardId parameters as Siddhi properties as shown below. Info Assigning shard IDs to nodes allows the system to identify each unique node when assigning parts of the aggregation. If the shard IDs are not assigned, system uses the unique node IDs (defined in step 1) for this purpose. For node 1: siddhi: properties: partitionById: true shardId: wso2-si-1 For node 2: siddhi: properties: partitionById: true shardId: wso2-si-2 Tip To maintain data consistency, do not change the shard IDs after the first configuration. When you enable the aggregation partitioning feature, a new column ID named SHARD_ID is introduced to the aggregation tables. Therefore, you need to do one of the following options after enabling this feature to avoid errors occuring due to the differences in the table schema. Delete all the aggregation tables for SECONDS , MINUTES , HOURS , DAYS , MONTHS , YEARS . Edit the aggregation tables by adding a new column named SHARD_ID , and specify it as a primary key. Configure a database, and then update the default configuration for the TEST_DB data source with parameter values suitable for your requirements. Warning As explained in above the events are processed in multiple active nodes. Eventhough this is usually a stateful operation, you can overcome the node-dependent calculations via distributed aggregation. This allows WSO2 SI to execute scripts that depend on incremental distributed aggregation. However, an active-active deployment can affect alerts because alerts also depend on some in-memory stateful operations such as windows. Due to this, alerts can be generated based on the events received by specific node. Thus the alerts are node-dependent, and you need to disable them to run scripts with distributed incremental aggregation.","title":"Active-Active Deployment"},{"location":"setup/deploying-si-as-an-active-active-deployment/#active-active-deployment","text":"The recommended deployment for WSO2 Streaming Integrator (SI) is the Minimum HA Deployment. However, that deployment pattern involves using only two nodes and it is not scalable beyond that. If you want to configure WSO2 SI as a scalable deployment, you can use the Active-Active deployment pattern. This section provides an overview of the Active-Active deployment pattern and instructions to configure it.","title":"Active-Active Deployment"},{"location":"setup/deploying-si-as-an-active-active-deployment/#overview","text":"The above diagram represents a deployment where you are not limited to two nodes. You can scale the event processing horizontally by adding more SI nodes to the deployment. In this deployment, it is recommended to configure the client application to publish events to multiple SI nodes in a Round Robin manner to ensure better fault tolerance. The publishing of events can be carried out by one or more clients. In order to perform aggregations in a distributed manner and achieve the scalability, this setup uses distributed aggregations. Distributed aggregations partially process aggregations in different nodes. This allows you to assign one node to process only a part of an aggregation (regional scaling, etc.). In order to do this all the aggregations must have a physical database and must be linked to the same database. Partitioning aggregations can be enabled at aggregation level and also at a global level. To enable it at the global level, add the following section with the @PartitionById annotation set to true in the <SI_HOME>/conf/server/deployment.yaml file. siddhi: properties: partitionById: true shardId: wso2-si If you want to enable for a specific aggregation then the @PartitionById annotation must be added before the aggregation definition as shown in the example below. e.g., To understand how an active-active cluster processes aggregations when aggregations are partitioned and assigned to different nodes, consider the following Siddhi query. To learn more about Siddhi queries, see Siddhi Query Guide . define stream TradeStream (symbol string, price double, quantity long, ;timestamp long); @store(type='rdbms',jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PartitionById(enable='true') define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year This query captures the information relating to a trade. Each transaction represents an event, and the information captured includes the symbol of the product, the price at which it is sold, the quantity sold during the transaction, and the timestamp of the transaction. Each node stores this information in the TEST_DB data store defined in the <SI_WORKER_HOME>/conf/server/deployment.yaml file. Now let's assume that the following input events were generated for the two nodes during a specific hour. Node 1 Event symbol price quantity 1 wso2 100 10 2 wso2 100 20 Node 2 Event symbol price quantity 1 wso2 100 10 2 wso2 100 30 Here, node 1 calculates an hourly total of 30, and node 2 calculates an hourly total of 40. When you retrieve the total for this hour via a retrieval query, the result is 70. You can find the steps to enable aggregation partitioning within the next subsection.","title":"Overview"},{"location":"setup/deploying-si-as-an-active-active-deployment/#configuring-an-active-active-cluster","text":"To configure the Streaming Integrator nodes and deploy them as an active-active cluster, edit the <SI_HOME>/conf/server/deployment.yaml file as follows: Before you begin: Download two binary packs of the WSO2 Streaming Integrator. Set up a working RDBMS instance to be used by the WSO2 Streaming Integrator cluster. For each node, enter a unique ID for the id property under the wso2.carbon section. This is used to identify each node within a cluster. For example, you can add IDs as shown below. For node 1: wso2.carbon: id: wso2-si-1 For node 2: wso2.carbon: id: wso2-si-2 Enable partitioning aggregations for each node, and assign a unique shard ID for each node. To do this, set the partitionById and shardId parameters as Siddhi properties as shown below. Info Assigning shard IDs to nodes allows the system to identify each unique node when assigning parts of the aggregation. If the shard IDs are not assigned, system uses the unique node IDs (defined in step 1) for this purpose. For node 1: siddhi: properties: partitionById: true shardId: wso2-si-1 For node 2: siddhi: properties: partitionById: true shardId: wso2-si-2 Tip To maintain data consistency, do not change the shard IDs after the first configuration. When you enable the aggregation partitioning feature, a new column ID named SHARD_ID is introduced to the aggregation tables. Therefore, you need to do one of the following options after enabling this feature to avoid errors occuring due to the differences in the table schema. Delete all the aggregation tables for SECONDS , MINUTES , HOURS , DAYS , MONTHS , YEARS . Edit the aggregation tables by adding a new column named SHARD_ID , and specify it as a primary key. Configure a database, and then update the default configuration for the TEST_DB data source with parameter values suitable for your requirements. Warning As explained in above the events are processed in multiple active nodes. Eventhough this is usually a stateful operation, you can overcome the node-dependent calculations via distributed aggregation. This allows WSO2 SI to execute scripts that depend on incremental distributed aggregation. However, an active-active deployment can affect alerts because alerts also depend on some in-memory stateful operations such as windows. Due to this, alerts can be generated based on the events received by specific node. Thus the alerts are node-dependent, and you need to disable them to run scripts with distributed incremental aggregation.","title":"Configuring an active-active cluster"},{"location":"setup/deploying-si-as-minimum-ha-cluster/","text":"Minimum High Availability (HA) Deployment \u00b6 The minimum high availability deployment mainly focuses on providing high availability that ensures the prevention of data loss if the system suffers a failure due to one or more unforeseeable reasons. One of the main adavantages of this deployment pattern is that it uses minimum amount of infrastructure resources possible. Thus deployment pattern is run with only two Streaming integration servers. In the minimum HA setup, one node is assigned as the active node while the other node is assigned as the passive node. Only the active node processes the incoming events and publishes the outgoing events. Internally, the active node publishes the events to the passive node, but the passive node does not process or send any events outside as mentioned earlier. In a scenario where the active node fails, the passive node is activated. Then the passive node starts receiving events and then publishing them from where the active node left off. Once the terminated (previously active) node restarts , it operates in the passive state. In the passive node, sources are in an inactive mode where they do not receive events into the system. Info In the passive node, databridge ports and Siddhi Store Query API endpoint are closed, but the admin API are accessible. For a two-node minimum HA cluster to work, only the active node should receive events. By design, you can only send events to the active node. To achieve this, you can use a load balancing mechanism that sends events in a failover manner as depicted in the diagram below. Before you begin: Before configuring a minimum HA cluster, you need to complete the following prerequisites: - For each WSO2 SI instance, you need a CPU with four cores, and a total memory of 4GB. For more information, see Installing the Streaming Integrator in a Virtual Machine . - Download and install two binary packs of WSO2 SI.. - Download, install and start a working RDBMS instance to be used for clustering the two nodes. - Download the MySQL connector from here . Extract and find the mysql-connector-java-5.*.*-bin.jar , and place it in the <SI_HOME>/lib directory of both nodes. - In order to retrieve the state of the Siddhi Applications deployed in the system (in case of a scenario where both the nodes fail), enable state persistence for both the nodes by specifying the same datasource/file location. For detailed instructions, see Configuring Database and File System State Persistence . - A client-side data publishing mechanism (such as a load balancer) that works in a failover manner must be available to publish events to one of the available nodes (i.e., to the active node). Configuring a minimum HA cluster \u00b6 There are three main configurations that are required to setup a minimum HA cluster. They are as follows: Cluster Configuration Persistent configuration HA configuration Note The configurations given below need to be done in the <SI_HOME>/conf/server/deployment.yaml file for both the WSO2 SI nodes in the cluster. If you need to run both SI instances in the same host, make sure that you do a port offset to change the default ports in one of the hosts. For more information about the default ports, see Configuring Default Ports . To configure the HA cluster, follow the steps below: For each node, enter a unique ID for the id property under the wso2.carbon section (e.g., id: wso2-si). This is used to identify each node within a cluster. To allow the two nodes to use same persistence storage, you need to configure RDBMS persistence configuration under state.persistence . The following is a configuration for db-based file persistence. Info This step covers persistent configuration. For this purpose, you can use MySQL, MSSQL, POSTGRES and Oracle database types. For more information about the supported database types, see Configuring Data Sources . - state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: PERSISTENCE_DB # A datasource with this name should be defined in wso2.datasources namespace table: PERSISTENCE_TABLE The datasource named PERSISTENCE_DB in the above configuration can be defined in the <SI_HOME>/conf/server/deployment.yaml file under wso2.datasources . The following is a sample datasource configuration. - name: PERSISTENCE_DB description: The MySQL datasource used for persistence jndiConfig: name: jdbc/PERSISTENCE_DB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/PERSISTENCE_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To allow the two nodes in the cluster to coordinate effectively, configure carbon coordination by updating the cluster.config section of the <SI_HOME>/conf/server/deployment.yaml as follows: Info This step covers cluster configuration. a. To enable the cluster mode, set the enabled property to true . `enabled: true` b. In order to cluster the two nodes together, enter the same ID as the group ID for both nodes (e.g., groupId: group-1 ). c. Enter the ID of the class that defines the coordination strategy for the cluster (e.g., coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy ). d. In the strategyConfig section, enter information as follows: datasource : Enter the name of the configured datasource shared by the nodes in the cluster as shown in the example below. Data handled by the cluster are persisted here. The following is a sample datasource configuration for a MySQL datasource that should appear under the dataSources subsection of the wso2.datasources section in the <SI_HOME>/conf/server/deployment.yaml file. Sample MySQL datasource - name: WSO2_CLUSTER_DB description: The MySQL datasource used for Cluster Coordination jndiConfig: name: jdbc/WSO2ClusterDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/WSO2_CLUSTER_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false For detailed instructions on how to configure a datasource, see Configuring Datasources . heartbeatInterval : Define the time interval (in milliseconds) at which the heartbeat pulse should occur for each node. Recommended value for it is 5000 milliseconds. heartbeatMaxRetry : Define the number of times to tghe system should retry to hear the heartbeat of the active node (that indicates that the node is alive) before the passive node becomes active. The recommended value is five times. eventPollingInterval : Define the time interval (in milliseconds) at which each node should listen for changes that occur in the cluster. Recommended value for it is 5000 milliseconds. The following is a sample cluster configuration. - cluster.config: enabled: true groupId: si coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: WSO2_CLUSTER_DB heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000 Next, add the deployment.config section to the <SI_HOME>/conf/server/deployment.yaml file with the following configurations. (HA configuration) Info This step covers HA configuration. type : To enable two-node minimum HA, set the type property to ha . passiveNodeDetailsWaitTimeOutMillis : The time duration (in miliseconds) to wait until the details of the passive node are available in the database so that the active node can retrieve them. Once this time duration elapses, a timeout occurs and the system sleeps for a time duration specified via the passiveNodeDetailsRetrySleepTimeMillis parameter. passiveNodeDetailsRetrySleepTimeMillis : The time duration (in milliseconds) to sleep before retying to retrieve details of the passive node again. This applies when the system has timed out after an attempt to retrieve these details. eventByteBufferQueueCapacity : The size of the queue that is used to keep events in the passive node. byteBufferExtractorThreadPoolSize : The number of worker threads that read events from the queue in the passive node. To configure the TCP server via which event synchronization is carried out from the active node to the passive node, add a subsection named eventSyncServer and enter information as follows: host : The hostname of the server in which the TCP server is spawned. port : The number of the port in which the TCP server is run. advertisedHost : This is specified when the host can be different from the actual server host. advertisedPort : This is specified when the port can be different from the actual port of the server. bossThreads : The number of boss threads to be allocated for the TCP server to handle the connections. The default value is 10 . workerThreads : The number of worker threads to be allocated for the TCP server to handle the connections. The default value is 10. . To configure the TCP client via which requests are sent to the SI cluster, add a subsection named eventSyncClientPool and add information as follows: maxActive : The maximum number of active connections that must be allowed in the TCP client pool. The default value is 10 . maxTotal : The maximum number of total connections that must be allowed in the TCP client pool. The default value is 10 . maxIdle : The maximum number of idle connections that must be allowed in the TCP client pool. The default value is 10 . maxWait : The maximum amount of time (in milliseconds) that the client pool must wait for an idle object in the connection pool. The default value is 6000 . minEvictableIdleTimeInMillis : The minimum number of milliseconds that an object can sit idle in the pool before it is eligible for eviction. The default value is 120000 . Info In a container environment, you can use an advertised host and an advertised port to avoid exposing the actual host and port. The following is sample HA configuration. ``` - deployment.config: type: ha passiveNodeDetailsWaitTimeOutMillis: 300000 passiveNodeDetailsRetrySleepTimeMillis: 500 eventByteBufferQueueCapacity: 20000 byteBufferExtractorThreadPoolSize: 5 eventSyncServer: host: localhost port: 9893 advertisedHost: localhost advertisedPort: 9893 bossThreads: 10 workerThreads: 10 eventSyncClientPool: maxActive: 10 maxTotal: 10 maxIdle: 10 maxWait: 60000 minEvictableIdleTimeMillis: 120000 ``` Starting the cluster \u00b6 To start the minimum HA cluster you configured, follow the steps below: Save the required Siddhi applications in the <SI_HOME>/wso2/server/deployment/siddhi-files directory of both nodes. In order to ensure that the Siddhi applications are completely synchronized between the active and the passive node, they must be added to the siddhi-files directory before the server startup. However, the synchronization can take place effectively even if the Siddhi applications are added while the server is running. Start both servers by navigating to the <SI_HOME>/bin directory and issuing one of the following commands (depending on your operating system: For Windows: server.bat For Linux/Mac OS : ./server.sh If the cluster is correctly configured, the following CLI logs can be viewed without any error logs: In the active node: [2018-09-09 23:56:54,272] INFO {org.wso2.carbon.stream.processor.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment [2018-09-09 23:56:54,294] INFO {org.wso2.carbon.stream.processor.core.ha.HAManager} - HA Deployment: Starting up as Active Node In the passive node: [2018-09-09 23:58:44,178] INFO {org.wso2.carbon.stream.processor.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment [2018-09-09 23:58:44,199] INFO {org.wso2.carbon.stream.processor.core.ha.HAManager} - HA Deployment: Starting up as Passive Node Info When deploying Siddhi applications in a two node minimum HA cluster, it is recommended to use a content synchronization mechanism since Siddhi applications must be deployed to both server nodes. You can use a common shared file system such as Network File System (NFS). You need to mount the <SI_HOME>/wso2/server/deployment/siddhi-files directory of the two nodes to the shared file system. Info To start two WSO2 SI Nodes in the same machine, <SI_HOME>/conf/server/deployment.yaml file -> wso2.transport.http namespace -> listenerConfigurations section must be updated to listen to different ports. The offset property in the <SI_HOME>/conf/server/deployment.yaml -> wso2.carbon section section -> ports subsection should also be changed in one SI instance to avoid conflicts when starting both servers.","title":"Minimum HA Cluster"},{"location":"setup/deploying-si-as-minimum-ha-cluster/#minimum-high-availability-ha-deployment","text":"The minimum high availability deployment mainly focuses on providing high availability that ensures the prevention of data loss if the system suffers a failure due to one or more unforeseeable reasons. One of the main adavantages of this deployment pattern is that it uses minimum amount of infrastructure resources possible. Thus deployment pattern is run with only two Streaming integration servers. In the minimum HA setup, one node is assigned as the active node while the other node is assigned as the passive node. Only the active node processes the incoming events and publishes the outgoing events. Internally, the active node publishes the events to the passive node, but the passive node does not process or send any events outside as mentioned earlier. In a scenario where the active node fails, the passive node is activated. Then the passive node starts receiving events and then publishing them from where the active node left off. Once the terminated (previously active) node restarts , it operates in the passive state. In the passive node, sources are in an inactive mode where they do not receive events into the system. Info In the passive node, databridge ports and Siddhi Store Query API endpoint are closed, but the admin API are accessible. For a two-node minimum HA cluster to work, only the active node should receive events. By design, you can only send events to the active node. To achieve this, you can use a load balancing mechanism that sends events in a failover manner as depicted in the diagram below. Before you begin: Before configuring a minimum HA cluster, you need to complete the following prerequisites: - For each WSO2 SI instance, you need a CPU with four cores, and a total memory of 4GB. For more information, see Installing the Streaming Integrator in a Virtual Machine . - Download and install two binary packs of WSO2 SI.. - Download, install and start a working RDBMS instance to be used for clustering the two nodes. - Download the MySQL connector from here . Extract and find the mysql-connector-java-5.*.*-bin.jar , and place it in the <SI_HOME>/lib directory of both nodes. - In order to retrieve the state of the Siddhi Applications deployed in the system (in case of a scenario where both the nodes fail), enable state persistence for both the nodes by specifying the same datasource/file location. For detailed instructions, see Configuring Database and File System State Persistence . - A client-side data publishing mechanism (such as a load balancer) that works in a failover manner must be available to publish events to one of the available nodes (i.e., to the active node).","title":"Minimum High Availability (HA) Deployment"},{"location":"setup/deploying-si-as-minimum-ha-cluster/#configuring-a-minimum-ha-cluster","text":"There are three main configurations that are required to setup a minimum HA cluster. They are as follows: Cluster Configuration Persistent configuration HA configuration Note The configurations given below need to be done in the <SI_HOME>/conf/server/deployment.yaml file for both the WSO2 SI nodes in the cluster. If you need to run both SI instances in the same host, make sure that you do a port offset to change the default ports in one of the hosts. For more information about the default ports, see Configuring Default Ports . To configure the HA cluster, follow the steps below: For each node, enter a unique ID for the id property under the wso2.carbon section (e.g., id: wso2-si). This is used to identify each node within a cluster. To allow the two nodes to use same persistence storage, you need to configure RDBMS persistence configuration under state.persistence . The following is a configuration for db-based file persistence. Info This step covers persistent configuration. For this purpose, you can use MySQL, MSSQL, POSTGRES and Oracle database types. For more information about the supported database types, see Configuring Data Sources . - state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: PERSISTENCE_DB # A datasource with this name should be defined in wso2.datasources namespace table: PERSISTENCE_TABLE The datasource named PERSISTENCE_DB in the above configuration can be defined in the <SI_HOME>/conf/server/deployment.yaml file under wso2.datasources . The following is a sample datasource configuration. - name: PERSISTENCE_DB description: The MySQL datasource used for persistence jndiConfig: name: jdbc/PERSISTENCE_DB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/PERSISTENCE_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false To allow the two nodes in the cluster to coordinate effectively, configure carbon coordination by updating the cluster.config section of the <SI_HOME>/conf/server/deployment.yaml as follows: Info This step covers cluster configuration. a. To enable the cluster mode, set the enabled property to true . `enabled: true` b. In order to cluster the two nodes together, enter the same ID as the group ID for both nodes (e.g., groupId: group-1 ). c. Enter the ID of the class that defines the coordination strategy for the cluster (e.g., coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy ). d. In the strategyConfig section, enter information as follows: datasource : Enter the name of the configured datasource shared by the nodes in the cluster as shown in the example below. Data handled by the cluster are persisted here. The following is a sample datasource configuration for a MySQL datasource that should appear under the dataSources subsection of the wso2.datasources section in the <SI_HOME>/conf/server/deployment.yaml file. Sample MySQL datasource - name: WSO2_CLUSTER_DB description: The MySQL datasource used for Cluster Coordination jndiConfig: name: jdbc/WSO2ClusterDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost:3306/WSO2_CLUSTER_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false For detailed instructions on how to configure a datasource, see Configuring Datasources . heartbeatInterval : Define the time interval (in milliseconds) at which the heartbeat pulse should occur for each node. Recommended value for it is 5000 milliseconds. heartbeatMaxRetry : Define the number of times to tghe system should retry to hear the heartbeat of the active node (that indicates that the node is alive) before the passive node becomes active. The recommended value is five times. eventPollingInterval : Define the time interval (in milliseconds) at which each node should listen for changes that occur in the cluster. Recommended value for it is 5000 milliseconds. The following is a sample cluster configuration. - cluster.config: enabled: true groupId: si coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: WSO2_CLUSTER_DB heartbeatInterval: 5000 heartbeatMaxRetry: 5 eventPollingInterval: 5000 Next, add the deployment.config section to the <SI_HOME>/conf/server/deployment.yaml file with the following configurations. (HA configuration) Info This step covers HA configuration. type : To enable two-node minimum HA, set the type property to ha . passiveNodeDetailsWaitTimeOutMillis : The time duration (in miliseconds) to wait until the details of the passive node are available in the database so that the active node can retrieve them. Once this time duration elapses, a timeout occurs and the system sleeps for a time duration specified via the passiveNodeDetailsRetrySleepTimeMillis parameter. passiveNodeDetailsRetrySleepTimeMillis : The time duration (in milliseconds) to sleep before retying to retrieve details of the passive node again. This applies when the system has timed out after an attempt to retrieve these details. eventByteBufferQueueCapacity : The size of the queue that is used to keep events in the passive node. byteBufferExtractorThreadPoolSize : The number of worker threads that read events from the queue in the passive node. To configure the TCP server via which event synchronization is carried out from the active node to the passive node, add a subsection named eventSyncServer and enter information as follows: host : The hostname of the server in which the TCP server is spawned. port : The number of the port in which the TCP server is run. advertisedHost : This is specified when the host can be different from the actual server host. advertisedPort : This is specified when the port can be different from the actual port of the server. bossThreads : The number of boss threads to be allocated for the TCP server to handle the connections. The default value is 10 . workerThreads : The number of worker threads to be allocated for the TCP server to handle the connections. The default value is 10. . To configure the TCP client via which requests are sent to the SI cluster, add a subsection named eventSyncClientPool and add information as follows: maxActive : The maximum number of active connections that must be allowed in the TCP client pool. The default value is 10 . maxTotal : The maximum number of total connections that must be allowed in the TCP client pool. The default value is 10 . maxIdle : The maximum number of idle connections that must be allowed in the TCP client pool. The default value is 10 . maxWait : The maximum amount of time (in milliseconds) that the client pool must wait for an idle object in the connection pool. The default value is 6000 . minEvictableIdleTimeInMillis : The minimum number of milliseconds that an object can sit idle in the pool before it is eligible for eviction. The default value is 120000 . Info In a container environment, you can use an advertised host and an advertised port to avoid exposing the actual host and port. The following is sample HA configuration. ``` - deployment.config: type: ha passiveNodeDetailsWaitTimeOutMillis: 300000 passiveNodeDetailsRetrySleepTimeMillis: 500 eventByteBufferQueueCapacity: 20000 byteBufferExtractorThreadPoolSize: 5 eventSyncServer: host: localhost port: 9893 advertisedHost: localhost advertisedPort: 9893 bossThreads: 10 workerThreads: 10 eventSyncClientPool: maxActive: 10 maxTotal: 10 maxIdle: 10 maxWait: 60000 minEvictableIdleTimeMillis: 120000 ```","title":"Configuring a minimum HA cluster"},{"location":"setup/deploying-si-as-minimum-ha-cluster/#starting-the-cluster","text":"To start the minimum HA cluster you configured, follow the steps below: Save the required Siddhi applications in the <SI_HOME>/wso2/server/deployment/siddhi-files directory of both nodes. In order to ensure that the Siddhi applications are completely synchronized between the active and the passive node, they must be added to the siddhi-files directory before the server startup. However, the synchronization can take place effectively even if the Siddhi applications are added while the server is running. Start both servers by navigating to the <SI_HOME>/bin directory and issuing one of the following commands (depending on your operating system: For Windows: server.bat For Linux/Mac OS : ./server.sh If the cluster is correctly configured, the following CLI logs can be viewed without any error logs: In the active node: [2018-09-09 23:56:54,272] INFO {org.wso2.carbon.stream.processor.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment [2018-09-09 23:56:54,294] INFO {org.wso2.carbon.stream.processor.core.ha.HAManager} - HA Deployment: Starting up as Active Node In the passive node: [2018-09-09 23:58:44,178] INFO {org.wso2.carbon.stream.processor.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment [2018-09-09 23:58:44,199] INFO {org.wso2.carbon.stream.processor.core.ha.HAManager} - HA Deployment: Starting up as Passive Node Info When deploying Siddhi applications in a two node minimum HA cluster, it is recommended to use a content synchronization mechanism since Siddhi applications must be deployed to both server nodes. You can use a common shared file system such as Network File System (NFS). You need to mount the <SI_HOME>/wso2/server/deployment/siddhi-files directory of the two nodes to the shared file system. Info To start two WSO2 SI Nodes in the same machine, <SI_HOME>/conf/server/deployment.yaml file -> wso2.transport.http namespace -> listenerConfigurations section must be updated to listen to different ports. The offset property in the <SI_HOME>/conf/server/deployment.yaml -> wso2.carbon section section -> ports subsection should also be changed in one SI instance to avoid conflicts when starting both servers.","title":"Starting the cluster"},{"location":"setup/deployment-guide/","text":"Deployment \u00b6 WSO2 Streaming Integrator(SI) is designed to deploy enterprise grade critical systems which demands high reliability, availability and performance. Thus, WSO2 SI is designed with support for deployment options which can handle increasing system loads (scalability) and provide near-zero downtime and by eliminate single point of failure (high availability). Furthermore, some of the key characteristics of WSO2 SI deployment architecture are ease of installation and deployment, high maintainability and DevOps friendliness. Mainly following three deployment options can be followed, Single Node Deployment Minimum High Available(HA) Deployment Scalable High Available(HA) Deployment","title":"Deployment"},{"location":"setup/deployment-guide/#deployment","text":"WSO2 Streaming Integrator(SI) is designed to deploy enterprise grade critical systems which demands high reliability, availability and performance. Thus, WSO2 SI is designed with support for deployment options which can handle increasing system loads (scalability) and provide near-zero downtime and by eliminate single point of failure (high availability). Furthermore, some of the key characteristics of WSO2 SI deployment architecture are ease of installation and deployment, high maintainability and DevOps friendliness. Mainly following three deployment options can be followed, Single Node Deployment Minimum High Available(HA) Deployment Scalable High Available(HA) Deployment","title":"Deployment"},{"location":"setup/installing-Stream-Processor-4.3.0/","text":"Installing Stream Processor 4.3.0 \u00b6 The following topics provide instructions on how to install WSO2 Stream Processor 4.3.0 using various platforms. Installing Stream Processor 4.3.0 Using Ansible Installing Stream Processor Using Docker Installing Stream Processor 4.3.0 Using Puppet Installing Stream Processor 4.3.0 Using Vagrant Installing Stream Processor Using Helm Installing Stream Processor Using AWS Cloud Formation Installing Stream Processor Using Docker Compose Installing Stream Processor Using Kubernetes Installing Stream Processor Using Apt Installing Stream Processor Using Yum Installing Stream Processor Using Brew","title":"Installing Stream Processor 4.3.0"},{"location":"setup/installing-Stream-Processor-4.3.0/#installing-stream-processor-430","text":"The following topics provide instructions on how to install WSO2 Stream Processor 4.3.0 using various platforms. Installing Stream Processor 4.3.0 Using Ansible Installing Stream Processor Using Docker Installing Stream Processor 4.3.0 Using Puppet Installing Stream Processor 4.3.0 Using Vagrant Installing Stream Processor Using Helm Installing Stream Processor Using AWS Cloud Formation Installing Stream Processor Using Docker Compose Installing Stream Processor Using Kubernetes Installing Stream Processor Using Apt Installing Stream Processor Using Yum Installing Stream Processor Using Brew","title":"Installing Stream Processor 4.3.0"},{"location":"setup/installing-Stream-Processor-Using-Docker/","text":"Installing Stream Processor Using Docker \u00b6 Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here . Downloading and installing WSO2 Stream Processor \u00b6 Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard Running WSO2 Stream Processor \u00b6 To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Installing Stream Processor Using Docker"},{"location":"setup/installing-Stream-Processor-Using-Docker/#installing-stream-processor-using-docker","text":"Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here .","title":"Installing Stream Processor Using Docker"},{"location":"setup/installing-Stream-Processor-Using-Docker/#downloading-and-installing-wso2-stream-processor","text":"Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard","title":"Downloading and installing WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor-Using-Docker/#running-wso2-stream-processor","text":"To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Running WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/","text":"Installing Stream Processor Using Kubernetes \u00b6 To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon Fully distributed deployment of WSO2 Stream Processor \u00b6 To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as <KUBERNETES_HOME> from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the <KUBERNETES_HOME>/pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 <directory_name> Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 <directory_name> Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in <KUBERNETES_HOME>/pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the <KUBERNETES_HOME>/pattern-distributed/scripts directory as follows. cd <KUBERNETES_HOME>/pattern-distributed/scripts Deploy the Kubernetes resources by executing the <KUBERNETES_HOME>/pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username=<WSO2_USERNAME> --wso2-password=<WSO2_PASSWORD> --cluster-admin-password=<K8S_CLUSTER_ADMIN_PASSWORD>WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\<EXTERNAL-IP> wso2sp-dashboard <EXTERNAL-IP> wso2sp-manager-1 <EXTERNAL-IP> wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Installing Stream Processor Using Kubernetes"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/#installing-stream-processor-using-kubernetes","text":"To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon","title":"Installing Stream Processor Using Kubernetes"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/#fully-distributed-deployment-of-wso2-stream-processor","text":"To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as <KUBERNETES_HOME> from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the <KUBERNETES_HOME>/pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 <directory_name> Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 <directory_name> Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in <KUBERNETES_HOME>/pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the <KUBERNETES_HOME>/pattern-distributed/scripts directory as follows. cd <KUBERNETES_HOME>/pattern-distributed/scripts Deploy the Kubernetes resources by executing the <KUBERNETES_HOME>/pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username=<WSO2_USERNAME> --wso2-password=<WSO2_PASSWORD> --cluster-admin-password=<K8S_CLUSTER_ADMIN_PASSWORD>WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\<EXTERNAL-IP> wso2sp-dashboard <EXTERNAL-IP> wso2sp-manager-1 <EXTERNAL-IP> wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Fully distributed deployment of WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor/","text":"Installing Stream Processor \u00b6 The following are options that you can use to install WSO2 Stream Processor. Installing Stream Processor 4.3.0","title":"Installing Stream Processor"},{"location":"setup/installing-Stream-Processor/#installing-stream-processor","text":"The following are options that you can use to install WSO2 Stream Processor. Installing Stream Processor 4.3.0","title":"Installing Stream Processor"},{"location":"setup/installing-si-in-vm/","text":"Installing the Streaming Integrator in a Virtual Machine \u00b6 Follow the steps given below to install and run WSO2 Streaming Integrator on a VM. System requirements \u00b6 Type Requirement CPU You require a minimum of one CPU with 2 cores. It is recommended to have a CPU with 4 cores. Memory ~ 4 GB minimum is recommended ~ 2 GB heap size Disk ~ 1 GB minimum (excluding space allocated for log files and databases.) Installing the Streaming Integrator \u00b6 Follow the steps below: Go to the Streaming Integrator product page and click Download to get the product installer . The installer that is compatible with your operating system is downloaded. Info Alternatively, go to Other Installation Options and click Binary to download the product distribution as a ZIP file. If you used the installer, double-click to open the installation wizard that guides you through the installation. When you finish, the product is installed and ready for use. Accessing the HOME directory \u00b6 Let's call the installation location of your product the <SI_HOME> directory. If you used the installer to install the product, this is located in a place specific to your OS as shown below: OS Home directory Mac OS /Library/WSO2/EnterpriseIntegrator/7.0.2/streaming-integrator Windows C:\\Program Files\\WSO2\\Enterprise Integrator\\7.0.2\\streaming-integrator Ubuntu /usr/lib/wso2/wso2ei/7.0.2/streaming-integrator CentOS /usr/lib64/wso2/wso2ei/7.0.2/streaming-integrator Uninstalling the product \u00b6 If you used the installer to install the product, you can uninstall by following the steps given below: OS Instructions Mac OS Open a terminal and run the following command as the root user: sudo bash /Library/WSO2/EnterpriseIntegrator/7.0.2 Windows Go to Start Menu -> Programs -> WSO2 -> Uninstall Enterprise Integrator 7.0.2 or search Uninstall Enterprise Integrator 7.0.2 and click the shortcut icon. This uninstalls the product from your computer. Ubuntu Open a terminal and run the following command: sudo apt purge wso2ei-7.0.2 CentOS Open a terminal and run the following command: sudo yum remove wso2ei-7.0.2 Running the Streaming Integrator \u00b6 Start the WSO2 Streaming Integrator by following the instructions given below. Using the installer \u00b6 On MacOS/Linux/CentOS , open a terminal and execute the command given below. sudo wso2si The operation log keeps running until the profile starts, which usually takes several seconds. Wait until the profile fully boots up and displays a message similar to \" WSO2 Carbon started in n seconds. \" On Windows , go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator . This opens a terminal and start the relevant profile. If you have installed the product using the installer and you want to manually run the product startup script from the <SI_HOME/bin> directory, you need to issue the following command: sudo sh launcher_streaming-integrator.sh This script automatically assigns the JAVA_HOME of your VM to the root user of your Streaming Integrator instance. Using the binary distribution \u00b6 Before you execute the product startup script, be sure to set the JAVA HOME in your machine. Use a JDK that is compatible with WSO2 Enterprise Integrator . Open a terminal and navigate to the <SI_HOME>/bin/ directory, where <SI_HOME> is the home directory of the distribution you downloaded. Execute the relevant command. On MacOS/Linux/CentOS sh server.sh On Windows server.bat By default, the HTTP listener port is 8290 and the default HTTPS listener port is 8253. Stopping the Streaming Integrator \u00b6 To stop the Streaming Integrator runtime, press Ctrl+C in the command window.","title":"Install in a VM"},{"location":"setup/installing-si-in-vm/#installing-the-streaming-integrator-in-a-virtual-machine","text":"Follow the steps given below to install and run WSO2 Streaming Integrator on a VM.","title":"Installing the Streaming Integrator in a Virtual Machine"},{"location":"setup/installing-si-in-vm/#system-requirements","text":"Type Requirement CPU You require a minimum of one CPU with 2 cores. It is recommended to have a CPU with 4 cores. Memory ~ 4 GB minimum is recommended ~ 2 GB heap size Disk ~ 1 GB minimum (excluding space allocated for log files and databases.)","title":"System requirements"},{"location":"setup/installing-si-in-vm/#installing-the-streaming-integrator","text":"Follow the steps below: Go to the Streaming Integrator product page and click Download to get the product installer . The installer that is compatible with your operating system is downloaded. Info Alternatively, go to Other Installation Options and click Binary to download the product distribution as a ZIP file. If you used the installer, double-click to open the installation wizard that guides you through the installation. When you finish, the product is installed and ready for use.","title":"Installing the Streaming Integrator"},{"location":"setup/installing-si-in-vm/#accessing-the-home-directory","text":"Let's call the installation location of your product the <SI_HOME> directory. If you used the installer to install the product, this is located in a place specific to your OS as shown below: OS Home directory Mac OS /Library/WSO2/EnterpriseIntegrator/7.0.2/streaming-integrator Windows C:\\Program Files\\WSO2\\Enterprise Integrator\\7.0.2\\streaming-integrator Ubuntu /usr/lib/wso2/wso2ei/7.0.2/streaming-integrator CentOS /usr/lib64/wso2/wso2ei/7.0.2/streaming-integrator","title":"Accessing the HOME directory"},{"location":"setup/installing-si-in-vm/#uninstalling-the-product","text":"If you used the installer to install the product, you can uninstall by following the steps given below: OS Instructions Mac OS Open a terminal and run the following command as the root user: sudo bash /Library/WSO2/EnterpriseIntegrator/7.0.2 Windows Go to Start Menu -> Programs -> WSO2 -> Uninstall Enterprise Integrator 7.0.2 or search Uninstall Enterprise Integrator 7.0.2 and click the shortcut icon. This uninstalls the product from your computer. Ubuntu Open a terminal and run the following command: sudo apt purge wso2ei-7.0.2 CentOS Open a terminal and run the following command: sudo yum remove wso2ei-7.0.2","title":"Uninstalling the product"},{"location":"setup/installing-si-in-vm/#running-the-streaming-integrator","text":"Start the WSO2 Streaming Integrator by following the instructions given below.","title":"Running the Streaming Integrator"},{"location":"setup/installing-si-in-vm/#using-the-installer","text":"On MacOS/Linux/CentOS , open a terminal and execute the command given below. sudo wso2si The operation log keeps running until the profile starts, which usually takes several seconds. Wait until the profile fully boots up and displays a message similar to \" WSO2 Carbon started in n seconds. \" On Windows , go to Start Menu -> Programs -> WSO2 -> Enterprise Integrator . This opens a terminal and start the relevant profile. If you have installed the product using the installer and you want to manually run the product startup script from the <SI_HOME/bin> directory, you need to issue the following command: sudo sh launcher_streaming-integrator.sh This script automatically assigns the JAVA_HOME of your VM to the root user of your Streaming Integrator instance.","title":"Using the installer"},{"location":"setup/installing-si-in-vm/#using-the-binary-distribution","text":"Before you execute the product startup script, be sure to set the JAVA HOME in your machine. Use a JDK that is compatible with WSO2 Enterprise Integrator . Open a terminal and navigate to the <SI_HOME>/bin/ directory, where <SI_HOME> is the home directory of the distribution you downloaded. Execute the relevant command. On MacOS/Linux/CentOS sh server.sh On Windows server.bat By default, the HTTP listener port is 8290 and the default HTTPS listener port is 8253.","title":"Using the binary distribution"},{"location":"setup/installing-si-in-vm/#stopping-the-streaming-integrator","text":"To stop the Streaming Integrator runtime, press Ctrl+C in the command window.","title":"Stopping the Streaming Integrator"},{"location":"setup/installing-si-using-docker/","text":"Installing Streaming Integrator Using Docker \u00b6 Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here . WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 . Downloading and installing WSO2 Streaming Integrator \u00b6 To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker pull wso2/streaming-integrator Running WSO2 Streaming Integrator \u00b6 To run WSO2 Streaming Integrator, issue the following command. docker run -it wso2/streaming-integrator Tip To expose the required ports via docker when running the docker container, issue the following command. docker run -it \\ -p 9443:9443 \\ -p 9090:9090 \\ -p 7070:7070 \\ -p 7443:7443 \\ -p 9712:9712 \\ -p 7711:7711 \\ -p 7611:7611 \\ wso2/streaming-integrator For more details about the ports in Streaming Integrator, see Configuring Default Ports","title":"Installing in Docker"},{"location":"setup/installing-si-using-docker/#installing-streaming-integrator-using-docker","text":"Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided in here . WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2 .","title":"Installing Streaming Integrator Using Docker"},{"location":"setup/installing-si-using-docker/#downloading-and-installing-wso2-streaming-integrator","text":"To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command. docker pull wso2/streaming-integrator","title":"Downloading and installing WSO2 Streaming Integrator"},{"location":"setup/installing-si-using-docker/#running-wso2-streaming-integrator","text":"To run WSO2 Streaming Integrator, issue the following command. docker run -it wso2/streaming-integrator Tip To expose the required ports via docker when running the docker container, issue the following command. docker run -it \\ -p 9443:9443 \\ -p 9090:9090 \\ -p 7070:7070 \\ -p 7443:7443 \\ -p 9712:9712 \\ -p 7711:7711 \\ -p 7611:7611 \\ wso2/streaming-integrator For more details about the ports in Streaming Integrator, see Configuring Default Ports","title":"Running WSO2 Streaming Integrator"},{"location":"setup/installing-si-using-kubernetes/","text":"Installing Streaming Integrator Using Kubernetes \u00b6 WSO2 Streaming Integrator can be deployed natively on kubernetes using Siddhi Kubernetes Operator. Streaming Integrator can be configured in SiddhiProcess yaml and passed to the CRD(Custom Resource Definition) for deployment. Siddhi logic can be directly written inline in SiddhiProcess yaml or passed as .siddhi files via config maps. To install WSO2 Streaming Integrator using Kubernetes, follow the steps below: Prerequisites \u00b6 A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster. Admin privilages to install Siddhi operator. Minikube \u00b6 Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation . Google Kubernetes Engine Cluster \u00b6 To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com Docker For Mac \u00b6 Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation . Install Siddhi Operator for Streaming Integrator \u00b6 To install the Siddhi Kubernetes operator for streaming integrator run the following commands. kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE streaming-integrator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m Deploy Streaming Integrator \u00b6 Siddhi app which consists the streaming integration logic can be deployed on kubernetes using the siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess yaml file as given below. To change the default configurations in streaming integrator you can provide the configuration that needed to overide through SiddhiProcess yaml Here we have updated the above SiddhiProcess yaml to have the modified configuration for auth.configs in order to change the default configurations in streaming integrator. Info we can overide the default deployment.yaml file by using this appoach Invoke Siddhi Applications \u00b6 To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows and then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 14d Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi app, list down the available pods by executing the following command. $ kubectl get pods NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m and then execute the following command in order to monitor the logs of the relevant pod. kubectl logs -f streaming-integrator-app-0-b4dcf85-npgj7 Info For more details regarding the Siddhi Operator, Please refer to the following document .","title":"Installing in Kubernetes"},{"location":"setup/installing-si-using-kubernetes/#installing-streaming-integrator-using-kubernetes","text":"WSO2 Streaming Integrator can be deployed natively on kubernetes using Siddhi Kubernetes Operator. Streaming Integrator can be configured in SiddhiProcess yaml and passed to the CRD(Custom Resource Definition) for deployment. Siddhi logic can be directly written inline in SiddhiProcess yaml or passed as .siddhi files via config maps. To install WSO2 Streaming Integrator using Kubernetes, follow the steps below:","title":"Installing Streaming Integrator Using Kubernetes"},{"location":"setup/installing-si-using-kubernetes/#prerequisites","text":"A Kubernetes cluster v1.10.11 or higher. Minikube Google Kubernetes Engine(GKE) Cluster Docker for Mac Or any other Kubernetes cluster. Admin privilages to install Siddhi operator.","title":"Prerequisites"},{"location":"setup/installing-si-using-kubernetes/#minikube","text":"Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Minikube using the following command. minikube addons enable ingress or disable Siddhi operator's automatically ingress creation .","title":"Minikube"},{"location":"setup/installing-si-using-kubernetes/#google-kubernetes-engine-cluster","text":"To install Siddhi operator, you have to give cluster admin permission to your account. In order to do that execute the following command (by replacing \"your-address@email.com\" with your account email address). kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com","title":"Google Kubernetes Engine Cluster"},{"location":"setup/installing-si-using-kubernetes/#docker-for-mac","text":"Siddhi operator automatically creates NGINX ingress. Therefore it to work we can either enable ingress on Docker for mac following the official documentation or disable Siddhi operator's automatically ingress creation .","title":"Docker For Mac"},{"location":"setup/installing-si-using-kubernetes/#install-siddhi-operator-for-streaming-integrator","text":"To install the Siddhi Kubernetes operator for streaming integrator run the following commands. kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE streaming-integrator 1 1 1 1 1m siddhi-parser 1 1 1 1 1m","title":"Install Siddhi Operator for Streaming Integrator"},{"location":"setup/installing-si-using-kubernetes/#deploy-streaming-integrator","text":"Siddhi app which consists the streaming integration logic can be deployed on kubernetes using the siddhi operator. Here we will creating a very simple Siddhi stream processing application that consumes events via HTTP, filers the input events on the type 'monitored' and logs the output on the console. This can be created using a SiddhiProcess yaml file as given below. To change the default configurations in streaming integrator you can provide the configuration that needed to overide through SiddhiProcess yaml Here we have updated the above SiddhiProcess yaml to have the modified configuration for auth.configs in order to change the default configurations in streaming integrator. Info we can overide the default deployment.yaml file by using this appoach","title":"Deploy Streaming Integrator"},{"location":"setup/installing-si-using-kubernetes/#invoke-siddhi-applications","text":"To invoke the Siddhi Apps, first obtain the external IP of the ingress load balancer using kubectl get ingress command as follows and then, add the host siddhi and related external IP (ADDRESS) to the /etc/hosts file in your machine. $ kubectl get ingress NAME HOSTS ADDRESS PORTS AGE siddhi siddhi 10.0.2.15 80 14d Minikube For Minikube, you have to use Minikube IP as the external IP. Hence, run minikube ip command to get the IP of the Minikube cluster. Use the following CURL command to send events to monitor-app deployed in Kubernetes. curl -X POST \\ http://siddhi/streaming-integrator-0/8080/checkPower \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ -H 'Host: siddhi' \\ -d '{ \"deviceType\": \"dryer\", \"power\": 600 }' To monitor the associated logs for the above siddhi app, list down the available pods by executing the following command. $ kubectl get pods NAME READY STATUS RESTARTS AGE streaming-integrator-app-0-b4dcf85-npgj7 1/1 Running 0 165m streaming-integrator-5f9fcb7679-n4zpj 1/1 Running 0 173m and then execute the following command in order to monitor the logs of the relevant pod. kubectl logs -f streaming-integrator-app-0-b4dcf85-npgj7 Info For more details regarding the Siddhi Operator, Please refer to the following document .","title":"Invoke Siddhi Applications"},{"location":"setup/migrating-from-stream-processor/","text":"Migrating from WSO2 Stream Processor \u00b6 The Streaming Integrator performs all functions that are also performed by WSO2 Stream Processor . It also has additional features to trigger integration flows in order to take action in response to results derived after analyzing data. If you are currently using WSO2 Stream Processor to carry out any streaming integration/stream processing activities and want to carry them out in the Streaming Integrator, you can migrate your setup as follows: Preparing to upgrade \u00b6 The following prerequisites should be completed before upgrading. Make a backup of the SP 4.4.0 database and copy the directory in order to backup the product configurations. Download the Streaming Integrator from the Enterprise Integrator Home Migrating Databases \u00b6 To connect the Streaming Integrator to the same databases as WSO2 SP 4.4.0 so that the persisted data can be accessed, configure the data sources as follows: Configure the data sources in the <SI_HOME>/conf/server/deployment.yaml file the same way you have configured them in <SP_HOME>/conf/wso2/worker/deployment.yaml file. Configure the data sources in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file the same way you have configured them in <SP_HOME>/conf/wso2/editor/deployment.yaml file. Check the data source configured for Business Rules in the <SP_HOME>/conf/wso2/dashboard/deployment.yaml file, and configure that data source with the same parameter values in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file. Info The Business Rules feature which was a part of the Dashboard profile of the Stream Processor is now shipped with Streaming Integrator Tooling. Therefore, configurations related to this feature are added in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file. For the complete list of data sources configured for the Streaming Integrator, see Configuring Data sources . Migrating Siddhi applications \u00b6 To migrate the Siddhi applications that you have deployed in WSO2 SP 4.4.0, follow the procedure below: Copy all the Siddhi applications in the <SP_HOME/wso2/worker/deployment/siddhi-files directory. Place the Siddhi applications you copied in the <SI_HOME/wso2/server/deployment/siddhi-files directory. Testing the migration \u00b6 Simulate a few events to the Siddhi applications deployed in the Streaming Integrator to test whether they are generating the expected results.","title":"Migrating from WSO2 SP"},{"location":"setup/migrating-from-stream-processor/#migrating-from-wso2-stream-processor","text":"The Streaming Integrator performs all functions that are also performed by WSO2 Stream Processor . It also has additional features to trigger integration flows in order to take action in response to results derived after analyzing data. If you are currently using WSO2 Stream Processor to carry out any streaming integration/stream processing activities and want to carry them out in the Streaming Integrator, you can migrate your setup as follows:","title":"Migrating from WSO2 Stream Processor"},{"location":"setup/migrating-from-stream-processor/#preparing-to-upgrade","text":"The following prerequisites should be completed before upgrading. Make a backup of the SP 4.4.0 database and copy the directory in order to backup the product configurations. Download the Streaming Integrator from the Enterprise Integrator Home","title":"Preparing to upgrade"},{"location":"setup/migrating-from-stream-processor/#migrating-databases","text":"To connect the Streaming Integrator to the same databases as WSO2 SP 4.4.0 so that the persisted data can be accessed, configure the data sources as follows: Configure the data sources in the <SI_HOME>/conf/server/deployment.yaml file the same way you have configured them in <SP_HOME>/conf/wso2/worker/deployment.yaml file. Configure the data sources in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file the same way you have configured them in <SP_HOME>/conf/wso2/editor/deployment.yaml file. Check the data source configured for Business Rules in the <SP_HOME>/conf/wso2/dashboard/deployment.yaml file, and configure that data source with the same parameter values in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file. Info The Business Rules feature which was a part of the Dashboard profile of the Stream Processor is now shipped with Streaming Integrator Tooling. Therefore, configurations related to this feature are added in the <SI__TOOLING_HOME>/conf/server/deployment.yaml file. For the complete list of data sources configured for the Streaming Integrator, see Configuring Data sources .","title":"Migrating Databases"},{"location":"setup/migrating-from-stream-processor/#migrating-siddhi-applications","text":"To migrate the Siddhi applications that you have deployed in WSO2 SP 4.4.0, follow the procedure below: Copy all the Siddhi applications in the <SP_HOME/wso2/worker/deployment/siddhi-files directory. Place the Siddhi applications you copied in the <SI_HOME/wso2/server/deployment/siddhi-files directory.","title":"Migrating Siddhi applications"},{"location":"setup/migrating-from-stream-processor/#testing-the-migration","text":"Simulate a few events to the Siddhi applications deployed in the Streaming Integrator to test whether they are generating the expected results.","title":"Testing the migration"},{"location":"setup/setting-up-physical-databases/","text":"Setting Up Physical Databases \u00b6 Defining data tables \u00b6 This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for Physical Stores . Adding primary and index keys Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner. Adding primary keys Description Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')) , those attributes would act as a composite primary key. Syntax @PrimaryKey('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key. Adding index keys Description An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table. Syntax To index by a single attribute: @Index('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); To index by multiple attributes: @Index('<attribute_1>''<attribute_2>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example @Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute. Defining Tables for Physical Stores \u00b6 This section explains how to define data tables to store data handled by the Streaming Integrator in physical databases. The @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); Info This method is not recommended in a production environment because it is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs: ) as shown in the example below. Info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this method is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source is defined in the wso2.datasources section of the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type='<DATABASE_TYPE>', datasource=\u2019<carbon.datasource.name>\u2019) Info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about defining datasources, see Configuring Datasources . The following database types are currently supported for the Streaming Integrator. Before you begin: In order to create and use an event table to store data, the following should be completed: - The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. - A database instance should be started. - The user IDs used to perform the required table operations must be granted the relevant privileges. - The relevant JDBC Driver must be downloaded and the jar must be placed in the <SI_HOME>|<SI_TOOLING_HOME>/lib directory. RDBMS Apache HBase Apache Solr MongoDB RDBMS \u00b6 The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2 Query syntax The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\"<jdbc.url>\", username=\"<username>\", password=\"<password>\",pool.properties=\"<prop1>:<val1>,<prop2>:<val2>\") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table <table_name>(col1 datatype1, col2 datatype2, col3 datatype3); Parameters The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma-separated list. Example The following is an example of an RDBMS table definition. @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); Apache HBase \u00b6 Query syntax The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\"<STRING>\", table.name=\"<STRING>\", column.family.name=\"<STRING>\") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") Parameters The following parameters are configured in the definition of an HBase event table. Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma-separated list. Example @Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long); Apache Solr \u00b6 Query syntax The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url=<solr-cloud-zookeeper-url>, collection=<solr-collection-name>, base.config=<config-name>, shards=<no-of-shards>, replicas=<no-of-replicas>, schema=<schema-definition>, commit.async=true|false) define table Footable (time long, date string); Parameters The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A Example This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string); MongoDB \u00b6 Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\"<MONGODB CONNECTION URI>\") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\"<ATTRIBUTE_NAME> <SORTING ORDER> <INDEX OPTIONS>\") define table <TABLE_NME> (<ATTRIBUTE1_NAME> <ATTRIBUTE1_TYPE>, <ATTRIBUTE2_NAME> <ATTRIBUTE2_TYPE>, <ATTRIBUTE3_NAME> <ATTRIBUTE3_TYPE>, ...); Parameters The mongodb.uri parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. Example The following query defines a MongoDB table named FooTable with the symbol , price , and volume attributes. The symbol attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long); Managing Stored Data via Streams \u00b6 Managing Stored Data via REST APIs \u00b6 Accessing and Manipulating Data in Multiple Tables \u00b6 The Streaming Integrator allows you to perform CRUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension . Performing CRUD operations for multiple tables In order to perform CRUD operations, the system parameter named perform.CRUD.operations needs to be set to true in the deployment.yaml file. The syntax for a Siddhi query to perform a CRUD operation in multiple tables is as follows. from TriggerStream#rdbms:cud(<STRING> datasource.name, <STRING> query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a data source named, SAMPLE_DB a Siddhi query can be written as follows. from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream Retrieving data from multiple tables In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query(<STRING> dataource.name, <STRING> query, <STRING> stream definition) select <attributes> insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows. from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderData insert into OutputStream;","title":"Setting Up Physical Databases"},{"location":"setup/setting-up-physical-databases/#setting-up-physical-databases","text":"","title":"Setting Up Physical Databases"},{"location":"setup/setting-up-physical-databases/#defining-data-tables","text":"This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for Physical Stores . Adding primary and index keys Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner. Adding primary keys Description Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')) , those attributes would act as a composite primary key. Syntax @PrimaryKey('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key. Adding index keys Description An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table. Syntax To index by a single attribute: @Index('<attribute_1>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); To index by multiple attributes: @Index('<attribute_1>''<attribute_2>') define table <event_table> (<attribute_1> <attribute_type>, <attribute_2> <attribute_type>, <attribute_3> <attribute_type>); Example @Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute.","title":"Defining data tables"},{"location":"setup/setting-up-physical-databases/#defining-tables-for-physical-stores","text":"This section explains how to define data tables to store data handled by the Streaming Integrator in physical databases. The @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); Info This method is not recommended in a production environment because it is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs: ) as shown in the example below. Info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this method is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source is defined in the wso2.datasources section of the <SI_HOME>|<SI_TOOLING_HOME>/conf/server/deployment.yaml file, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type='<DATABASE_TYPE>', datasource=\u2019<carbon.datasource.name>\u2019) Info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about defining datasources, see Configuring Datasources . The following database types are currently supported for the Streaming Integrator. Before you begin: In order to create and use an event table to store data, the following should be completed: - The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. - A database instance should be started. - The user IDs used to perform the required table operations must be granted the relevant privileges. - The relevant JDBC Driver must be downloaded and the jar must be placed in the <SI_HOME>|<SI_TOOLING_HOME>/lib directory. RDBMS Apache HBase Apache Solr MongoDB","title":"Defining Tables for Physical Stores"},{"location":"setup/setting-up-physical-databases/#rdbms","text":"The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2 Query syntax The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\"<jdbc.url>\", username=\"<username>\", password=\"<password>\",pool.properties=\"<prop1>:<val1>,<prop2>:<val2>\") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table <table_name>(col1 datatype1, col2 datatype2, col3 datatype3); Parameters The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma-separated list. Example The following is an example of an RDBMS table definition. @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long);","title":"RDBMS"},{"location":"setup/setting-up-physical-databases/#apache-hbase","text":"Query syntax The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\"<STRING>\", table.name=\"<STRING>\", column.family.name=\"<STRING>\") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") Parameters The following parameters are configured in the definition of an HBase event table. Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma-separated list. Example @Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long);","title":"Apache HBase"},{"location":"setup/setting-up-physical-databases/#apache-solr","text":"Query syntax The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url=<solr-cloud-zookeeper-url>, collection=<solr-collection-name>, base.config=<config-name>, shards=<no-of-shards>, replicas=<no-of-replicas>, schema=<schema-definition>, commit.async=true|false) define table Footable (time long, date string); Parameters The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A Example This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string);","title":"Apache Solr"},{"location":"setup/setting-up-physical-databases/#mongodb","text":"Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\"<MONGODB CONNECTION URI>\") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\"<ATTRIBUTE_NAME> <SORTING ORDER> <INDEX OPTIONS>\") define table <TABLE_NME> (<ATTRIBUTE1_NAME> <ATTRIBUTE1_TYPE>, <ATTRIBUTE2_NAME> <ATTRIBUTE2_TYPE>, <ATTRIBUTE3_NAME> <ATTRIBUTE3_TYPE>, ...); Parameters The mongodb.uri parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. Example The following query defines a MongoDB table named FooTable with the symbol , price , and volume attributes. The symbol attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long);","title":"MongoDB"},{"location":"setup/setting-up-physical-databases/#managing-stored-data-via-streams","text":"","title":"Managing Stored Data via Streams"},{"location":"setup/setting-up-physical-databases/#managing-stored-data-via-rest-apis","text":"","title":"Managing Stored Data via REST APIs"},{"location":"setup/setting-up-physical-databases/#accessing-and-manipulating-data-in-multiple-tables","text":"The Streaming Integrator allows you to perform CRUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension . Performing CRUD operations for multiple tables In order to perform CRUD operations, the system parameter named perform.CRUD.operations needs to be set to true in the deployment.yaml file. The syntax for a Siddhi query to perform a CRUD operation in multiple tables is as follows. from TriggerStream#rdbms:cud(<STRING> datasource.name, <STRING> query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a data source named, SAMPLE_DB a Siddhi query can be written as follows. from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream Retrieving data from multiple tables In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query(<STRING> dataource.name, <STRING> query, <STRING> stream definition) select <attributes> insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows. from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderData insert into OutputStream;","title":"Accessing and Manipulating Data in Multiple Tables"}]}